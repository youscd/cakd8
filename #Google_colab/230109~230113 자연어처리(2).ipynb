{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true,"mount_file_id":"158ylwZKB2CrgQZurja-FUi-HUOtVVw8H","authorship_tag":"ABX9TyPNUpX0I4vRszRiiEeT3qYF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"934f6cc022b1491fb7ab7e165b779e21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b991e74841b477badb544cbbb550c23","IPY_MODEL_c4f058f62bc0456cafa36da61055ec4b","IPY_MODEL_cbad6d98d4694525976ce105ff9ee132"],"layout":"IPY_MODEL_8eae7f35e4b24d86a1956a48fada0dc3"}},"4b991e74841b477badb544cbbb550c23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bb61450ecdb468ba361ee79474ef38c","placeholder":"​","style":"IPY_MODEL_c36c313c937a43db9bda32108bec0270","value":"Downloading: 100%"}},"c4f058f62bc0456cafa36da61055ec4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cc2b60988414aa080713e19abbde75d","max":249928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f81a0eb6c1a4730979134feb4068f14","value":249928}},"cbad6d98d4694525976ce105ff9ee132":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_121ec3a05e8b4b76bc2e090cf815c467","placeholder":"​","style":"IPY_MODEL_b99b3ef100fe496f89922f78aada180d","value":" 250k/250k [00:00&lt;00:00, 2.04MB/s]"}},"8eae7f35e4b24d86a1956a48fada0dc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bb61450ecdb468ba361ee79474ef38c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c36c313c937a43db9bda32108bec0270":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9cc2b60988414aa080713e19abbde75d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f81a0eb6c1a4730979134feb4068f14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"121ec3a05e8b4b76bc2e090cf815c467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b99b3ef100fe496f89922f78aada180d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5318c508cdb44b67a2307c2c4da9d619":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b046c7eecf594a859fd8e95bdfa6f787","IPY_MODEL_6d3c80b107ad4fb880c57afe475adc9f","IPY_MODEL_a7f6bd41d2694d35942e715d3e234d1c"],"layout":"IPY_MODEL_6bfa0733ba404695a0927aa9402e2e96"}},"b046c7eecf594a859fd8e95bdfa6f787":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a37e80efd9648a7830a30503cba5dc2","placeholder":"​","style":"IPY_MODEL_2eef67b68a21404fbf0e264f18fbd517","value":"Downloading: 100%"}},"6d3c80b107ad4fb880c57afe475adc9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee3e7c770b614bdabd83c09a7f4e31b9","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2f47a035667481dbb412915c7c5b2e3","value":49}},"a7f6bd41d2694d35942e715d3e234d1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_312a006d92b24436b5cac8ef8f772fb2","placeholder":"​","style":"IPY_MODEL_ddabac975a104475a73577bbe2c1781d","value":" 49.0/49.0 [00:00&lt;00:00, 2.15kB/s]"}},"6bfa0733ba404695a0927aa9402e2e96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a37e80efd9648a7830a30503cba5dc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eef67b68a21404fbf0e264f18fbd517":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee3e7c770b614bdabd83c09a7f4e31b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f47a035667481dbb412915c7c5b2e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"312a006d92b24436b5cac8ef8f772fb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddabac975a104475a73577bbe2c1781d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"542e57c09aae4ef48e18727b6d54f451":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_113d83e4cb1e4aafa587339aa596d075","IPY_MODEL_f323cbaca3bf4dc89b20920e3ea613fd","IPY_MODEL_08633d696f374f3583fb7ddf72a1e5b4"],"layout":"IPY_MODEL_d66451c1c12b415998eed3da167613fc"}},"113d83e4cb1e4aafa587339aa596d075":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9f8135c45d2475aa4267b6dd243c342","placeholder":"​","style":"IPY_MODEL_eceef486ed164fbf8b1895c8c61f83be","value":"Downloading: 100%"}},"f323cbaca3bf4dc89b20920e3ea613fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_789765a7cdeb4d78861935f2f12487be","max":619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad3bfde8753f49babee0ee4182940671","value":619}},"08633d696f374f3583fb7ddf72a1e5b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc4aa1782e514bbc97c85f8c51425de3","placeholder":"​","style":"IPY_MODEL_325e6fce3959440c92a8d6d1dbcfdfe3","value":" 619/619 [00:00&lt;00:00, 31.9kB/s]"}},"d66451c1c12b415998eed3da167613fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9f8135c45d2475aa4267b6dd243c342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eceef486ed164fbf8b1895c8c61f83be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"789765a7cdeb4d78861935f2f12487be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad3bfde8753f49babee0ee4182940671":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc4aa1782e514bbc97c85f8c51425de3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"325e6fce3959440c92a8d6d1dbcfdfe3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d30479a31ca4e44b72ea12712e869bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88a372a41f6e41ff8432c122cce41e13","IPY_MODEL_5d8d0d96abc346f288b225eed794f6d5","IPY_MODEL_907c6e793ead40c99e2df7394b1df3a1"],"layout":"IPY_MODEL_43ac47dc3b9e430e8086a2be7f2301d3"}},"88a372a41f6e41ff8432c122cce41e13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3282ce3a0dc34d2fb633fa1ebe9db7f5","placeholder":"​","style":"IPY_MODEL_3e7e3ad1bbfb453e915894f35f25ddd2","value":"Downloading: 100%"}},"5d8d0d96abc346f288b225eed794f6d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5651b13b2dfb4e708caca8c749b85935","max":438218004,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7e9056bc1e247bd81182b6448a82472","value":438218004}},"907c6e793ead40c99e2df7394b1df3a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98bd56734d8940149bb8559d55306887","placeholder":"​","style":"IPY_MODEL_b0adb4fa6f93405a93ee4e3ee40de279","value":" 438M/438M [00:06&lt;00:00, 64.0MB/s]"}},"43ac47dc3b9e430e8086a2be7f2301d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3282ce3a0dc34d2fb633fa1ebe9db7f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e7e3ad1bbfb453e915894f35f25ddd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5651b13b2dfb4e708caca8c749b85935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7e9056bc1e247bd81182b6448a82472":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98bd56734d8940149bb8559d55306887":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0adb4fa6f93405a93ee4e3ee40de279":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e48c9db774f462dbb00b58e5fb226c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b148c46754c48f4bffddacc4fc3f4a1","IPY_MODEL_51e33c80c43d43158dc2cc695a4133af","IPY_MODEL_d241b55b7ec047e69478b8a9e0dc364e"],"layout":"IPY_MODEL_b7222ba1df3842a5bdc047145cdb7219"}},"1b148c46754c48f4bffddacc4fc3f4a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d173ea19e43742cf9cdabde3dd9a0f46","placeholder":"​","style":"IPY_MODEL_2e970c7e072a40ab8351be328a1e206b","value":"Downloading: 100%"}},"51e33c80c43d43158dc2cc695a4133af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac015e9aa9f34f45b7f5164eb38b1b99","max":249928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e97483e7d754db185f5e0f2cab29bcb","value":249928}},"d241b55b7ec047e69478b8a9e0dc364e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbac9f386c54499e9ce92ce81c012135","placeholder":"​","style":"IPY_MODEL_f7de92947cc1470d90acc8f193641283","value":" 250k/250k [00:00&lt;00:00, 3.08MB/s]"}},"b7222ba1df3842a5bdc047145cdb7219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d173ea19e43742cf9cdabde3dd9a0f46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e970c7e072a40ab8351be328a1e206b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac015e9aa9f34f45b7f5164eb38b1b99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e97483e7d754db185f5e0f2cab29bcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbac9f386c54499e9ce92ce81c012135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7de92947cc1470d90acc8f193641283":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fe5899fd2284fc089e8a67c67a03826":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0443d835bf54eb78ede82402e53dbce","IPY_MODEL_39648524585a41068d8d187dd282c944","IPY_MODEL_65bf40f6e5fe46a2b07f3ef4ee84d521"],"layout":"IPY_MODEL_8c476717d7e049e3bb2684578368a9a7"}},"d0443d835bf54eb78ede82402e53dbce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac1ef3cab0994210a1704cf494c0494c","placeholder":"​","style":"IPY_MODEL_23b86b767ec243dbb8b50096baeb2bbc","value":"Downloading: 100%"}},"39648524585a41068d8d187dd282c944":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98942fbd09334dfe9f589e61acbc464f","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2272537eb6094a8b994946261a83ed7c","value":49}},"65bf40f6e5fe46a2b07f3ef4ee84d521":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d00684c06c2471a8c3153d2c1712ed9","placeholder":"​","style":"IPY_MODEL_c93ea8c12bf94dc89f62c09954c7359b","value":" 49.0/49.0 [00:00&lt;00:00, 2.89kB/s]"}},"8c476717d7e049e3bb2684578368a9a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac1ef3cab0994210a1704cf494c0494c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b86b767ec243dbb8b50096baeb2bbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98942fbd09334dfe9f589e61acbc464f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2272537eb6094a8b994946261a83ed7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d00684c06c2471a8c3153d2c1712ed9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c93ea8c12bf94dc89f62c09954c7359b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f225d1bd8fae41958508c98f0c974ec3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6f0446e15e44fa6aadc684c05998dff","IPY_MODEL_0396814f88d84b28b3995681de3fae6b","IPY_MODEL_11e4480e38374f7bba1b8b70f6093ca0"],"layout":"IPY_MODEL_e38611dc4af44163ae0db87ed7f2531b"}},"a6f0446e15e44fa6aadc684c05998dff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f0aa1db6a2643c88fd62750d9c77c0e","placeholder":"​","style":"IPY_MODEL_03516010b6d44ddd98a0ff162a9183e3","value":"Downloading: 100%"}},"0396814f88d84b28b3995681de3fae6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6233d2c3c6d4183a2575cc8d96604a7","max":619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8349ac4ca8244664b395827ce70064b9","value":619}},"11e4480e38374f7bba1b8b70f6093ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c503ad45251b492aad9acb3576ce14a4","placeholder":"​","style":"IPY_MODEL_aa0ebfec8faa45a590ff8184b31f04bb","value":" 619/619 [00:00&lt;00:00, 28.2kB/s]"}},"e38611dc4af44163ae0db87ed7f2531b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f0aa1db6a2643c88fd62750d9c77c0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03516010b6d44ddd98a0ff162a9183e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6233d2c3c6d4183a2575cc8d96604a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8349ac4ca8244664b395827ce70064b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c503ad45251b492aad9acb3576ce14a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa0ebfec8faa45a590ff8184b31f04bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"216c7b0827c34dae89fe8d9f03a9c4f0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8699e87857c34b13bab8ee4c7f0830be","IPY_MODEL_08aea8ec8c6b4581bbb90aa103838969","IPY_MODEL_60f31947ae374b4894cfda100134aeb7"],"layout":"IPY_MODEL_e65df6f13687438e8d3b6942e30a43cf"}},"8699e87857c34b13bab8ee4c7f0830be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3e02fe5925f418c9c6f3a83bf23b61a","placeholder":"​","style":"IPY_MODEL_64cbd799d84343d891764f04907b378a","value":"Downloading: 100%"}},"08aea8ec8c6b4581bbb90aa103838969":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0024210009f341c69f6aa861cb825113","max":438218004,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75958d4ea121435cbb3d60c56518845a","value":438218004}},"60f31947ae374b4894cfda100134aeb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bca36fccacd4a95b512c92eb77e98f0","placeholder":"​","style":"IPY_MODEL_ce39154c292e4f139f59805653023771","value":" 438M/438M [00:10&lt;00:00, 41.3MB/s]"}},"e65df6f13687438e8d3b6942e30a43cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3e02fe5925f418c9c6f3a83bf23b61a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64cbd799d84343d891764f04907b378a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0024210009f341c69f6aa861cb825113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75958d4ea121435cbb3d60c56518845a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bca36fccacd4a95b512c92eb77e98f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce39154c292e4f139f59805653023771":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49a1929edf2547c5a8969574f14ec3ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07c1d99f435f4b4baeffb8641d124bdf","IPY_MODEL_bcead3587651482caac9fbc1e892bc07","IPY_MODEL_7d47edb48dfd4026bd40900d97f0a8ba"],"layout":"IPY_MODEL_b99618507fc049efa7544b64556e3a3f"}},"07c1d99f435f4b4baeffb8641d124bdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae9b66202b5f4a3c977c0675f63bdaf7","placeholder":"​","style":"IPY_MODEL_ec45412bf15644efb5b8f061401c0dd2","value":"Epoch 0: 100%"}},"bcead3587651482caac9fbc1e892bc07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6510aba2ef8d4491a98bde0d774328db","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a2e4ed8ea3aa4f3db0f7261ca1786b4a","value":1}},"7d47edb48dfd4026bd40900d97f0a8ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56dbb824e4b54289a863d5b5fc04a4da","placeholder":"​","style":"IPY_MODEL_943fb06e12274054a47fcd0f7e1a0f11","value":" 6251/6251 [53:21&lt;00:00,  1.95it/s, loss=0.304, v_num=0, acc=0.812, val_loss=0.272, val_acc=0.887]"}},"b99618507fc049efa7544b64556e3a3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"ae9b66202b5f4a3c977c0675f63bdaf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec45412bf15644efb5b8f061401c0dd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6510aba2ef8d4491a98bde0d774328db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2e4ed8ea3aa4f3db0f7261ca1786b4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56dbb824e4b54289a863d5b5fc04a4da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"943fb06e12274054a47fcd0f7e1a0f11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56615059717b4001a9124574bfb7a7c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7952c09e6faf4b8ba06fc12fe0cead85","IPY_MODEL_ad7322f262b1406ca9d662c5e2692cc6","IPY_MODEL_6f3a8e35272044b98f6af45a6e178ef9"],"layout":"IPY_MODEL_473c5ad0a80e480ba3c87782d7be94dd"}},"7952c09e6faf4b8ba06fc12fe0cead85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ece8a727bc74240901ec83a0cb8cb4a","placeholder":"​","style":"IPY_MODEL_1e47eb3509f74afa969b892b2285e19c","value":"Validation DataLoader 0: 100%"}},"ad7322f262b1406ca9d662c5e2692cc6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e68d8bd89e04de7841c1f1b90058278","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbc1b36e310842938f7f1bd7ae09f194","value":1}},"6f3a8e35272044b98f6af45a6e178ef9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30de6a3abf8f4cf59683e3c6e6c1eafc","placeholder":"​","style":"IPY_MODEL_4d42659eef1b41fca5f32ca7e9c33cfd","value":" 1563/1563 [05:30&lt;00:00,  4.73it/s]"}},"473c5ad0a80e480ba3c87782d7be94dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"1ece8a727bc74240901ec83a0cb8cb4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e47eb3509f74afa969b892b2285e19c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e68d8bd89e04de7841c1f1b90058278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbc1b36e310842938f7f1bd7ae09f194":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30de6a3abf8f4cf59683e3c6e6c1eafc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d42659eef1b41fca5f32ca7e9c33cfd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79e3d220a9674039b2614a6114fc8422":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aebf205376b5401389115efbe7de6b44","IPY_MODEL_a6172aa62a1c4db3946202afec43d72f","IPY_MODEL_ba57602191d847169ed5ba9065f51e04"],"layout":"IPY_MODEL_f0de303ac857443399e97c0448996810"}},"aebf205376b5401389115efbe7de6b44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0850e9f548054575b21e45ead41f0f24","placeholder":"​","style":"IPY_MODEL_39012873babb49e5bce113e7969860b2","value":"Downloading: 100%"}},"a6172aa62a1c4db3946202afec43d72f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dd4f208d1ba42e2bab2a13737b6c4bd","max":249928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48df0eafa3de4bd0bfa99c386b592f3f","value":249928}},"ba57602191d847169ed5ba9065f51e04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_411b434505b1443f896f2455d2d864a4","placeholder":"​","style":"IPY_MODEL_a92c4a2299524007903ea80ca7eba945","value":" 250k/250k [00:00&lt;00:00, 244kB/s]"}},"f0de303ac857443399e97c0448996810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0850e9f548054575b21e45ead41f0f24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39012873babb49e5bce113e7969860b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dd4f208d1ba42e2bab2a13737b6c4bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48df0eafa3de4bd0bfa99c386b592f3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"411b434505b1443f896f2455d2d864a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a92c4a2299524007903ea80ca7eba945":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a105ea417d14a959847a27b82823e37":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbde3f2fa4b047ac8319c4c52744b0ad","IPY_MODEL_2a5ff4a1cb634e34b4623a8dd9566fc1","IPY_MODEL_b46074c6b2f1448ca2c83d3b4cd3df9a"],"layout":"IPY_MODEL_b1053b5c5086459c8c1842fbb704b3f5"}},"bbde3f2fa4b047ac8319c4c52744b0ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d18a377845de400c80bf3cecb0b2dbdd","placeholder":"​","style":"IPY_MODEL_e0528291bfcc4d3e9052d639f916e1d6","value":"Downloading: 100%"}},"2a5ff4a1cb634e34b4623a8dd9566fc1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_995c25b7889e4fbfbd69843b99bd3b04","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed9ec33d43104347960a7227f9613658","value":49}},"b46074c6b2f1448ca2c83d3b4cd3df9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53e5cb6cd1ea475dbfe10a0124cee0e1","placeholder":"​","style":"IPY_MODEL_b905ab6047794fbc8258f041873ff8df","value":" 49.0/49.0 [00:00&lt;00:00, 2.63kB/s]"}},"b1053b5c5086459c8c1842fbb704b3f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d18a377845de400c80bf3cecb0b2dbdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0528291bfcc4d3e9052d639f916e1d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"995c25b7889e4fbfbd69843b99bd3b04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9ec33d43104347960a7227f9613658":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53e5cb6cd1ea475dbfe10a0124cee0e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b905ab6047794fbc8258f041873ff8df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44a2600aca2c40f6ab2644ddd0c1d606":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_acb2a2fc428d4f3bb961effae0157479","IPY_MODEL_a6314d34eabe4c479f2f02ddc2272d69","IPY_MODEL_336d2fe6ae324d7a9ca74d09a47d5803"],"layout":"IPY_MODEL_0eb0520728d044a9b4f082ada414bb54"}},"acb2a2fc428d4f3bb961effae0157479":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8ee057eba194b09bdd6ff9e2f898609","placeholder":"​","style":"IPY_MODEL_edc513c3dfd9486c8440e942616b0c4f","value":"Downloading: 100%"}},"a6314d34eabe4c479f2f02ddc2272d69":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7735f0a771064ff98bb5650740c030cb","max":619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6142e0ce613a4a37a44c1c7c02a4ed1a","value":619}},"336d2fe6ae324d7a9ca74d09a47d5803":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee84706c3b3949b99a67dbba159d046a","placeholder":"​","style":"IPY_MODEL_5912cc9d110e42248b47f69ab1569e2d","value":" 619/619 [00:00&lt;00:00, 36.3kB/s]"}},"0eb0520728d044a9b4f082ada414bb54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8ee057eba194b09bdd6ff9e2f898609":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc513c3dfd9486c8440e942616b0c4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7735f0a771064ff98bb5650740c030cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6142e0ce613a4a37a44c1c7c02a4ed1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee84706c3b3949b99a67dbba159d046a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5912cc9d110e42248b47f69ab1569e2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"529582c2f1da48aab65cd13d620a485f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7554364a5424e04913fa0ca146fbda0","IPY_MODEL_d5e8b7e6486444eeb41db799b934c360","IPY_MODEL_7ab38d840a2e4b1fb7d3d1ec32160e56"],"layout":"IPY_MODEL_1f48ac393c5b4e80a1e85666e86ff07e"}},"c7554364a5424e04913fa0ca146fbda0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d90fae61c5144a1811ce11c24f823a4","placeholder":"​","style":"IPY_MODEL_68faa7cd9bbb47ed9682644bf3ae2c7e","value":"Downloading: 100%"}},"d5e8b7e6486444eeb41db799b934c360":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4465a28de2be461188e14b634a735b3c","max":438218004,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2dc999e3b5104febb71fc9c7c0c10d08","value":438218004}},"7ab38d840a2e4b1fb7d3d1ec32160e56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ced1f1ede28e4102b7779688757a2eaf","placeholder":"​","style":"IPY_MODEL_6c7193de10374a8394a8c7ef7d67d44c","value":" 438M/438M [00:05&lt;00:00, 83.8MB/s]"}},"1f48ac393c5b4e80a1e85666e86ff07e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d90fae61c5144a1811ce11c24f823a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68faa7cd9bbb47ed9682644bf3ae2c7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4465a28de2be461188e14b634a735b3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dc999e3b5104febb71fc9c7c0c10d08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ced1f1ede28e4102b7779688757a2eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c7193de10374a8394a8c7ef7d67d44c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05dae5cf5b194d75b950c73dabff1e8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a80aec5894144083bace990b27c2673d","IPY_MODEL_e106b0c60fa544d792aec84464d253b3","IPY_MODEL_1503632855b74cd580d67407af069f78"],"layout":"IPY_MODEL_153f2c8d85dc4f80b270d49b9a8e30f3"}},"a80aec5894144083bace990b27c2673d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_362286618a114cd6b7178fc655490e89","placeholder":"​","style":"IPY_MODEL_7cef0b3ca8dc492c8efc523993704e5d","value":"Epoch 5: 100%"}},"e106b0c60fa544d792aec84464d253b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9e9c61db25d465192e3d7c733c897c6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0852929361a54386a4cd1ed0f7f840fd","value":1}},"1503632855b74cd580d67407af069f78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edc322f5a8694e498cb8b7f09ac1ecb0","placeholder":"​","style":"IPY_MODEL_02cb49aeda61412ba06ada4c19220f38","value":" 876/876 [27:28&lt;00:00,  1.88s/it, loss=0.174, v_num=0, acc=1.000, val_loss=1.260, val_acc=0.637]"}},"153f2c8d85dc4f80b270d49b9a8e30f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"362286618a114cd6b7178fc655490e89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cef0b3ca8dc492c8efc523993704e5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9e9c61db25d465192e3d7c733c897c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0852929361a54386a4cd1ed0f7f840fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"edc322f5a8694e498cb8b7f09ac1ecb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02cb49aeda61412ba06ada4c19220f38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c04c45812d70453fb3495f0855129d80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1a7457f4b59347c5afd51db94c1b041f","IPY_MODEL_1b8aba8282ca4a8297193b57817b4fbd","IPY_MODEL_dde9519b67644169ae2a9822566de3fb"],"layout":"IPY_MODEL_882e992f5e9b4740a0cc54079a50305b"}},"1a7457f4b59347c5afd51db94c1b041f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb26fdc08e9b4310a43f18a512545813","placeholder":"​","style":"IPY_MODEL_b33c504fbd374baeb6f0a48fde5613d2","value":"Validation DataLoader 0: 100%"}},"1b8aba8282ca4a8297193b57817b4fbd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_f21e298e6f144505970513318ba052ea","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a664e66e40834a96a812d49c1b5ee26a","value":1}},"dde9519b67644169ae2a9822566de3fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43b24d302ee44d69bd26f3a0f7334e77","placeholder":"​","style":"IPY_MODEL_da704c31f95c45e6801adafc89e40f53","value":" 94/94 [00:10&lt;00:00,  9.37it/s]"}},"882e992f5e9b4740a0cc54079a50305b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"cb26fdc08e9b4310a43f18a512545813":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b33c504fbd374baeb6f0a48fde5613d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f21e298e6f144505970513318ba052ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a664e66e40834a96a812d49c1b5ee26a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43b24d302ee44d69bd26f3a0f7334e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da704c31f95c45e6801adafc89e40f53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5a34d4a54f44fff83b6766d290fa0b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_311a9a4428f24d708aceb6c4e017c2df","IPY_MODEL_047674a91b72438b9e6e87cc7a4ecf73","IPY_MODEL_0a96b6b56fd84bb2adc178ecbf68ae9a"],"layout":"IPY_MODEL_cbd92a6407bc41a091fd3d458dcb48e2"}},"311a9a4428f24d708aceb6c4e017c2df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eddb67708d14440cb8b84c723b3369b9","placeholder":"​","style":"IPY_MODEL_f267f04e1da94a27ab1bd16d1cd94b00","value":"Validation DataLoader 0: 100%"}},"047674a91b72438b9e6e87cc7a4ecf73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_2264b67496e142329a7f9499f518c3e0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8bad534e5da44d6b655efd16f1f9e8b","value":1}},"0a96b6b56fd84bb2adc178ecbf68ae9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83e1f4220fca4f44a114eeb740e834a3","placeholder":"​","style":"IPY_MODEL_75d65de116044d4d992da7463c51e1a0","value":" 94/94 [00:10&lt;00:00,  9.23it/s]"}},"cbd92a6407bc41a091fd3d458dcb48e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"eddb67708d14440cb8b84c723b3369b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f267f04e1da94a27ab1bd16d1cd94b00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2264b67496e142329a7f9499f518c3e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8bad534e5da44d6b655efd16f1f9e8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83e1f4220fca4f44a114eeb740e834a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75d65de116044d4d992da7463c51e1a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31760ac6ae2d4ae5a43e715bea024d39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bccefaa7a634db4b6452dbdcc819b09","IPY_MODEL_91a073fa6afb406da5803bceccec9bd0","IPY_MODEL_fe94d444a61241748e528fed91b36d71"],"layout":"IPY_MODEL_f7affb8f77004a86baf840aec4a7a5e8"}},"8bccefaa7a634db4b6452dbdcc819b09":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63f403b4715a4dc88b6e123a55fd50e9","placeholder":"​","style":"IPY_MODEL_25c0b83f21104afca81c690657063262","value":"Validation DataLoader 0: 100%"}},"91a073fa6afb406da5803bceccec9bd0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed674a4a73a442b69a3727bb6a0209ab","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e079b6a74c847dc9b87d079c102a888","value":1}},"fe94d444a61241748e528fed91b36d71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6670bb71b81244dfb0e8f09718da331f","placeholder":"​","style":"IPY_MODEL_dd1b46e97efe444eb7b549a36521a5ef","value":" 94/94 [00:10&lt;00:00,  9.20it/s]"}},"f7affb8f77004a86baf840aec4a7a5e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"63f403b4715a4dc88b6e123a55fd50e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25c0b83f21104afca81c690657063262":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed674a4a73a442b69a3727bb6a0209ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e079b6a74c847dc9b87d079c102a888":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6670bb71b81244dfb0e8f09718da331f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd1b46e97efe444eb7b549a36521a5ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b96f900d41c04e86bfa06b41bab2a7bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b289b436d47c4be585505cc399fc6208","IPY_MODEL_8ee166b3817343f2b8c66348a0f5b9aa","IPY_MODEL_63532f42a4d64f1bb3a57b229ba29056"],"layout":"IPY_MODEL_15d0bb2988594f4aa512205b24c6a342"}},"b289b436d47c4be585505cc399fc6208":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db61108877a847a98fc9c0e696ac6c4f","placeholder":"​","style":"IPY_MODEL_604ec591961a414788b9c7dbb22960e4","value":"Validation DataLoader 0: 100%"}},"8ee166b3817343f2b8c66348a0f5b9aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_34f60ac65a1b40118f12fdf688b944d5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1670ac2063d64669a10ad56e8649ff83","value":1}},"63532f42a4d64f1bb3a57b229ba29056":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_605d935b3cf748678615570d43564ae5","placeholder":"​","style":"IPY_MODEL_60fe8e98bf0c4301ab4427f3492b9732","value":" 94/94 [00:10&lt;00:00,  9.21it/s]"}},"15d0bb2988594f4aa512205b24c6a342":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"db61108877a847a98fc9c0e696ac6c4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"604ec591961a414788b9c7dbb22960e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34f60ac65a1b40118f12fdf688b944d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1670ac2063d64669a10ad56e8649ff83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"605d935b3cf748678615570d43564ae5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60fe8e98bf0c4301ab4427f3492b9732":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9294030e511047e28c0abfc18c6155a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f1a010dd7c74861841002977e3853ae","IPY_MODEL_84b828f4b97241b5babf5d824dd78674","IPY_MODEL_a4d76bac04de4b7e93452d4ae9cf8566"],"layout":"IPY_MODEL_b3aadb6531fd4e75b3368dc17726b938"}},"8f1a010dd7c74861841002977e3853ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84d83c7a165048f8b7126c38b828aabc","placeholder":"​","style":"IPY_MODEL_c5863e4e7b324fbb8e259a0929956393","value":"Validation DataLoader 0: 100%"}},"84b828f4b97241b5babf5d824dd78674":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1ba515e43074f82961fd8fdac6347f6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab700f2e0b7e4a488591636722d7c73c","value":1}},"a4d76bac04de4b7e93452d4ae9cf8566":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5511c7d3d9014dd693e150e86038e518","placeholder":"​","style":"IPY_MODEL_daefb6c8fc81441a99150b086f213775","value":" 94/94 [00:10&lt;00:00,  9.14it/s]"}},"b3aadb6531fd4e75b3368dc17726b938":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"84d83c7a165048f8b7126c38b828aabc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5863e4e7b324fbb8e259a0929956393":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1ba515e43074f82961fd8fdac6347f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab700f2e0b7e4a488591636722d7c73c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5511c7d3d9014dd693e150e86038e518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"daefb6c8fc81441a99150b086f213775":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cfc9a923eab4c54825f5fdeb372a39a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_632cd397c5f84b5a9dfa8c911019a004","IPY_MODEL_36a1557fbd7a406cba5fef8c143b44e6","IPY_MODEL_1d599bf4d6984c118536ccde1f94f39f"],"layout":"IPY_MODEL_1450d2a858ec471ba246d6b4a43dcf5f"}},"632cd397c5f84b5a9dfa8c911019a004":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_257a54e74af3403580209ea3087d6467","placeholder":"​","style":"IPY_MODEL_e623b1611511498c924e456b385e689c","value":"Validation DataLoader 0: 100%"}},"36a1557fbd7a406cba5fef8c143b44e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d744aef0eef422084c225d6b0e28bae","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0805cd0802fd4ef480f4dc2e603be9fe","value":1}},"1d599bf4d6984c118536ccde1f94f39f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3575f33f22944956889d5da613a33ba9","placeholder":"​","style":"IPY_MODEL_cd274677499a425fbfcb5f435e2cb000","value":" 94/94 [00:10&lt;00:00,  9.19it/s]"}},"1450d2a858ec471ba246d6b4a43dcf5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"257a54e74af3403580209ea3087d6467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e623b1611511498c924e456b385e689c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d744aef0eef422084c225d6b0e28bae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0805cd0802fd4ef480f4dc2e603be9fe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3575f33f22944956889d5da613a33ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd274677499a425fbfcb5f435e2cb000":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f38cea50add4bcb848305e2997cca10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19e4e0c5ad304c78ac76e46972dc6555","IPY_MODEL_65d5788b684b45ceb32cfcb541a1c7dc","IPY_MODEL_634a35b2d9dc4a8faf3f1230eb047fcf"],"layout":"IPY_MODEL_c193196c528349a68cd51bd6f65e7589"}},"19e4e0c5ad304c78ac76e46972dc6555":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dc315f19e544fbda0b31e7a6122ff6b","placeholder":"​","style":"IPY_MODEL_67c933a606f44eb9b263b143602826ae","value":"Epoch 0: 100%"}},"65d5788b684b45ceb32cfcb541a1c7dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89874fe7ba154e86b92a0f7d5136f5f7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9109eedb9aeb4384b1e519bc31609579","value":1}},"634a35b2d9dc4a8faf3f1230eb047fcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35ecf4235f124c849727865f6d7c9f06","placeholder":"​","style":"IPY_MODEL_51f15974764d44c6adcccf494ee94672","value":" 1689/1689 [09:16&lt;00:00,  3.04it/s, loss=0.212, v_num=0, acc=0.957, val_loss=0.216, val_acc=0.929]"}},"c193196c528349a68cd51bd6f65e7589":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"0dc315f19e544fbda0b31e7a6122ff6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67c933a606f44eb9b263b143602826ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89874fe7ba154e86b92a0f7d5136f5f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9109eedb9aeb4384b1e519bc31609579":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35ecf4235f124c849727865f6d7c9f06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51f15974764d44c6adcccf494ee94672":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad2b82ca30294425b6b266f7de5518d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83ab621e534544e690fa6bab55bb50c8","IPY_MODEL_53a1d9291b1648b5a77f89383686b5fc","IPY_MODEL_00aeabfe7fc84d07b792bf7a8238ab9f"],"layout":"IPY_MODEL_5260869685564b02bdf6f747d17f6727"}},"83ab621e534544e690fa6bab55bb50c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0358e1bf5b25416f891105aae2d19090","placeholder":"​","style":"IPY_MODEL_37350a9da355428abbd4deb354982821","value":"Validation DataLoader 0: 100%"}},"53a1d9291b1648b5a77f89383686b5fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_194f1bf71fec49f1af89f76ebeff83a2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7829d774c8bd402f803f75fae942e316","value":1}},"00aeabfe7fc84d07b792bf7a8238ab9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f3e129163b44069a93a0f264a14affe","placeholder":"​","style":"IPY_MODEL_6714d3ac17344c57b7baaa2d9bbcff1f","value":" 94/94 [00:10&lt;00:00,  8.66it/s]"}},"5260869685564b02bdf6f747d17f6727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"0358e1bf5b25416f891105aae2d19090":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37350a9da355428abbd4deb354982821":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"194f1bf71fec49f1af89f76ebeff83a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7829d774c8bd402f803f75fae942e316":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f3e129163b44069a93a0f264a14affe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6714d3ac17344c57b7baaa2d9bbcff1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3afad6acfc81492a8117ab083b9798b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_761b0529b63a404684951695ab5d1603","IPY_MODEL_61a9defe0d8a4f679e59cda2cc13c074","IPY_MODEL_2287a43f0f3a4a61bf0c311459af8ae6"],"layout":"IPY_MODEL_b6a627361a694115b503ef5fee190ae7"}},"761b0529b63a404684951695ab5d1603":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3258b24ee02241dd906adc3d1ac0d0df","placeholder":"​","style":"IPY_MODEL_5244a34bc6f047dea9a1af7665654aff","value":"Downloading: 100%"}},"61a9defe0d8a4f679e59cda2cc13c074":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6407c4e170740feb9ff836cd649ef33","max":249928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_df6e0834ae644822bfabeac2142b9c87","value":249928}},"2287a43f0f3a4a61bf0c311459af8ae6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d90b283ea2a4121a276987dea4ae671","placeholder":"​","style":"IPY_MODEL_3279c3f035314bcf9678a8d51ae263e2","value":" 250k/250k [00:00&lt;00:00, 866kB/s]"}},"b6a627361a694115b503ef5fee190ae7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3258b24ee02241dd906adc3d1ac0d0df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5244a34bc6f047dea9a1af7665654aff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6407c4e170740feb9ff836cd649ef33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df6e0834ae644822bfabeac2142b9c87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d90b283ea2a4121a276987dea4ae671":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3279c3f035314bcf9678a8d51ae263e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b70cdd9733e24f62922a99f0f58aaeb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f70baee7d7574d8fac04c39cef539e4a","IPY_MODEL_02438465cef3449e8a843437a3c899d1","IPY_MODEL_5427950691bc4d3abaefd8cbe2a29611"],"layout":"IPY_MODEL_658e3412eeb740b6aea9c3d3c77d32c5"}},"f70baee7d7574d8fac04c39cef539e4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae30eb6940c14c24a700e432dda01b05","placeholder":"​","style":"IPY_MODEL_a109ae64e80940c09d9fef4f35859201","value":"Downloading: 100%"}},"02438465cef3449e8a843437a3c899d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bec71aadf8a24113a5c21318e45dce1f","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_03b090d683764c3da505fa2f0b1b1a7c","value":49}},"5427950691bc4d3abaefd8cbe2a29611":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80e7abce8c2e4862b5ecb7ff66fde78b","placeholder":"​","style":"IPY_MODEL_1bf240bf4088420da8c0dd4d4fe41adb","value":" 49.0/49.0 [00:00&lt;00:00, 3.45kB/s]"}},"658e3412eeb740b6aea9c3d3c77d32c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae30eb6940c14c24a700e432dda01b05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a109ae64e80940c09d9fef4f35859201":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bec71aadf8a24113a5c21318e45dce1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03b090d683764c3da505fa2f0b1b1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80e7abce8c2e4862b5ecb7ff66fde78b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bf240bf4088420da8c0dd4d4fe41adb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b33b4d5c56c448a98428f22c0125503":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96aec9e5ec4248b28a88ab118dec2fe1","IPY_MODEL_161fd874e1c645d4b49dd64cb4f92504","IPY_MODEL_b129935b8d994ef0a0d9f4a37b3aff4e"],"layout":"IPY_MODEL_7ebed85d76644fd29c3aa5ccf96f7068"}},"96aec9e5ec4248b28a88ab118dec2fe1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d0d7bad399a44bbb27060be1f83b5d0","placeholder":"​","style":"IPY_MODEL_a377ecbe4e13471cbeafa2706e09b206","value":"Downloading: 100%"}},"161fd874e1c645d4b49dd64cb4f92504":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4e047f4ba1f4309882b1fc5312ffbfe","max":619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4534aa22dd5461b90cbc6566c77c073","value":619}},"b129935b8d994ef0a0d9f4a37b3aff4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c486765c27964f779142536ae387eec3","placeholder":"​","style":"IPY_MODEL_82355e0ebe7a4d9cb3fcbe217f430221","value":" 619/619 [00:00&lt;00:00, 43.2kB/s]"}},"7ebed85d76644fd29c3aa5ccf96f7068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0d7bad399a44bbb27060be1f83b5d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a377ecbe4e13471cbeafa2706e09b206":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4e047f4ba1f4309882b1fc5312ffbfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4534aa22dd5461b90cbc6566c77c073":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c486765c27964f779142536ae387eec3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82355e0ebe7a4d9cb3fcbe217f430221":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc527e02eade41528187ccd90c38a4e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9dd3df3b4340457cb3b3bcfaa0530c6f","IPY_MODEL_917c862e6d074e15af581dac892caa8b","IPY_MODEL_3b546b73a133478f9c7b4f5b298d31c9"],"layout":"IPY_MODEL_fefd45c296444db9ad53038fe0c56187"}},"9dd3df3b4340457cb3b3bcfaa0530c6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c467eb7690f44cf99397ed853d534870","placeholder":"​","style":"IPY_MODEL_0543a3173b814608a9bf9dad92747c7f","value":"Downloading: 100%"}},"917c862e6d074e15af581dac892caa8b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e3fb356633455780f19ed6a2add497","max":438218004,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fb280271a104d40ae2715b57dae25ef","value":438218004}},"3b546b73a133478f9c7b4f5b298d31c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3226bb845ae044e18845f17d450112eb","placeholder":"​","style":"IPY_MODEL_78de8d35890b4409a06cfcc2b228e0c1","value":" 438M/438M [00:05&lt;00:00, 83.2MB/s]"}},"fefd45c296444db9ad53038fe0c56187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c467eb7690f44cf99397ed853d534870":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0543a3173b814608a9bf9dad92747c7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"95e3fb356633455780f19ed6a2add497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb280271a104d40ae2715b57dae25ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3226bb845ae044e18845f17d450112eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78de8d35890b4409a06cfcc2b228e0c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"144165179b1240188f998b78223ea8c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36869bc1933f4761872efcd0be72ab53","IPY_MODEL_ec5be25aebce4843b24c784934db3af8","IPY_MODEL_c233bd6bf94b41ddba58ee7ba807bdca"],"layout":"IPY_MODEL_3a36740160634029bdea81ff9e96f323"}},"36869bc1933f4761872efcd0be72ab53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79447a470c08427382802d5b2aee5f83","placeholder":"​","style":"IPY_MODEL_b7f032d0d1454d49a6cd48e2e9d1a932","value":"Epoch 0: 100%"}},"ec5be25aebce4843b24c784934db3af8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_785b118b04764969ab6a210578a6b39a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b41aa89942b2491b9ac15d48c3504399","value":1}},"c233bd6bf94b41ddba58ee7ba807bdca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd1d7987f55f46f1b98f0685a0599ae0","placeholder":"​","style":"IPY_MODEL_470f315f45a44666a7f9b6b3577aa0b3","value":" 7688/7688 [1:24:01&lt;00:00,  1.52it/s, loss=0.432, v_num=0, acc=1.000, val_loss=0.456, val_acc=0.867]"}},"3a36740160634029bdea81ff9e96f323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"79447a470c08427382802d5b2aee5f83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7f032d0d1454d49a6cd48e2e9d1a932":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"785b118b04764969ab6a210578a6b39a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b41aa89942b2491b9ac15d48c3504399":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd1d7987f55f46f1b98f0685a0599ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"470f315f45a44666a7f9b6b3577aa0b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c667a1bd2e1a49b4ba6239507e4cf2c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_183ff9d02308440ca5c91320ac12ea86","IPY_MODEL_5f3b858b83234a0e94562916d26e2d6b","IPY_MODEL_333a9cc3e163449fad23a6e485954c1d"],"layout":"IPY_MODEL_6c97a4016d084557b21b761707e80a18"}},"183ff9d02308440ca5c91320ac12ea86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_161a909dbc0b438687c721d1bf521ee1","placeholder":"​","style":"IPY_MODEL_26f362fe835644a9b229bb5a9a8cdbec","value":"Validation DataLoader 0: 100%"}},"5f3b858b83234a0e94562916d26e2d6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e66f26534aa4e7bb7774fd6e348caae","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5849421080c349e082b35972e923e1cc","value":1}},"333a9cc3e163449fad23a6e485954c1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6419f291ddbb4c92ab90438c1b17759f","placeholder":"​","style":"IPY_MODEL_f4480310a74740c198c07ddacf1f707d","value":" 691/691 [02:35&lt;00:00,  4.43it/s]"}},"6c97a4016d084557b21b761707e80a18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"161a909dbc0b438687c721d1bf521ee1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26f362fe835644a9b229bb5a9a8cdbec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e66f26534aa4e7bb7774fd6e348caae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5849421080c349e082b35972e923e1cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6419f291ddbb4c92ab90438c1b17759f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4480310a74740c198c07ddacf1f707d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 230109 Day95"],"metadata":{"id":"Iozwvh7o5qXF"}},{"cell_type":"markdown","source":["## 셀프 어텐션\n","- 트랜스포머 구조에서 멀티 헤드 어텐션은 셀프 어텐션(self attention)이라고도 불리며, 트랜스포머 경쟁력의 원천은 셀프 어텐션에 있다\n","\n","- 어텐션(attention)은 시퀀스 입력에 수행하는 기계학습 방법의 일종으로,\\\n","시퀀스 요소들 가운데 태스크 수행에 중요한 요소는 집중하고 그렇지 않은 요소는 무시해 태스크 수행 성능을 끌어 올리며 기계 번역 과제에 처음 도입되었다\n","\n","- 기계 번역에 어텐션을 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움되는 단어들 위주로 취사 선택해서 번역 품질을 끌어 올리게 된다\\\n","즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소들만 추린다\n","\n","- 셀프 어텐션이란, 말 그대로 자기 자신에 수행하는 어텐션 기법으로 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출한다는 것이다\n","\n","- 합성곱 신경망과 비교 :\\\n","CNN은 합성곱 필터(convolution filter) 라는 특수한 장치를 이용해 시퀀스의 지역적인 특징을 잡아내는 모델이다.\\\n","자연어는 기본적으로 시퀀스(단어 혹은 형태소의 나열)이고, 특정 단어 기준 주변 문맥이 의미 형성에 중요한 역할을 하므로 CNN이 자연어 처리에 널리 쓰인다.\\\n","하지만 CNN은 합성곱 필터 크기를 넘어서는 문맥은 읽어내기 어렵다는 단점이 있다.\\\n","예컨대 필터 크기가 3(3개 단어씩 처리)이라면 4칸 이상 떨어져 있는 단어 사이의 의미는 캐치하기 어려운 것이다.\n","\n","- 순환 신경망과 비교 :\\\n","RNN 역시 시퀀스 정보를 압축하는 데 강점이 있는 구조이다.\\\n","소스 언어 시퀀스인 \"어제, 카페, 갔었어, 거기, 사람, 많더라\" 를 인코딩해야 한다고 가정해 보면 RNN은 소스 시퀀스를 '차례대로' 처리한다.\\\n","하지만 RNN은 시퀀스 길이가 길어질수록 정보 압축에 문제가 발생하며, 오래 전에 입력된 단어는 잊어버리거나 특정 단어 정보를 과도하게 반영해 전체 정보를 왜곡하는 경우가 자주 생긴다.\\\n","기계 번역을 할 때 RNN을 사용한다면 인코더가 디코더로 넘기는 정보는 소스 시퀀스의 마지막인 \"많더라\" 라는 단어의 의미가 많이 반영될 수밖에 없으며, RNN은 입력 정보를 차례대로 처리하고 오래 전에 읽었던 단어는 잊어버리는 경향이 있다.\n","\n","- 어텐션과 비교 :\\\n","'cafe' 에 대응하는 소스 언어의 단어는 '카페' 이고 이는 소스 시퀀스의 초반부에 등장한 상황에서 cafe라는 단어를 디코딩해야 할 때 카페를 반드시 참조해야 한다.\\\n","어텐션이 없는 단순 RNN을 사용하면 워낙 초반에 입력된 단어라 모델이 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아질 수 있다.\\\n","어텐션은 이러한 문제점을 해결하기 위해 제안되었으며, 디코더 쪽 RNN에 어텐션을 추가하는 방식이다.\\\n","어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 \"어떤 요소에 주목해야 할지\" 알려주므로 카페가 소스 시퀀스 초반에 등장하거나 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있다.\n","\n","- 셀프 어텐션은 자기 자신에 수행하는 어텐션이다.\\\n","입력 시퀀스가 \"어제, 카페, 갔었어, 거기, 사람, 많더라\" 일 때 '거기' 라는 단어가 어떤 의미를 가지는지 계산하는 상황에서 잘 학습된 셀프 어텐션 모델이라면 거기에 대응하는 장소는 '카페' 라는 사실을 알아챌 수 있다.\\\n","그뿐만 아니라 거기는 '갔었어' 와도 연관이 있음을 확인할 수 있다.\\\n","트랜스포머 인코더 블록 내부에서는 이처럼 \"거기\" 라는 단어를 인코딩할 때 '카페', '갔었어' 라는 단어의 의미를 강조해서 반영한다.\n","\n","- 셀프 어텐션 수행 대상은 \"입력 시퀀스 전체\" 이며, 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산을 수행해 문맥 전체를 고려하기 때문에 지역적인 문맥만 보는 CNN 대비 강점이 있다.\\\n","아울러 모든 경우의 수를 고려(단어들 서로가 서로를 1대 1로 바라보게 함)하기 때문에 시퀀스 길이가 길어지더라도 정보를 잊거나 왜곡할 염려가 없다.\\\n","이는 RNN의 단점을 극복한 지점이다.\n","\n","- 어텐션과 셀프 어텐션의 주요 차이\n"," - 어텐션은 소스 시퀀스 전체 단어들(어제, 카페, …, 많더라)과 타깃 시퀀스 단어 하나(cafe) 사이를 연결하는 데 쓰인다.\\\n"," 반면 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결한다.\n","\n"," - 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작한다.\n","타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 셀프어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행한다."],"metadata":{"id":"L4s5QmUf5ssk"}},{"cell_type":"markdown","source":["### 일반화된 셀프 어텐션 : 쿼리-키-값 모델\n","\n","- 셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 세 가지 요소가 서로 영향을 주고 받는 구조이다.\\\n","트랜스포머 블록에는 문장 내 각 단어가 벡터(vector) 형태로 입력되는데 여기서 벡터란 숫자의 나열 정도로 이해할 수 있다.\n","\n","- 각 단어 벡터는 블록 내에서 어떤 계산 과정을 거쳐 쿼리, 키, 밸류 세 가지로 변환된다.\\\n","만일 트랜스포머 블록에 입력되는 문장이 여섯 개 단어로 구성돼 있다면 이 블록의 셀프 어텐션 계산 대상은 쿼리 벡터 6개, 키 벡터 6개, 밸류 백터 6개 등 모두 18개가 된다.\\\n","셀프 어텐션은 쿼리 단어 각각에 대해 모든 키 단어와 얼마나 유기적인 관계를 맺고 있는지 그 합이 1인 '확률값' 으로 나타낸다.\\\n","'카페' 라는 쿼리 단어와 가장 관련이 높은 키 단어는 '거기' 라는 점(0.4)을 확인할 수 있다.\n","\n","- 셀프 어텐션 모듈은 밸류 벡터들을 '가중합(weighted sum)' 하는 방식으로 계산을 마무리한다.\\\n","새롭게 만들어지는 카페 벡터(Z카페)는 문장에 속한 모든 단어 쌍 사이의 관계가 녹아 있다.\\\n","Z카페 = 0.1×V어제 + 0.1×V카페 + 0.1×V갔었어 + 0.4×V거기 + 0.2×V사람 + 0.1×V많더라\n","\n","- 카페에 대해서만 계산 예를 들었지만 이러한 방식으로 나머지 단어들도 셀프 어텐션을 각각 수행한다.\\\n","모드 시퀀스를 대상으로 셀프 어텐션 계산이 끝나면 그 결과를 다음 블록으로 넘긴다.\\\n","이처럼 트랜스포머 모델은 셀프 어텐션을 블록(레이어) 수만큼 반복한다.\n"],"metadata":{"id":"yREzvxKQ6C7F"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r9zTzTbH9p2G","executionInfo":{"status":"ok","timestamp":1673224741619,"user_tz":-540,"elapsed":10764,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"d0d9ead6-9458-467d-fe40-f2db3f9e2311"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NG7znVKU50Uc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 셀프 어텐션 동작 원리\n","\n","셀프 어텐션 출력 과정 : 쿼리, 키, 밸류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n","- 입력 (단어 개수, 단어 임베딩 차원 수)\n","\n","- 쿼리, 키, 밸류 만들기 : 세가지 행렬은 태스크를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트\n","\n","- 셀프 어텐션 계산\n","  - 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n","  - 소프트맥스 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤 소프트맥스를 취하는 과정\n","  - 소프트맥스 확률과 밸류를 가중합하기\n"],"metadata":{"id":"gAfoJ37d94xN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2c-pQLo04Uk"},"outputs":[],"source":["# 변수 정의\n","import torch\n","\n","# 입력 벡터 시퀀스 (단어개수 3, 차원수 4)\n","x = torch.tensor([\n","    [1.0, 0.0, 1.0, 0.0],\n","    [0.0, 2.0, 0.0, 2.0],\n","    [1.0, 1.0, 1.0, 1.0],\n","])\n","\n","# 셀프 어텐션은 쿼리, 키, 밸류 3개 요소 사이의 문맥적 관계성을 추출하는 과정이다\n","# 쿼리, 키, 밸류를 만들어주는 행렬 (w)\n","w_query = torch.tensor([\n","    [1.0, 0.0, 1.0],\n","    [1.0, 0.0, 0.0],\n","    [0.0, 0.0, 1.0],\n","    [0.0, 1.0, 1.0]\n","])\n","\n","w_key = torch.tensor([\n","    [0.0, 0.0, 1.0],\n","    [1.0, 1.0, 0.0],\n","    [0.0, 1.0, 0.0],\n","    [1.0, 1.0, 0.0]\n","])\n","\n","w_value = torch.tensor([\n","    [0.0, 2.0, 0.0],\n","    [0.0, 3.0, 0.0],\n","    [1.0, 0.0, 3.0],\n","    [1.0, 1.0, 0.0]\n","])"]},{"cell_type":"code","source":["# 쿼리, 키, 밸류 만들기\n","# w_key, w_query, w_value 세 행렬은 태스크를 가장 잘 수행할 수 있는 방향으로 학습과정에서 업데이트 된다\n","keys = torch.matmul(x, w_key)\n","querys = torch.matmul(x, w_query)\n","values = torch.matmul(x, w_value)\n","\n","print(keys, '\\n')\n","print(querys, '\\n')\n","print(values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyUFSBMv_nIG","executionInfo":{"status":"ok","timestamp":1673225398620,"user_tz":-540,"elapsed":5,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"242cd729-45ec-43c9-8d04-62a060221942"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 1.],\n","        [4., 4., 0.],\n","        [2., 3., 1.]]) \n","\n","tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]]) \n","\n","tensor([[1., 2., 3.],\n","        [2., 8., 0.],\n","        [2., 6., 3.]])\n"]}]},{"cell_type":"code","source":["# 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n","attn_scores = torch.matmul(querys, keys.T)\n","print(querys, '\\n')\n","print(keys.T, '\\n')\n","attn_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAW6fQmpARFU","executionInfo":{"status":"ok","timestamp":1673226317456,"user_tz":-540,"elapsed":281,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"41fb910e-dfd6-457d-e0a9-dfd90d012260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]]) \n","\n","tensor([[0., 4., 2.],\n","        [1., 4., 3.],\n","        [1., 0., 1.]]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.,  4.,  4.],\n","        [ 4., 16., 12.],\n","        [ 4., 12., 10.]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import numpy as np\n","print(keys, '\\n')\n","print(keys.shape[-1], '\\n')\n","np.sqrt(keys.shape[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbX_aKDlD2rZ","executionInfo":{"status":"ok","timestamp":1673226361386,"user_tz":-540,"elapsed":253,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"40410ff5-6785-4e52-c6f2-6a4896196036"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 1.],\n","        [4., 4., 0.],\n","        [2., 3., 1.]]) \n","\n","3 \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["1.7320508075688772"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# softmax 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤, softmax 를 취하는 과정\n","from torch.nn.functional import softmax\n","key_dim_sqrt = np.sqrt(keys.shape[-1])\n","attn_probs = softmax(attn_scores / key_dim_sqrt, dim=-1)\n","attn_probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IS_z5x23EBbN","executionInfo":{"status":"ok","timestamp":1673226500723,"user_tz":-540,"elapsed":17,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"6252d3c9-452e-4b25-e5c3-6f6ac9dbf990"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n","        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n","        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# softmax 확률과 밸류를 가중합하기\n","weighted_values = torch.matmul(attn_probs, values)\n","print(values, '\\n')\n","weighted_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vMQVBk9eEjh9","executionInfo":{"status":"ok","timestamp":1673226639834,"user_tz":-540,"elapsed":252,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"6f78827e-fccc-4eb6-a418-21f8f99074eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [2., 8., 0.],\n","        [2., 6., 3.]]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.8639, 6.3194, 1.7042],\n","        [1.9991, 7.8141, 0.2735],\n","        [1.9926, 7.4796, 0.7359]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"id":"Dhu_1YkGFFaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 트랜스포머에 적용된 기술\n","- 인코더와 디코더 블록의 구조는 '멀티 헤드 어텐션', '피드포워드 뉴럴 네트워크', '잔차 연결 및 레이어 정규화' 등 세 가지 구성 요소가 기본\n"],"metadata":{"id":"J2Npkkz4Gvpn"}},{"cell_type":"markdown","source":["### * 피드포워드 뉴럴네트워크\n"," - 입력은 현재 블록의 멀티 헤드 어텐션의 개별 출력 벡터\n"," - 피드포워드 뉴럴네트워크란 신경망(neural network)의 한 종류로 입력층(input layer,  x), 은닉층(hidden layer,  h), 출력층(ouput layer,  y)의 3개 계층으로 구성\n","\n"," - 이전 뉴런 값과 그에 해당하는 가중치를 가중합(weighted sum)한 결과에 바이어스(bias)를 더해 만든다.\\\n"," 가중치들과 바이어스는 학습 과정에서 업데이트된다.\n"," \n"," - 활성 함수(activation function,  f)는 현재 계산하고 있는 뉴런의 출력을 일정 범위로 제한하는 역할 (비선형변환을 위함). ReLU(Rectified Linear Unit)\n","\n"," - 입력층 뉴런이 각각 [2,1] 이고 그에 해당하는 가중치가 [3,2] ,바이어스(bias)가 1 이라면,\\\n"," 은닉층 첫번째 뉴런 값은 2×3+1×2+1=9 가 되며 이 값은 양수이므로 ReLU를 통과해도 그대로 유지된다\n","\n"," - ★ 트랜스포머에서는 은닉층의 뉴런 갯수(즉, 은닉층의 차원수)를 입력층의 네 배로 설정한다.\\\n"," 예컨대 피드포워드 뉴럴네트워크의 입력 벡터가 768차원일 경우 은닉층은 2048차원까지 늘렸다가 출력층에서 이를 다시 768차원으로 줄이는 것이다"],"metadata":{"id":"E7MUP7XPh5jM"}},{"cell_type":"markdown","source":["### * 잔차 연결\n"," - 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n"," - 입력을  x , 이번 계산 대상 블록을 F 라고 할 때 잔차 연결은 F(x)+x 로 간단히 실현\n"," - 잔차 연결을 두지 않았을 때는  f1 ,  f2 ,  f3 을 연속으로 수행하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성됨.\\\n"," 다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n"," - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음"],"metadata":{"id":"1qlmgHNLG8Ot"}},{"cell_type":"markdown","source":["### * 레이어 정규화\n"," - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나누어 정규화(normalization) 을 수행하는 기법\n"," - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과가 있다"],"metadata":{"id":"Z3XJEkSaiARN"}},{"cell_type":"markdown","source":["### * 드롭아웃\n"," - 딥러닝 모델은 그 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버릴 염려가 있다.\\\n"," 이를 과적합(overfitting)이라고 하며, 드롭아웃(dropout)은 이러한 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법이다\n"," - torch.nn.Dropout 객체는 뉴런별로 드롭아웃을 수행할지 말지를 확률적으로 결정하는 함수\\\n"," p=0.2라는 말은 드롭아웃 수행 비율이 평균적으로 20%가 되게끔 하는 것"],"metadata":{"id":"vxZZCDNTiEQ9"}},{"cell_type":"markdown","source":["### * 옵티마이저\n"," - 딥러닝 모델 학습은 모델 출력과 정답 사이의 오차(error)를 최소화하는 방향을 구하고 이 방향에 맞춰 모델 전체의 파라미터(parameter)들을 업데이트하는 과정.\\\n"," 이때 오차를 최소화하는 방향을 그레디언트(gradient)라고 하며 오차를 최소화하는 과정을 최적화(optimization)라고 한다\n"," - 오차를 구하려면 현재 시점의 모델에 입력을 넣어서 처음부터 끝까지 계산해보고 정답과 비교해야 하며, 오차를 구하기 위해 이같이 모델 처음부터 끝까지 순서대로 계산해보는 과정을 순전파(forward propagation) 라고 한다.\n"," - 오차를 구했다면 오차를 최소화하는 최초의 그래디언트를 구할 수 있으며, 이는 미분(devative)으로 구한다.\\\n"," 이후 미분의 연쇄 법칙(chain rule)에 따라 모델 각 가중치별 그래디언트 역시 구할 수 있으며, 이 과정은 순전파의 역순으로 순차적으로 수행되는데 이를 역전파(backpropagation)라고 한다\n","\n"," - 학습 과정은 '미니 배치 단위' 로 이루어지며 이는 눈을 가린 상태에서 산등성이를 한걸음씩 내려가는 과정에 비유할 수 있다.\\\n"," 내가 지금 있는 위치에서 360도 모든 방향에 대해 한발한발 내딛어보고 가장 경사가 급한 쪽으로 한걸음씩 내려가는 과정을 반복하는 것이다.\n"," - 모델을 업데이트할 때(산등성이를 내려갈 때) 중요한 것은 방향(기울기)과 보폭(learning rate)인데 이는 최적화 도구(optimizer)의 도움을 받는다.\\\n"," 트랜스포머 모델이 쓰는 최적화 도구가 바로 '아담 옵티마이저(Adam Optimizer)' 이며 아담 옵티마이저는 오차를 줄이는 성능이 좋아서 트랜스포머 말고도 널리 쓰인다.\n"," - 아담 옵티마이저의 핵심 동작 원리는 방향(기울기)과 보폭(learning rate)을 적절하게 정해주는 것이다.\\\n"," 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 한다(모멘텀 기법).\\\n"," 보폭의 경우 안가본 곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정한다.\n"],"metadata":{"id":"RRpJOQyGiHnM"}},{"cell_type":"code","source":[],"metadata":{"id":"nMxC_u6WG46k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 문장을 벡터로 변환하기\n","* BERT 개념 이해\\\n","https://moondol-ai.tistory.com/463\n"],"metadata":{"id":"pg7KknUKQpWe"}},{"cell_type":"code","source":["# 의존성 패키지 설치\n","!pip install ratsnlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FmAr2OY6QuzR","executionInfo":{"status":"ok","timestamp":1673231386798,"user_tz":-540,"elapsed":11758,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"18c1c20c-e55f-4b0d-ea68-d96acffff3bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ratsnlp\n","  Downloading ratsnlp-1.0.52-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from ratsnlp) (1.1.4)\n","Collecting flask-ngrok>=0.0.25\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Collecting Korpora>=0.2.0\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.10.0\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flask-cors>=3.0.10\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Collecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.5/582.5 KB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (6.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (21.3)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.13.0+cu116)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2022.11.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.4.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.9.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.21.6)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2.25.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (3.8.2)\n","Collecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2022.6.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n","Requirement already satisfied: Six in /usr/local/lib/python3.8/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from Korpora>=0.2.0->ratsnlp) (1.2.0)\n","Collecting dataclasses>=0.6\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (3.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.1->ratsnlp) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (4.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.15.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.4.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.51.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.19.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (22.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (4.0.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.2.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=38884376cd1e4b81aeeb66d41ba064c311953c0b19354d4c095a28a3203098b2\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, dataclasses, sacremoses, pyDeprecate, torchmetrics, Korpora, huggingface-hub, transformers, flask-ngrok, flask-cors, pytorch-lightning, ratsnlp\n","Successfully installed Korpora-0.2.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 huggingface-hub-0.11.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 ratsnlp-1.0.52 sacremoses-0.0.53 tokenizers-0.10.3 torchmetrics-0.11.0 transformers-4.10.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses"]}}},"metadata":{}}]},{"cell_type":"code","source":["# 토크나이저 선언 : BERT(kcbert-base) 모델이 쓰는 토크나이저 선언\n","# 오픈소스 Transformers in Huggingface\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    \"beomi/kcbert-base\",\n","    do_lower_case=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["934f6cc022b1491fb7ab7e165b779e21","4b991e74841b477badb544cbbb550c23","c4f058f62bc0456cafa36da61055ec4b","cbad6d98d4694525976ce105ff9ee132","8eae7f35e4b24d86a1956a48fada0dc3","4bb61450ecdb468ba361ee79474ef38c","c36c313c937a43db9bda32108bec0270","9cc2b60988414aa080713e19abbde75d","8f81a0eb6c1a4730979134feb4068f14","121ec3a05e8b4b76bc2e090cf815c467","b99b3ef100fe496f89922f78aada180d","5318c508cdb44b67a2307c2c4da9d619","b046c7eecf594a859fd8e95bdfa6f787","6d3c80b107ad4fb880c57afe475adc9f","a7f6bd41d2694d35942e715d3e234d1c","6bfa0733ba404695a0927aa9402e2e96","8a37e80efd9648a7830a30503cba5dc2","2eef67b68a21404fbf0e264f18fbd517","ee3e7c770b614bdabd83c09a7f4e31b9","e2f47a035667481dbb412915c7c5b2e3","312a006d92b24436b5cac8ef8f772fb2","ddabac975a104475a73577bbe2c1781d","542e57c09aae4ef48e18727b6d54f451","113d83e4cb1e4aafa587339aa596d075","f323cbaca3bf4dc89b20920e3ea613fd","08633d696f374f3583fb7ddf72a1e5b4","d66451c1c12b415998eed3da167613fc","e9f8135c45d2475aa4267b6dd243c342","eceef486ed164fbf8b1895c8c61f83be","789765a7cdeb4d78861935f2f12487be","ad3bfde8753f49babee0ee4182940671","fc4aa1782e514bbc97c85f8c51425de3","325e6fce3959440c92a8d6d1dbcfdfe3"]},"id":"FDxDCUZqXJhV","executionInfo":{"status":"ok","timestamp":1673231483756,"user_tz":-540,"elapsed":5017,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"082693bc-0375-4dd9-d327-40a7d23c1c48"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934f6cc022b1491fb7ab7e165b779e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5318c508cdb44b67a2307c2c4da9d619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542e57c09aae4ef48e18727b6d54f451"}},"metadata":{}}]},{"cell_type":"code","source":["# 모델 선언 : BERT(kcbert-base) 모델을 읽어들임\n","from transformers import BertConfig, BertModel\n","pretrained_model_config = BertConfig.from_pretrained(\n","    \"beomi/kcbert-base\"\n",")\n","\n","model = BertModel.from_pretrained(\n","    \"beomi/kcbert-base\",\n","    config=pretrained_model_config,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["5d30479a31ca4e44b72ea12712e869bb","88a372a41f6e41ff8432c122cce41e13","5d8d0d96abc346f288b225eed794f6d5","907c6e793ead40c99e2df7394b1df3a1","43ac47dc3b9e430e8086a2be7f2301d3","3282ce3a0dc34d2fb633fa1ebe9db7f5","3e7e3ad1bbfb453e915894f35f25ddd2","5651b13b2dfb4e708caca8c749b85935","d7e9056bc1e247bd81182b6448a82472","98bd56734d8940149bb8559d55306887","b0adb4fa6f93405a93ee4e3ee40de279"]},"id":"nyH_i_UJXi1a","executionInfo":{"status":"ok","timestamp":1673231587760,"user_tz":-540,"elapsed":9689,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"d4a32d0f-ecc3-48a1-d8b7-219911088112"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d30479a31ca4e44b72ea12712e869bb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# 내용 확인\n","# 블록(레이어)의 수 12개, 헤드의 수 12개, 어휘집합의 크기 3만개 등의 정보 확인\n","pretrained_model_config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Be00USnRX7E0","executionInfo":{"status":"ok","timestamp":1673231607035,"user_tz":-540,"elapsed":9,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"48c89e2a-611b-4c69-e041-712cb8ccc889"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertConfig {\n","  \"_name_or_path\": \"beomi/kcbert-base\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 300,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# 입력값 만들기\n","sentences = ['안녕하세요', '하이']\n","features = tokenizer(\n","    sentences,\n","    max_length=10,\n","    padding='max_length',\n","    truncation=True,\n",")\n","\n","features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"va1vRPjOYCJ1","executionInfo":{"status":"ok","timestamp":1673232455980,"user_tz":-540,"elapsed":262,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"04525d76-8d55-4c0f-87c2-a9fd59a99b02"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 3, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["* `features` 의 내용을 확인\n"," - 두 개의 입력 문장 각각에 대해 워드피스 토큰화를 수행한 뒤 이를 토큰 인덱스로 변환한 결과가 input_ids 이다\\\n"," BERT 모델은 문장 시작에 CLS, 끝에 SEP라는 스페셜 토큰을 추가하기 때문에 문장 두 개 모두 앞뒤에 이들 토큰에 대응하는 인덱스 2, 3 이 덧붙여져 있음을 볼 수 있다.\n","\n"," - 토큰 최대 길이(max_length)를 10으로 설정하고, 토큰 길이가 이보다 짧으면 최대 길이에 맞게 패딩(0)을 주고(padding=\"max_length\"), 길면 자르는(truncation=True) 것으로 설정해 두었기 때문에 input_ids의 길이는 두 문장 모두 10 인 것을 확인할 수 있다\n"],"metadata":{"id":"tB58l754aU79"}},{"cell_type":"code","source":["features.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1UnofL4Z0iF","executionInfo":{"status":"ok","timestamp":1673232458940,"user_tz":-540,"elapsed":247,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"aad60ccd-c83c-41e6-e9f1-05fec3180112"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["features['input_ids']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0alXbovEZ161","executionInfo":{"status":"ok","timestamp":1673232460551,"user_tz":-540,"elapsed":240,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"e70d174c-dc21-4626-d6ba-06d5fe85bc27"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 3, 0, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["features['attention_mask']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jx3da3HiZ3yd","executionInfo":{"status":"ok","timestamp":1673232462915,"user_tz":-540,"elapsed":5,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"f2529856-1fcb-4e5a-8fe6-d63ba40f5601"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["features['token_type_ids']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4EOWLDquaAe9","executionInfo":{"status":"ok","timestamp":1673232465177,"user_tz":-540,"elapsed":256,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"28be786b-0bd2-4d82-d151-d4f50ababe1b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":[],"metadata":{"id":"yfgn0QpzZTHN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BERT 로 단어/문장 수준 벡터 구하기\n","\n","* BERT 임베딩 추출\\\n","위에서 만든 `features` 를 파이토치 텐서(tensor)로 변환\n","\n"," - 파이토치 모델의 입력값 자료형은 파이토치에서 제공하는 텐서(tensor)여야 한다.\\\n","따라서 코드3에서 만든 파이썬 리스트(list) 형태의 features를 텐서로 변환\n"],"metadata":{"id":"ME0x7S1vZOw9"}},{"cell_type":"code","source":["# 피처를 토치 텐서로 변환\n","features = {k : torch.tensor(v) for k, v in features.items()}\n","features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72u4eT4EZNGO","executionInfo":{"status":"ok","timestamp":1673232475063,"user_tz":-540,"elapsed":410,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"cfe2fd38-3292-4267-a757-ca01ed5b867e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n","         [    2, 15830,     3,     0,     0,     0,     0,     0,     0,     0]]),\n"," 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n"," 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n","         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# BERT 모델 실행 (임베딩 계산하기)\n","# features 를 입력해 계산한다\n","outputs = model(**features)\n","outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swz82Jy2Zmec","executionInfo":{"status":"ok","timestamp":1673233485422,"user_tz":-540,"elapsed":7,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"96459ab0-0026-454e-ab16-cabd9ae500ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6969, -0.8248,  1.7512,  ..., -0.3732,  0.7399,  1.1907],\n","         [-1.4803, -0.4398,  0.9444,  ..., -0.7405, -0.0211,  1.3064],\n","         [-1.4299, -0.5033, -0.2069,  ...,  0.1285, -0.2611,  1.6057],\n","         ...,\n","         [-1.4406,  0.3431,  1.4043,  ..., -0.0565,  0.8450, -0.2170],\n","         [-1.3625, -0.2404,  1.1757,  ...,  0.8876, -0.1054,  0.0734],\n","         [-1.4244,  0.1518,  1.2920,  ...,  0.0245,  0.7572,  0.0080]],\n","\n","        [[ 0.7565, -1.7148,  2.1903,  ..., -0.4291,  0.9469,  0.7394],\n","         [ 1.1048, -1.3408,  1.9444,  ...,  0.3340,  0.2276, -0.5220],\n","         [-0.1317, -0.6982,  1.3086,  ...,  0.4448,  0.0836, -0.6280],\n","         ...,\n","         [ 0.0666, -0.7225,  1.8425,  ...,  1.2435,  1.6184,  0.1339],\n","         [ 0.1781, -0.7823,  1.6892,  ...,  1.1230,  1.5262,  0.1855],\n","         [-0.0183, -0.8602,  2.5182,  ...,  0.9201,  0.3697, -0.7420]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n","        [-0.9349,  0.1990, -0.0390,  ...,  0.5419,  0.0375, -0.9960]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["자연어를 벡터로 바꾼 결과를 임베딩(embedding) 또는 리프레젠테이션(representation)이라고 한다.\\\n","\"안녕하세요\", \"하이!\" 라는 문장은 단어 수준의 벡터 시퀀스로, 문장 수준의 벡터로 변환할 수 있다.\\\n","전자를 단어 수준 임베딩(리프레젠테이션), 후자를 문장 수준 임베딩(리프레젠테이션)이라고 부른다."],"metadata":{"id":"BQoTsGwKfRm2"}},{"cell_type":"code","source":["# BERT 마지막 레이어의 단어 수준 벡터들 확인\n","# 문장 두개에 속한 각각의 토큰(최대길이 10)을 768차원의 벡터로 변환했다는 의미\n","print(outputs.last_hidden_state.shape)\n","outputs.last_hidden_state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uknhuFwubvLz","executionInfo":{"status":"ok","timestamp":1673233488272,"user_tz":-540,"elapsed":7,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"bc042dc5-673b-4ec3-a26d-2fd7e7ba4d61"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 10, 768])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.6969, -0.8248,  1.7512,  ..., -0.3732,  0.7399,  1.1907],\n","         [-1.4803, -0.4398,  0.9444,  ..., -0.7405, -0.0211,  1.3064],\n","         [-1.4299, -0.5033, -0.2069,  ...,  0.1285, -0.2611,  1.6057],\n","         ...,\n","         [-1.4406,  0.3431,  1.4043,  ..., -0.0565,  0.8450, -0.2170],\n","         [-1.3625, -0.2404,  1.1757,  ...,  0.8876, -0.1054,  0.0734],\n","         [-1.4244,  0.1518,  1.2920,  ...,  0.0245,  0.7572,  0.0080]],\n","\n","        [[ 0.7565, -1.7148,  2.1903,  ..., -0.4291,  0.9469,  0.7394],\n","         [ 1.1048, -1.3408,  1.9444,  ...,  0.3340,  0.2276, -0.5220],\n","         [-0.1317, -0.6982,  1.3086,  ...,  0.4448,  0.0836, -0.6280],\n","         ...,\n","         [ 0.0666, -0.7225,  1.8425,  ...,  1.2435,  1.6184,  0.1339],\n","         [ 0.1781, -0.7823,  1.6892,  ...,  1.1230,  1.5262,  0.1855],\n","         [-0.0183, -0.8602,  2.5182,  ...,  0.9201,  0.3697, -0.7420]]],\n","       grad_fn=<NativeLayerNormBackward0>)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# BERT 마지막 레이어의 문서 수준 벡터 확인\n","# 문장 두 개를 각각 768차원의 벡터로 변환\n","# 문서 분류 과제 같이 문장 전체를 벡터 하나로 변환한 뒤 이 벡터에 어떤 계산을 수행하는 태스크에 활용\n","print(outputs.pooler_output.shape)\n","outputs.pooler_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_t56QVXOe6z2","executionInfo":{"status":"ok","timestamp":1673233573696,"user_tz":-540,"elapsed":14,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"9b786742-996d-4f3b-a25d-34c37e39b9ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 768])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n","        [-0.9349,  0.1990, -0.0390,  ...,  0.5419,  0.0375, -0.9960]],\n","       grad_fn=<TanhBackward0>)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"1984gIz7fiT9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 태스크 모듈 만들기\n","- 파인튜닝을 수행하기 위해서는 단어 혹은 문장 수준 임베딩 위에 태스크를 수행하기 위한 작은 모듈을 추가해야한다\n","\n","- 다운스트림 데이터로 프리트레인 마친 BERT와 그 위의 작은 모듈을 포함한 전체 모델을 업데이트하는 과정\n","\n","- 어떤 모듈을 사용할지는 다운스트림 태스크별로 조금씩 다르다"],"metadata":{"id":"dtwPrwEiftR1"}},{"cell_type":"markdown","source":["### * 문장 벡터 활용 : 문서 분류 등\n","- 문장을 워드피스(wordpiece) 로 토큰화한 뒤 앞뒤에 문장 시작과 끝을 알리는 스페셜 토큰 'CLS' 와 'SEP' 를 각각 추가하여 BERT에 입력\n","\n","- 이후 BERT 모델의 마지막 블록(레이어)의 출력 가운데 CLS에 해당하는 벡터를 추출. 트랜스포머 인코더 블록에서는 모든 단어가 서로 영향을 끼치기 때문에 마지막 블록 CLS 벡터는 문장 전체(이 영화 재미없네요)의 의미가 벡터 하나로 응집된 것이라고 할 수 있다.\n","\n","- 이렇게 뽑은 CLS 벡터에 작은 모듈을 하나 추가해, 그 출력이 미리 정해 놓은 범주(예컨대 긍정, 중립, 부정)가 될 확률이 되도록 한다.\n","\n","- BERT와 그 위에 쌓은 작은 모듈을 포함한 전체 모델의 출력이 정답 레이블과 최대한 같아지도록 모델 전체를 업데이트"],"metadata":{"id":"ORnoWBexfyrO"}},{"cell_type":"markdown","source":["### * 단어 벡터 활용 : 개체명 인식 등\n","- 문서 분류는 마지막 블록의 CLS 벡터만을 사용하는 반면, 개체명 인식 같은 과제에서는 마지막 블록의 모든 단어 벡터를 활용\n","\n","- 문서 분류 때와 동일한 방식으로 입력값을 만들고 BERT의 마지막 레이어까지 계산을 수행.\n","\n","- BERT 모델의 마지막 블록(레이어)의 출력은 문장 내 모든 단어에 해당하는 벡터들의 시퀀스가 된다.\n","\n","- 이렇게 뽑은 단어 벡터들 위에 작은 모듈을 각각 추가해, 그 출력이 각 개체명 범주(기관명, 인명, 지명 등)가 될 확률이 되도록 한다.\n","\n","- 학습 과정에서는 BERT와 그 위에 쌓은 각각의 작은 모듈을 포함한 전체 모델의 출력이 정답 레이블과 최대한 같아지도록 모델 전체를 업데이트\n"],"metadata":{"id":"l-rdLC1Mf5Lm"}},{"cell_type":"code","source":[],"metadata":{"id":"D57f1QSUftBt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 230110 Day96"],"metadata":{"id":"GIIq5q2oDFIE"}},{"cell_type":"markdown","source":["## 문서 분류 모델 학습하기\n","- 영화 리뷰 감성 분석 모델 만들기"],"metadata":{"id":"sTC7NA6cQl79"}},{"cell_type":"code","source":["!pip install ratsnlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZ4qWTjEDHo2","executionInfo":{"status":"ok","timestamp":1673318660221,"user_tz":-540,"elapsed":11484,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"ab083b69-0926-4fc9-b2c4-389333265c5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ratsnlp\n","  Downloading ratsnlp-1.0.52-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flask-ngrok>=0.0.25\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Collecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.5/582.5 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flask-cors>=3.0.10\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from ratsnlp) (1.1.4)\n","Collecting Korpora>=0.2.0\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers==4.10.0\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.64.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.21.6)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2022.11.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.9.1)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.4.0)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (6.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (21.3)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.13.0+cu116)\n","Collecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (3.8.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2022.6.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2.25.1)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n","Requirement already satisfied: Six in /usr/local/lib/python3.8/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from Korpora>=0.2.0->ratsnlp) (1.2.0)\n","Collecting dataclasses>=0.6\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (3.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.1->ratsnlp) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.15.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (57.4.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.19.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.4.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.51.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.38.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (22.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (4.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.2.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=072a4229f9e938cbc9d972af651603e40c20bd608e565a87214cfb949dfa9f4b\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, dataclasses, sacremoses, pyDeprecate, torchmetrics, Korpora, huggingface-hub, transformers, flask-ngrok, flask-cors, pytorch-lightning, ratsnlp\n","Successfully installed Korpora-0.2.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 huggingface-hub-0.11.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 ratsnlp-1.0.52 sacremoses-0.0.53 tokenizers-0.10.3 torchmetrics-0.11.0 transformers-4.10.0\n"]}]},{"cell_type":"code","source":["# 구글 드라이브 연동하기\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXL7mJTxDPAk","executionInfo":{"status":"ok","timestamp":1673318676878,"user_tz":-540,"elapsed":11249,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"2be66fa7-8479-444f-e56e-98bdcb7fe785"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["# 모델 하이퍼 파라미터와 저장 위치 등 설정 정보 선언\n","import torch\n","from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n","\n","args = ClassificationTrainArguments(\n","    pretrained_model_name = \"beomi/kcbert-base\",\n","    downstream_corpus_name = \"nsmc\",\n","    downstream_model_dir = \"/gdrive/MyDrive/nlpbook/checkpoint-doccls\",\n","    batch_size=32 if torch.cuda.is_available() else 4,\n","    learning_rate=5e-5,\n","    max_seq_length=128,\n","    epochs=1,\n","    seed=7,\n",")"],"metadata":{"id":"o7_zgbg7Do0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 재현을 위해 랜덤 시드 고정\n","# set_seed() : args 에 지정된 시드로 고정하는 역할\n","from ratsnlp import nlpbook\n","nlpbook.set_seed(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bl95ys5cGOpn","executionInfo":{"status":"ok","timestamp":1673318709397,"user_tz":-540,"elapsed":262,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"fcb63b91-e3fe-49a8-afc4-8be2b8cba9b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["set seed: 7\n"]}]},{"cell_type":"code","source":["# 메세지 출력 등을 위한 logger 설정 (각종 로그들 출력)\n","nlpbook.set_logger(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_F0QdVQfHXA7","executionInfo":{"status":"ok","timestamp":1673318710664,"user_tz":-540,"elapsed":5,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"01758b30-0e72-44ae-b404-b4f883b38118"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='document-classification', downstream_corpus_name='nsmc', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-doccls', max_seq_length=128, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n","INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='document-classification', downstream_corpus_name='nsmc', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-doccls', max_seq_length=128, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n"]}]},{"cell_type":"code","source":["# 실습에 사용할 말뭉치 다운로드\n","# 코랩 환경 로컬의 root_dir(/content/Korpora) 이하에 저장됨\n","from Korpora import Korpora\n","Korpora.fetch(\n","    corpus_name=args.downstream_corpus_name,\n","    root_dir=args.downstream_corpus_root_dir,\n","    force_download=True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIsc9hFpHroj","executionInfo":{"status":"ok","timestamp":1673318715598,"user_tz":-540,"elapsed":1889,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"c4161289-dc2c-4f5f-d34d-89c5f1c5ac85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nsmc] download ratings_train.txt: 14.6MB [00:00, 89.5MB/s]                            \n","[nsmc] download ratings_test.txt: 4.90MB [00:00, 41.8MB/s]                            \n"]}]},{"cell_type":"code","source":["# 토크나이저 준비\n","# kcbert-base 모델이 사용하는 토크나이저 선언\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["2e48c9db774f462dbb00b58e5fb226c7","1b148c46754c48f4bffddacc4fc3f4a1","51e33c80c43d43158dc2cc695a4133af","d241b55b7ec047e69478b8a9e0dc364e","b7222ba1df3842a5bdc047145cdb7219","d173ea19e43742cf9cdabde3dd9a0f46","2e970c7e072a40ab8351be328a1e206b","ac015e9aa9f34f45b7f5164eb38b1b99","0e97483e7d754db185f5e0f2cab29bcb","bbac9f386c54499e9ce92ce81c012135","f7de92947cc1470d90acc8f193641283","0fe5899fd2284fc089e8a67c67a03826","d0443d835bf54eb78ede82402e53dbce","39648524585a41068d8d187dd282c944","65bf40f6e5fe46a2b07f3ef4ee84d521","8c476717d7e049e3bb2684578368a9a7","ac1ef3cab0994210a1704cf494c0494c","23b86b767ec243dbb8b50096baeb2bbc","98942fbd09334dfe9f589e61acbc464f","2272537eb6094a8b994946261a83ed7c","5d00684c06c2471a8c3153d2c1712ed9","c93ea8c12bf94dc89f62c09954c7359b","f225d1bd8fae41958508c98f0c974ec3","a6f0446e15e44fa6aadc684c05998dff","0396814f88d84b28b3995681de3fae6b","11e4480e38374f7bba1b8b70f6093ca0","e38611dc4af44163ae0db87ed7f2531b","4f0aa1db6a2643c88fd62750d9c77c0e","03516010b6d44ddd98a0ff162a9183e3","a6233d2c3c6d4183a2575cc8d96604a7","8349ac4ca8244664b395827ce70064b9","c503ad45251b492aad9acb3576ce14a4","aa0ebfec8faa45a590ff8184b31f04bb"]},"id":"gns4ey3DITwU","executionInfo":{"status":"ok","timestamp":1673318719484,"user_tz":-540,"elapsed":1626,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"6ff6f0dc-9666-4178-f499-c1d0533e8654"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e48c9db774f462dbb00b58e5fb226c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe5899fd2284fc089e8a67c67a03826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f225d1bd8fae41958508c98f0c974ec3"}},"metadata":{}}]},{"cell_type":"markdown","source":["### * 학습데이터 구축 (생성)\n","- 딥러닝 모델을 학습하려면 학습데이터를 배치(batch) 단위로 지속적으로 모델에 공급해 주어야 한다.\\\n","파이토치(PyTorch)에서는 이 역할을 \"데이터 로더(DataLoader)\" 가 수행한다\n","\n","- NsmcCorpus :\\\n","CSV 파일 형식의 NSMC 데이터를 “문장(영화 리뷰) + 레이블(긍정, 부정)” 형태로 읽어들이는 역할\n","\n","- ClassificationDataset : 각 instance를 포함하는 데이터셋 역할을 수행.\n"," - NsmcCorpus와 토크나이저를 품고 있으며, NsmcCorpus가 넘겨준 데이터(문장, 레이블)를 모델이 학습할 수 있는 형태로 가공한다.\n"," - 문장을 토큰화하고 이를 인덱스로 변환하는 한편, 레이블 역시 정수(integer)로 바꿔주는 역할을 한다.\n","\n","- input_ids, attention_mask, token_type_ids 의 길이가 모두 128인 이유는 토큰 기준 최대 길이(max_seq_length)를 args에서 128로 설정해 두었기 때문\n","\n","- input_ids에 패딩 토큰([PAD])의 인덱스에 해당하는 0이 많이 붙어 있음을 확인할 수 있다\n"," - 분석 대상 문장의 토큰 길이가 max_seq_length보다 짧기 때문\n"," - 이보다 긴 문장일 경우 128로 줄인다\n","\n","- attention_mask는 해당 토큰이 패딩 토큰인지(0) 아닌지(1)를 나타내며,\\\n","token_type_ids는 세그먼트(segment) 정보로 기본값은 모두 0으로 넣는다.\\\n","label은 정수로 변환\n","\n","- sampler는 샘플링 방식을 정의\n"," - 데이터 로더는 배치를 만들 때 ClassificationDataset이 들고 있는 전체 인스턴스 가운데 batch_size 갯수만큼을 비복원(replacement=False) 랜덤 추출한다.\n"," - 학습용 데이터 로더와 달리 평가용 데이터 로더는 SequentialSampler를 사용한다.\n"," \n","- SequentialSampler는 batch_size만큼의 갯수만큼을 인스턴스 순서대로 추출하는 역할을 한다. 학습 때 배치 구성은 랜덤으로 하는 것이 좋으나 평가할 때는 평가용 데이터 전체를 사용하기 때문에 굳이 랜덤으로 구성할 이유가 없기 때문에 SequentialSampler를 쓴다.\n","\n","- collate_fn은 이렇게 뽑힌 인스턴스를 배치로 만드는 역할을 하는 함수"],"metadata":{"id":"-oHenAqMJwd8"}},{"cell_type":"code","source":["# 학습 데이터 구축\n","from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n","from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset\n","\n","corpus = NsmcCorpus()\n","train_dataset = ClassificationDataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='train',\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=args.batch_size,\n","    sampler=RandomSampler(train_dataset, replacement=False),\n","    collate_fn=nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6QDKNmoJK52","executionInfo":{"status":"ok","timestamp":1673318780996,"user_tz":-540,"elapsed":56761,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"3cf18008-25d2-4065-f117-51b3ca3ecaf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from dataset file at /content/Korpora/nsmc\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/nsmc\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/nsmc/ratings_train.txt\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/nsmc/ratings_train.txt\n","INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n","INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n","INFO:ratsnlp:tokenize sentences [took 31.295 s]\n","INFO:ratsnlp:tokenize sentences [took 31.295 s]\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 아 더빙.. 진짜 짜증나네요 목소리\n","INFO:ratsnlp:sentence: 아 더빙.. 진짜 짜증나네요 목소리\n","INFO:ratsnlp:tokens: [CLS] 아 더 ##빙 . . 진짜 짜증나네 ##요 목소리 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 아 더 ##빙 . . 진짜 짜증나네 ##요 목소리 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2170, 832, 5045, 17, 17, 7992, 29734, 4040, 10720, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n","INFO:ratsnlp:sentence: 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n","INFO:ratsnlp:tokens: [CLS] 흠 . . . 포 ##스터 ##보고 초딩 ##영화 ##줄 . . . . 오버 ##연기 ##조차 가볍 ##지 않 ##구나 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 흠 . . . 포 ##스터 ##보고 초딩 ##영화 ##줄 . . . . 오버 ##연기 ##조차 가볍 ##지 않 ##구나 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 1\n","INFO:ratsnlp:label: 1\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 3521, 17, 17, 17, 3294, 13069, 8190, 10635, 13796, 4006, 17, 17, 17, 17, 17613, 19625, 9790, 17775, 4102, 2175, 8030, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 3521, 17, 17, 17, 3294, 13069, 8190, 10635, 13796, 4006, 17, 17, 17, 17, 17613, 19625, 9790, 17775, 4102, 2175, 8030, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 너무재밓었다그래서보는것을추천한다\n","INFO:ratsnlp:sentence: 너무재밓었다그래서보는것을추천한다\n","INFO:ratsnlp:tokens: [CLS] 너무 ##재 ##밓 ##었다 ##그래 ##서 ##보는 ##것을 ##추 ##천 ##한다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 너무 ##재 ##밓 ##었다 ##그래 ##서 ##보는 ##것을 ##추 ##천 ##한다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8069, 4089, 7847, 8217, 9791, 4072, 9136, 8750, 4142, 4244, 8008, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8069, 4089, 7847, 8217, 9791, 4072, 9136, 8750, 4142, 4244, 8008, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n","INFO:ratsnlp:sentence: 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n","INFO:ratsnlp:tokens: [CLS] 교도소 이야기 ##구먼 . . 솔직히 재미 ##는 없다 . . 평 ##점 조정 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 교도소 이야기 ##구먼 . . 솔직히 재미 ##는 없다 . . 평 ##점 조정 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 12164, 9089, 9828, 17, 17, 8876, 10827, 4008, 8131, 17, 17, 3288, 4213, 16612, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 12164, 9089, 9828, 17, 17, 8876, 10827, 4008, 8131, 17, 17, 3288, 4213, 16612, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n","INFO:ratsnlp:sentence: 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n","INFO:ratsnlp:tokens: [CLS] 사이 ##몬 ##페 ##그 ##의 익 ##살 ##스런 연기 ##가 돋 ##보 ##였던 영화 ! 스파이 ##더 ##맨 ##에서 늙어 ##보이 ##기만 했던 커 ##스 ##틴 던 ##스트 ##가 너무나도 이뻐 ##보 ##였다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 사이 ##몬 ##페 ##그 ##의 익 ##살 ##스런 연기 ##가 돋 ##보 ##였던 영화 ! 스파이 ##더 ##맨 ##에서 늙어 ##보이 ##기만 했던 커 ##스 ##틴 던 ##스트 ##가 너무나도 이뻐 ##보 ##였다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 1\n","INFO:ratsnlp:label: 1\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8538, 4880, 4335, 4313, 4042, 2452, 4471, 10670, 11219, 4009, 870, 4010, 13043, 9376, 5, 24034, 4356, 4617, 7971, 22878, 11980, 9235, 10129, 3010, 4103, 4713, 834, 8795, 4009, 22110, 23997, 4010, 9827, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8538, 4880, 4335, 4313, 4042, 2452, 4471, 10670, 11219, 4009, 870, 4010, 13043, 9376, 5, 24034, 4356, 4617, 7971, 22878, 11980, 9235, 10129, 3010, 4103, 4713, 834, 8795, 4009, 22110, 23997, 4010, 9827, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/nsmc/cached_train_BertTokenizer_128_nsmc_document-classification [took 22.957 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/nsmc/cached_train_BertTokenizer_128_nsmc_document-classification [took 22.957 s]\n"]}]},{"cell_type":"code","source":["# 테스트 데이터 구축\n","val_dataset = ClassificationDataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='test',\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=args.batch_size,\n","    sampler=SequentialSampler(val_dataset),\n","    collate_fn=nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjhUFfpmOwmH","executionInfo":{"status":"ok","timestamp":1673318808227,"user_tz":-540,"elapsed":19487,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"2be27ead-f8ff-413e-b729-112953104e50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from dataset file at /content/Korpora/nsmc\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/nsmc\n","INFO:ratsnlp:loading test data... LOOKING AT /content/Korpora/nsmc/ratings_test.txt\n","INFO:ratsnlp:loading test data... LOOKING AT /content/Korpora/nsmc/ratings_test.txt\n","INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n","INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n","INFO:ratsnlp:tokenize sentences [took 10.983 s]\n","INFO:ratsnlp:tokenize sentences [took 10.983 s]\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 굳 ㅋ\n","INFO:ratsnlp:sentence: 굳 ㅋ\n","INFO:ratsnlp:tokens: [CLS] 굳 ㅋ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 굳 ㅋ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 1\n","INFO:ratsnlp:label: 1\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 352, 192, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 352, 192, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: GDNTOPCLASSINTHECLUB\n","INFO:ratsnlp:sentence: GDNTOPCLASSINTHECLUB\n","INFO:ratsnlp:tokens: [CLS] G ##D ##N ##TO ##P ##C ##L ##A ##S ##S ##I ##N ##T ##H ##E ##C ##L ##U ##B [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] G ##D ##N ##TO ##P ##C ##L ##A ##S ##S ##I ##N ##T ##H ##E ##C ##L ##U ##B [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 40, 4452, 4581, 25144, 4579, 4881, 4450, 4580, 4985, 4985, 4506, 4581, 4850, 5121, 4451, 4881, 4450, 5167, 4756, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 40, 4452, 4581, 25144, 4579, 4881, 4450, 4580, 4985, 4985, 4506, 4581, 4850, 5121, 4451, 4881, 4450, 5167, 4756, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n","INFO:ratsnlp:sentence: 뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아\n","INFO:ratsnlp:tokens: [CLS] 뭐야 이 평 ##점 ##들은 . . . . 나쁘 ##진 않지만 10 ##점 짜리 ##는 더더욱 아니잖아 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 뭐야 이 평 ##점 ##들은 . . . . 나쁘 ##진 않지만 10 ##점 짜리 ##는 더더욱 아니잖아 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 10691, 2451, 3288, 4213, 7977, 17, 17, 17, 17, 10476, 4153, 15426, 8240, 4213, 21394, 4008, 15616, 13439, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 10691, 2451, 3288, 4213, 7977, 17, 17, 17, 17, 10476, 4153, 15426, 8240, 4213, 21394, 4008, 15616, 13439, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n","INFO:ratsnlp:sentence: 지루하지는 않은데 완전 막장임... 돈주고 보기에는....\n","INFO:ratsnlp:tokens: [CLS] 지 ##루 ##하지는 않은데 완전 막장 ##임 . . . 돈주고 보기에 ##는 . . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 지 ##루 ##하지는 않은데 완전 막장 ##임 . . . 돈주고 보기에 ##는 . . . . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2688, 4532, 16036, 20879, 8357, 15971, 4252, 17, 17, 17, 13900, 25253, 4008, 17, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 2688, 4532, 16036, 20879, 8357, 15971, 4252, 17, 17, 17, 13900, 25253, 4008, 17, 17, 17, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n","INFO:ratsnlp:sentence: 3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??\n","INFO:ratsnlp:tokens: [CLS] 3 ##D ##만 아니었 ##어도 별 다섯 개 줬 ##을텐데 . . 왜 3 ##D ##로 나와서 제 심 ##기를 불편 ##하게 하죠 ? ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 3 ##D ##만 아니었 ##어도 별 다섯 개 줬 ##을텐데 . . 왜 3 ##D ##로 나와서 제 심 ##기를 불편 ##하게 하죠 ? ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:label: 0\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 22, 4452, 4049, 18851, 8194, 1558, 23887, 220, 2648, 9243, 17, 17, 2332, 22, 4452, 4091, 10045, 2545, 2015, 8313, 10588, 8007, 18566, 32, 32, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 22, 4452, 4049, 18851, 8194, 1558, 23887, 220, 2648, 9243, 17, 17, 2332, 22, 4452, 4091, 10045, 2545, 2015, 8313, 10588, 8007, 18566, 32, 32, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/nsmc/cached_test_BertTokenizer_128_nsmc_document-classification [took 7.758 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/nsmc/cached_test_BertTokenizer_128_nsmc_document-classification [took 7.758 s]\n"]}]},{"cell_type":"markdown","source":["### 모델 초기화\n","프리트레인이 완료된 BERT 모델을 읽고, 문서 분류를 수행할 모델을 초기화\n","- BertForSequenceClassification :\\\n","프리트레인을 마친 BERT 모델 위에 문서 분류용 태스크 모듈이 덧붙여진 형태의 모델 클래스\n"],"metadata":{"id":"7lTPIwtCQEdz"}},{"cell_type":"code","source":["# 모델 초기화\n","from transformers import BertConfig, BertForSequenceClassification\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n","    num_labels=corpus.num_labels,\n",")"],"metadata":{"id":"gYycjhWQP9Ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(\n","    args.pretrained_model_name,\n","    config=pretrained_model_config\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["216c7b0827c34dae89fe8d9f03a9c4f0","8699e87857c34b13bab8ee4c7f0830be","08aea8ec8c6b4581bbb90aa103838969","60f31947ae374b4894cfda100134aeb7","e65df6f13687438e8d3b6942e30a43cf","d3e02fe5925f418c9c6f3a83bf23b61a","64cbd799d84343d891764f04907b378a","0024210009f341c69f6aa861cb825113","75958d4ea121435cbb3d60c56518845a","2bca36fccacd4a95b512c92eb77e98f0","ce39154c292e4f139f59805653023771"]},"id":"HNIEmSs7P9GU","executionInfo":{"status":"ok","timestamp":1673318881364,"user_tz":-540,"elapsed":12863,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"b6bc31ce-a93b-47ad-849f-c71f80d41300"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"216c7b0827c34dae89fe8d9f03a9c4f0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 학습 준비 : Task 와 Trainer 준비\n","from ratsnlp.nlpbook.classification import ClassificationTask\n","task = ClassificationTask(model, args)"],"metadata":{"id":"gtTOF8PQQ87D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = nlpbook.get_trainer(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bq0tjZxBRMj7","executionInfo":{"status":"ok","timestamp":1673318890607,"user_tz":-540,"elapsed":9,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"0d2b3370-c11c-4e8b-9255-160a232bfd26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["# 준비한 데이터와 모델로 학습 시작\n","# 학습 결과물(체크포인트)은 미리 연동해둔 구글 드라이브의 준비된 위치 (/gdrive/MyDrive/nlpbook/checkpoint-doccls1)에 저장된다\n","trainer.fit(\n","    task,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["49a1929edf2547c5a8969574f14ec3ac","07c1d99f435f4b4baeffb8641d124bdf","bcead3587651482caac9fbc1e892bc07","7d47edb48dfd4026bd40900d97f0a8ba","b99618507fc049efa7544b64556e3a3f","ae9b66202b5f4a3c977c0675f63bdaf7","ec45412bf15644efb5b8f061401c0dd2","6510aba2ef8d4491a98bde0d774328db","a2e4ed8ea3aa4f3db0f7261ca1786b4a","56dbb824e4b54289a863d5b5fc04a4da","943fb06e12274054a47fcd0f7e1a0f11","56615059717b4001a9124574bfb7a7c3","7952c09e6faf4b8ba06fc12fe0cead85","ad7322f262b1406ca9d662c5e2692cc6","6f3a8e35272044b98f6af45a6e178ef9","473c5ad0a80e480ba3c87782d7be94dd","1ece8a727bc74240901ec83a0cb8cb4a","1e47eb3509f74afa969b892b2285e19c","4e68d8bd89e04de7841c1f1b90058278","bbc1b36e310842938f7f1bd7ae09f194","30de6a3abf8f4cf59683e3c6e6c1eafc","4d42659eef1b41fca5f32ca7e9c33cfd"]},"id":"41AEg3eeRPqj","executionInfo":{"status":"ok","timestamp":1673322102825,"user_tz":-540,"elapsed":3209684,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"01393d4f-df51-413e-a5ed-b9630a308ffd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /gdrive/MyDrive/nlpbook/checkpoint-doccls/lightning_logs\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n","  rank_zero_warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                          | Params\n","--------------------------------------------------------\n","0 | model | BertForSequenceClassification | 108 M \n","--------------------------------------------------------\n","108 M     Trainable params\n","0         Non-trainable params\n","108 M     Total params\n","435.680   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a1929edf2547c5a8969574f14ec3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56615059717b4001a9124574bfb7a7c3"}},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"2NIeViAaWQ3s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습 마친 모델을 실전 투입하기\n","- 영화 리뷰 감성 분석 웹서비스 만들기"],"metadata":{"id":"QigZ1GfbfcDU"}},{"cell_type":"code","source":["!pip install ratsnlp"],"metadata":{"id":"B_J3P2bOWe4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 연동하기\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673322158474,"user_tz":-540,"elapsed":4384,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"a78d4e43-71c8-4b1b-d3dc-73d6e3d431e9","id":"o6dlurSZWe40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","source":["### * 각종 설정\n","모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언\n","\n","- 인자(argument)의 역할과 내용\n"," - pretrained_model_name :\\\n"," 이전 장에서 파인튜닝한 모델이 사용한 프리트레인 마친 언어모델 이름\\\n"," (단 해당 모델은 허깅페이스 라이브러리에 등록되어 있어야 합니다)\n","\n"," - downstream_model_dir : 이전 장에서 파인튜닝한 모델의 체크포인트 저장 위치.\n","\n"," - max_seq_length : 토큰 기준 입력 문장 최대 길이. 아무 것도 입력하지 않으면 128입니다.\n"],"metadata":{"id":"7054pXEdWjTX"}},{"cell_type":"code","source":["# 인퍼런스 설정\n","from ratsnlp.nlpbook.classification import ClassificationDeployArguments\n","args = ClassificationDeployArguments(\n","    pretrained_model_name=\"beomi/kcbert-base\",\n","    downstream_model_dir=\"/gdrive/MyDrive/nlpbook/checkpoint-doccls\",\n","    max_seq_length=128,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GX1FFSORWg9s","executionInfo":{"status":"ok","timestamp":1673322288135,"user_tz":-540,"elapsed":8,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"0d21e0c9-5ff0-403f-8f5c-8af8bcdb6551"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downstream_model_checkpoint_fpath: /gdrive/MyDrive/nlpbook/checkpoint-doccls/epoch=0-val_loss=0.27.ckpt\n"]}]},{"cell_type":"code","source":["# 모델 로딩 : 파인튜닝을 마친 모델과 토크나이저를 읽어들임\n","import torch\n","from transformers import BertConfig, BertForSequenceClassification\n","\n","# 체크포인트 로드\n","fine_tuned_model_ckpt = torch.load(\n","    args.downstream_model_checkpoint_fpath,\n","    map_location=torch.device(\"cpu\")\n",")\n","\n","# 파인튜닝한 모델이 사용한 프리트레인 마친 언어모델의 설정값들을 읽어들일 수 있음\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n","    num_labels=fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",")\n","\n","# 초기화한 BERT 모델에 체크포인트 (fine_tuned_model_ckpt) 를 읽어들이게 된다\n","model = BertForSequenceClassification(pretrained_model_config)   # BERT 모델 초기화\n","model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n","\n","# 모델이 평가모드로 전환, 드롭아웃 등 학습 때만 사용하는 기법들을 무효화하는 역할\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iZmNiR6Wgz8","executionInfo":{"status":"ok","timestamp":1673322426444,"user_tz":-540,"elapsed":3583,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"7f8f0ad1-832f-4719-b783-cd677416beaa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(300, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# 토크나이저 초기화\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"id":"mZEa7hNtWgpt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### * 인퍼런스 함수 선언\n","- 문장(sentence)에 토큰화를 수행한 뒤 input_ids, attention_mask, token_type_ids를 만든다.\n","\n","- 이들 입력값을 파이토치 텐서(tensor) 자료형으로 변환한 뒤 모델에 입력합니다.\n"," - 모델 출력 값(outputs.logits)은 소프트맥스 함수 적용 이전의 로짓(logit) 형태인데요. 여기에 소프트맥스 함수를 써서 모델 출력을 [부정일 확률, 긍정일 확률] 형태의 확률 형태로 바꾼다.\n","\n","- 마지막으로 모델 출력을 약간 후처리하여 예측 확률의 최댓값이 부정 위치일 경우 해당 문장이 부정(positive), 반대의 경우 긍정(positive)이 되도록 pred 값을 만든다.\n"],"metadata":{"id":"nFq19vy2c_4E"}},{"cell_type":"code","source":["def inference_fn(sentence):\n","  inputs = tokenizer(\n","      [sentence],\n","      max_length=args.max_seq_length,\n","      padding='max_length',\n","      truncation=True,\n","  )\n","\n","  with torch.no_grad():   # 가중치 업데이트 X\n","    outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()}) \n","    prob = outputs.logits.softmax(dim=1)\n","    positive_prob = round(prob[0][1].item(), 4)\n","    negative_prob = round(prob[0][0].item(), 4)\n","    pred = \"긍정 (positive)\" if torch.argmax(prob) == 1 else \"부정 (negative)\"\n","\n","  return {\n","      'sentence' : sentence,\n","      'prediction' : pred,\n","      'positive_data' : f\"긍정 {positive_prob}\",\n","      'negative_data' : f\"부정 {negative_prob}\",\n","      'positive_width' : f\"{positive_prob * 100}%\",\n","      'negative_width' : f\"{round(negative_prob * 100, 2)}%\",\n","  }"],"metadata":{"id":"XC8ynuQSY1n1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = '좋아요'\n","inference_fn(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdxBITn3Wglc","executionInfo":{"status":"ok","timestamp":1673322451492,"user_tz":-540,"elapsed":400,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"bef41c93-9471-4700-e7a3-7b83f9d620bd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sentence': '좋아요',\n"," 'prediction': '긍정 (positive)',\n"," 'positive_data': '긍정 0.9732',\n"," 'negative_data': '부정 0.0268',\n"," 'positive_width': '97.32%',\n"," 'negative_width': '2.68%'}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["sentence = '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??'\n","inference_fn(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqtLNBpkWghL","executionInfo":{"status":"ok","timestamp":1673322453975,"user_tz":-540,"elapsed":748,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"86c46f66-6b4d-4cee-d26e-b1627d0a4707"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sentence': '3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??',\n"," 'prediction': '부정 (negative)',\n"," 'positive_data': '긍정 0.0691',\n"," 'negative_data': '부정 0.9309',\n"," 'positive_width': '6.909999999999999%',\n"," 'negative_width': '93.09%'}"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":[],"metadata":{"id":"9ULqZHBEkr8O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### * 웹서비스 만들기 준비\n","`ngrok`은 코랩 로컬에서 실행 중인 웹서비스를 안전하게 외부에서 접근 가능하도록 해주는 도구입니다.\\\n","`ngrok`을 실행하려면 [회원가입](https://dashboard.ngrok.com/signup) 후 [로그인](https://dashboard.ngrok.com/login)을 한 뒤 [이곳](https://dashboard.ngrok.com/get-started/your-authtoken)에 접속해 인증 토큰(authtoken)을 확인해야 합니다.\\\n","예를 들어 확인된 `authtoken`이 `test111`이라면 다음과 같이 실행합니다.\n","\n","```bash\n","!mkdir /root/.ngrok2 && echo \"authtoken: test111\" > /root/.ngrok2/ngrok.yml\n","```\n"],"metadata":{"id":"m-QsUFGDksL8"}},{"cell_type":"code","source":["!mkdir /root/.ngrok2 && echo \"authtoken: 2K7ID8ZcT7kK16cuvUNOrYHgoxc_81tzFhnzDzLHatQ7xguDQ\" > /root/.ngrok2/ngrok.yml"],"metadata":{"id":"eONB57YMk09V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qz-ABYcDlbHc","executionInfo":{"status":"ok","timestamp":1673322466566,"user_tz":-540,"elapsed":347,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"3b518776-e1ad-455d-cd3e-332ea6d77ada"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 68\n","drwx------ 1 root root 4096 Jan 10 03:47 .\n","drwxr-xr-x 1 root root 4096 Jan 10 03:42 ..\n","-r-xr-xr-x 1 root root 1169 Jan  1  2000 .bashrc\n","drwxr-xr-x 1 root root 4096 Jan 10 02:45 .cache\n","drwx------ 1 root root 4096 Jan 10 02:43 .config\n","drwxr-xr-x 5 root root 4096 Jan  5 14:48 .ipython\n","drwx------ 2 root root 4096 Jan  5 14:48 .jupyter\n","drwxr-xr-x 2 root root 4096 Jan  5 14:45 .keras\n","drwxr-xr-x 1 root root 4096 Jan  5 14:48 .local\n","drwxr-xr-x 2 root root 4096 Jan 10 03:47 .ngrok2\n","drwxr-xr-x 4 root root 4096 Jan  5 14:48 .npm\n","drwx------ 3 root root 4096 Jan 10 02:48 .nv\n","-rw-r--r-- 1 root root  148 Aug 17  2015 .profile\n","-r-xr-xr-x 1 root root  254 Jan  1  2000 .tmux.conf\n","-rw-r--r-- 1 root root  165 Jan  5 14:48 .wget-hsts\n"]}]},{"cell_type":"markdown","source":["### * 웹서비스 개시"],"metadata":{"id":"QUbLlwu7n9jc"}},{"cell_type":"code","source":["from ratsnlp.nlpbook.paircls import get_web_service_app\n","app = get_web_service_app(inference_fn)\n","app.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqqj9ud7n8vE","executionInfo":{"status":"ok","timestamp":1673326811340,"user_tz":-540,"elapsed":4338848,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"eea254ec-5f58-4661-a116-80e818429cc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"ratsnlp.nlpbook.classification.deploy\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://fbe2-35-237-28-121.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jlaN2xcSyqG0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 230111 Day97"],"metadata":{"id":"509bDinTK8i-"}},{"cell_type":"markdown","source":["## 문장 쌍 분류 모델 학습하기\n","- 전제와 가설을 검증하는 자연어 추론 모델"],"metadata":{"id":"2ypn02SmLxbG"}},{"cell_type":"code","source":["!pip install ratsnlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UkY7IYpaK-iO","executionInfo":{"status":"ok","timestamp":1673396017183,"user_tz":-540,"elapsed":14686,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"e9de481d-6968-49e2-f1aa-0e447bc15aca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ratsnlp\n","  Downloading ratsnlp-1.0.52-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Korpora>=0.2.0\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flask-ngrok>=0.0.25\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Collecting flask-cors>=3.0.10\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Collecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.5/582.5 KB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from ratsnlp) (1.1.4)\n","Collecting transformers==4.10.0\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2022.11.0)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (21.3)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.13.0+cu116)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.9.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.21.6)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (6.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.4.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2022.6.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (3.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2.25.1)\n","Collecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n","Requirement already satisfied: Six in /usr/local/lib/python3.8/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from Korpora>=0.2.0->ratsnlp) (1.2.0)\n","Collecting dataclasses>=0.6\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (3.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.1->ratsnlp) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.51.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.4.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (57.4.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.38.4)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.19.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (22.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.2.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ddab4f562f13c4cd1d37cb7d73146c99a314963839e83e2504256586c848079b\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, dataclasses, sacremoses, pyDeprecate, torchmetrics, Korpora, huggingface-hub, transformers, flask-ngrok, flask-cors, pytorch-lightning, ratsnlp\n","Successfully installed Korpora-0.2.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 huggingface-hub-0.11.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 ratsnlp-1.0.52 sacremoses-0.0.53 tokenizers-0.10.3 torchmetrics-0.11.0 transformers-4.10.0\n"]}]},{"cell_type":"code","source":["# 구글 드라이브 연동하기\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUzoFi2ILDzX","executionInfo":{"status":"ok","timestamp":1673396049645,"user_tz":-540,"elapsed":12193,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"a114a3a4-ff9a-4893-9c21-c7d6eb99cb00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["# 모델 하이퍼 파라미터와 저장 위치 등 설정 정보 선언\n","import torch\n","from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n","\n","args = ClassificationTrainArguments(\n","    pretrained_model_name = \"beomi/kcbert-base\",\n","    downstream_task_name = \"pair-classification\",\n","    downstream_corpus_name = \"klue-nli\",\n","    downstream_model_dir = \"/gdrive/MyDrive/nlpbook/checkpoint-paircls6\",\n","    batch_size=32 if torch.cuda.is_available() else 4,\n","    learning_rate=5e-5,\n","    max_seq_length=64,\n","    epochs=1,\n","    tpu_cores=0 if torch.cuda.is_available() else 8,\n","    seed=7,\n",")"],"metadata":{"id":"fIrxvoZcLSCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 재현을 위해 랜덤 시드 고정\n","from ratsnlp import nlpbook\n","nlpbook.set_seed(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P3qH5q44MrXW","executionInfo":{"status":"ok","timestamp":1673396511086,"user_tz":-540,"elapsed":3,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"03ffeb53-5958-42ae-df1b-f871f662370d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["set seed: 7\n"]}]},{"cell_type":"code","source":["# 메세지 출력 등을 위한 logger 설정\n","nlpbook.set_logger(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uX1zWWjsNFdu","executionInfo":{"status":"ok","timestamp":1673396537442,"user_tz":-540,"elapsed":5,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"83df8f24-1bf9-400c-f2f1-34a29ca3ec6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='pair-classification', downstream_corpus_name='klue-nli', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-paircls6', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=6, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n","INFO:ratsnlp:Training/evaluation parameters ClassificationTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='pair-classification', downstream_corpus_name='klue-nli', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-paircls6', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=6, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n"]}]},{"cell_type":"code","source":["# 실습에 사용할 말뭉치 다운로드\n","nlpbook.download_downstream_dataset(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5valRClhNLxW","executionInfo":{"status":"ok","timestamp":1673396568106,"user_tz":-540,"elapsed":1800,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"ca789e32-cdf0-4dab-af0c-fbf803030b97"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: 100%|██████████| 12.3M/12.3M [00:00<00:00, 82.3MB/s]\n","Downloading: 100%|██████████| 1.47M/1.47M [00:00<00:00, 66.6MB/s]\n"]}]},{"cell_type":"code","source":["# 토크나이저 준비\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["79e3d220a9674039b2614a6114fc8422","aebf205376b5401389115efbe7de6b44","a6172aa62a1c4db3946202afec43d72f","ba57602191d847169ed5ba9065f51e04","f0de303ac857443399e97c0448996810","0850e9f548054575b21e45ead41f0f24","39012873babb49e5bce113e7969860b2","6dd4f208d1ba42e2bab2a13737b6c4bd","48df0eafa3de4bd0bfa99c386b592f3f","411b434505b1443f896f2455d2d864a4","a92c4a2299524007903ea80ca7eba945","7a105ea417d14a959847a27b82823e37","bbde3f2fa4b047ac8319c4c52744b0ad","2a5ff4a1cb634e34b4623a8dd9566fc1","b46074c6b2f1448ca2c83d3b4cd3df9a","b1053b5c5086459c8c1842fbb704b3f5","d18a377845de400c80bf3cecb0b2dbdd","e0528291bfcc4d3e9052d639f916e1d6","995c25b7889e4fbfbd69843b99bd3b04","ed9ec33d43104347960a7227f9613658","53e5cb6cd1ea475dbfe10a0124cee0e1","b905ab6047794fbc8258f041873ff8df","44a2600aca2c40f6ab2644ddd0c1d606","acb2a2fc428d4f3bb961effae0157479","a6314d34eabe4c479f2f02ddc2272d69","336d2fe6ae324d7a9ca74d09a47d5803","0eb0520728d044a9b4f082ada414bb54","c8ee057eba194b09bdd6ff9e2f898609","edc513c3dfd9486c8440e942616b0c4f","7735f0a771064ff98bb5650740c030cb","6142e0ce613a4a37a44c1c7c02a4ed1a","ee84706c3b3949b99a67dbba159d046a","5912cc9d110e42248b47f69ab1569e2d"]},"id":"iUCwX768NTBt","executionInfo":{"status":"ok","timestamp":1673396658548,"user_tz":-540,"elapsed":10948,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"73e131d3-301b-4d9c-ecf0-183475d9f92a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e3d220a9674039b2614a6114fc8422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a105ea417d14a959847a27b82823e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a2600aca2c40f6ab2644ddd0c1d606"}},"metadata":{}}]},{"cell_type":"code","source":["# 학습 데이터 세트 구축\n","from ratsnlp.nlpbook.paircls import KlueNLICorpus\n","from ratsnlp.nlpbook.classification import ClassificationDataset\n","\n","corpus = KlueNLICorpus()\n","train_dataset = ClassificationDataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='train',\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAB1-Ch-NnDn","executionInfo":{"status":"ok","timestamp":1673396780249,"user_tz":-540,"elapsed":11526,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"1e90bef9-c472-49db-ff1a-7fc65aa29da7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/klue-nli\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/klue-nli/klue_nli_train.json\n","INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n","INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n","INFO:ratsnlp:tokenize sentences [took 8.763 s]\n","INFO:ratsnlp:tokenize sentences [took 8.763 s]\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n","INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 잤다.\n","INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 잤 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: contradiction\n","INFO:ratsnlp:label: contradiction\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 2491, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.\n","INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 소닉붐이 정말 멋있었다.\n","INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 소 ##닉 ##붐 ##이 정말 멋 ##있 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: neutral\n","INFO:ratsnlp:label: neutral\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 1895, 5623, 5969, 4017, 8050, 1348, 4188, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.\n","INFO:ratsnlp:sentence A, B: 100분간 잘껄 그래도 소닉붐땜에 2점준다 + 100분간 자는게 더 나았을 것 같다.\n","INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 100 ##분간 잘 ##껄 그래도 소 ##닉 ##붐 ##땜에 2 ##점 ##준다 [SEP] 100 ##분간 자는 ##게 더 나 ##았을 것 같다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: neutral\n","INFO:ratsnlp:label: neutral\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8327, 15760, 2483, 4260, 8446, 1895, 5623, 5969, 10319, 21, 4213, 10172, 3, 8327, 15760, 15095, 4199, 832, 587, 25331, 258, 8604, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.\n","INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 근처에서 즐길거리 찾기는 어렵습니다.\n","INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 근처에 ##서 즐 ##길 ##거리 찾 ##기는 어렵 ##습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: contradiction\n","INFO:ratsnlp:label: contradiction\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 29671, 4072, 2676, 4583, 8181, 2851, 8189, 9775, 8046, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.\n","INFO:ratsnlp:sentence A, B: 101빌딩 근처에 나름 즐길거리가 많습니다. + 101빌딩 주변에 젊은이들이 즐길거리가 많습니다.\n","INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 10 ##1 ##빌 ##딩 근처에 나름 즐 ##길 ##거리가 많습니다 . [SEP] 10 ##1 ##빌 ##딩 주변에 젊은이들이 즐 ##길 ##거리가 많습니다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: neutral\n","INFO:ratsnlp:label: neutral\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n","INFO:ratsnlp:features: ClassificationFeatures(input_ids=[2, 8240, 4068, 4647, 4389, 29671, 13715, 2676, 4583, 14516, 14617, 17, 3, 8240, 4068, 4647, 4389, 12298, 22790, 2676, 4583, 14516, 14617, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=2)\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.997 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/klue-nli/cached_train_BertTokenizer_64_klue-nli_pair-classification [took 1.997 s]\n"]}]},{"cell_type":"code","source":["# 학습 데이터 로더 구축 (랜덤한 비복원추출)\n","from torch.utils.data import DataLoader, RandomSampler\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size = args.batch_size,\n","    sampler = RandomSampler(train_dataset, replacement=False),\n","    collate_fn = nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"id":"8qRXWwi7OEqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가용 데이터 로더 구축\n","from torch.utils.data import SequentialSampler\n","val_dataset = ClassificationDataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='test',\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=args.batch_size,\n","    sampler=SequentialSampler(val_dataset),   # 학습 데이터는 RandamSampler\n","    collate_fn=nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_a7DCI5rPFVx","executionInfo":{"status":"ok","timestamp":1673397949730,"user_tz":-540,"elapsed":4,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"3a9a163d-31af-403b-bc1e-504ea820388e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.074 s]\n","INFO:ratsnlp:Loading features from cached file /content/Korpora/klue-nli/cached_test_BertTokenizer_64_klue-nli_pair-classification [took 0.074 s]\n"]}]},{"cell_type":"code","source":["# 모델 초기화\n","from transformers import BertConfig, BertForSequenceClassification\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n","    num_labels = corpus.num_labels,\n",")\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    args.pretrained_model_name,\n","    config=pretrained_model_config,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["529582c2f1da48aab65cd13d620a485f","c7554364a5424e04913fa0ca146fbda0","d5e8b7e6486444eeb41db799b934c360","7ab38d840a2e4b1fb7d3d1ec32160e56","1f48ac393c5b4e80a1e85666e86ff07e","3d90fae61c5144a1811ce11c24f823a4","68faa7cd9bbb47ed9682644bf3ae2c7e","4465a28de2be461188e14b634a735b3c","2dc999e3b5104febb71fc9c7c0c10d08","ced1f1ede28e4102b7779688757a2eaf","6c7193de10374a8394a8c7ef7d67d44c"]},"id":"U2gtYahcSaSf","executionInfo":{"status":"ok","timestamp":1673398112340,"user_tz":-540,"elapsed":7870,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"68e04f4e-10a9-4abd-c8e4-8e31e5eba20e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"529582c2f1da48aab65cd13d620a485f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 모델 학습 - 태스크 정의\n","from ratsnlp.nlpbook.classification import ClassificationTask\n","task = ClassificationTask(model, args)\n","\n","# 모델 학습 - 트레이너 정의\n","trainer = nlpbook.get_trainer(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42V7yifWTKdt","executionInfo":{"status":"ok","timestamp":1673398199574,"user_tz":-540,"elapsed":457,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"afcee01f-a7eb-485b-9948-d04a34a0c274"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["# 모델 학습\n","trainer.fit(\n","    task,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["05dae5cf5b194d75b950c73dabff1e8d","a80aec5894144083bace990b27c2673d","e106b0c60fa544d792aec84464d253b3","1503632855b74cd580d67407af069f78","153f2c8d85dc4f80b270d49b9a8e30f3","362286618a114cd6b7178fc655490e89","7cef0b3ca8dc492c8efc523993704e5d","c9e9c61db25d465192e3d7c733c897c6","0852929361a54386a4cd1ed0f7f840fd","edc322f5a8694e498cb8b7f09ac1ecb0","02cb49aeda61412ba06ada4c19220f38","c04c45812d70453fb3495f0855129d80","1a7457f4b59347c5afd51db94c1b041f","1b8aba8282ca4a8297193b57817b4fbd","dde9519b67644169ae2a9822566de3fb","882e992f5e9b4740a0cc54079a50305b","cb26fdc08e9b4310a43f18a512545813","b33c504fbd374baeb6f0a48fde5613d2","f21e298e6f144505970513318ba052ea","a664e66e40834a96a812d49c1b5ee26a","43b24d302ee44d69bd26f3a0f7334e77","da704c31f95c45e6801adafc89e40f53","e5a34d4a54f44fff83b6766d290fa0b4","311a9a4428f24d708aceb6c4e017c2df","047674a91b72438b9e6e87cc7a4ecf73","0a96b6b56fd84bb2adc178ecbf68ae9a","cbd92a6407bc41a091fd3d458dcb48e2","eddb67708d14440cb8b84c723b3369b9","f267f04e1da94a27ab1bd16d1cd94b00","2264b67496e142329a7f9499f518c3e0","b8bad534e5da44d6b655efd16f1f9e8b","83e1f4220fca4f44a114eeb740e834a3","75d65de116044d4d992da7463c51e1a0","31760ac6ae2d4ae5a43e715bea024d39","8bccefaa7a634db4b6452dbdcc819b09","91a073fa6afb406da5803bceccec9bd0","fe94d444a61241748e528fed91b36d71","f7affb8f77004a86baf840aec4a7a5e8","63f403b4715a4dc88b6e123a55fd50e9","25c0b83f21104afca81c690657063262","ed674a4a73a442b69a3727bb6a0209ab","1e079b6a74c847dc9b87d079c102a888","6670bb71b81244dfb0e8f09718da331f","dd1b46e97efe444eb7b549a36521a5ef","b96f900d41c04e86bfa06b41bab2a7bb","b289b436d47c4be585505cc399fc6208","8ee166b3817343f2b8c66348a0f5b9aa","63532f42a4d64f1bb3a57b229ba29056","15d0bb2988594f4aa512205b24c6a342","db61108877a847a98fc9c0e696ac6c4f","604ec591961a414788b9c7dbb22960e4","34f60ac65a1b40118f12fdf688b944d5","1670ac2063d64669a10ad56e8649ff83","605d935b3cf748678615570d43564ae5","60fe8e98bf0c4301ab4427f3492b9732","9294030e511047e28c0abfc18c6155a5","8f1a010dd7c74861841002977e3853ae","84b828f4b97241b5babf5d824dd78674","a4d76bac04de4b7e93452d4ae9cf8566","b3aadb6531fd4e75b3368dc17726b938","84d83c7a165048f8b7126c38b828aabc","c5863e4e7b324fbb8e259a0929956393","e1ba515e43074f82961fd8fdac6347f6","ab700f2e0b7e4a488591636722d7c73c","5511c7d3d9014dd693e150e86038e518","daefb6c8fc81441a99150b086f213775","1cfc9a923eab4c54825f5fdeb372a39a","632cd397c5f84b5a9dfa8c911019a004","36a1557fbd7a406cba5fef8c143b44e6","1d599bf4d6984c118536ccde1f94f39f","1450d2a858ec471ba246d6b4a43dcf5f","257a54e74af3403580209ea3087d6467","e623b1611511498c924e456b385e689c","1d744aef0eef422084c225d6b0e28bae","0805cd0802fd4ef480f4dc2e603be9fe","3575f33f22944956889d5da613a33ba9","cd274677499a425fbfcb5f435e2cb000"]},"id":"fz0Fjvq5TZxm","executionInfo":{"status":"ok","timestamp":1673400942215,"user_tz":-540,"elapsed":1658202,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"fe2b9734-0a7b-4517-8564-173c540a0135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /gdrive/MyDrive/nlpbook/checkpoint-paircls6/lightning_logs\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n","  rank_zero_warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                          | Params\n","--------------------------------------------------------\n","0 | model | BertForSequenceClassification | 108 M \n","--------------------------------------------------------\n","108 M     Trainable params\n","0         Non-trainable params\n","108 M     Total params\n","435.683   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05dae5cf5b194d75b950c73dabff1e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c04c45812d70453fb3495f0855129d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5a34d4a54f44fff83b6766d290fa0b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31760ac6ae2d4ae5a43e715bea024d39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96f900d41c04e86bfa06b41bab2a7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9294030e511047e28c0abfc18c6155a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cfc9a923eab4c54825f5fdeb372a39a"}},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"yjHyjnvHXmnO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 학습을 마친 모델을 실전 투입하기\n","- 전제와 가설을 검증하는 웹서비스 만들기"],"metadata":{"id":"Egu36xa1YB6-"}},{"cell_type":"code","source":["# 환경 설정하기 - 의존성 패키지 설치\n","#!pip install ratsnlp"],"metadata":{"id":"OxoUwI4PYIGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 연동하기\n","#from google.colab import drive\n","#drive.mount('/gdrive', force_remount=True)"],"metadata":{"id":"AuVsSNUmZfUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인퍼런스 설정\n","from ratsnlp.nlpbook.classification import ClassificationDeployArguments\n","args = ClassificationDeployArguments(\n","    pretrained_model_name=\"beomi/kcbert-base\",\n","    downstream_model_dir=\"/gdrive/MyDrive/nlpbook/checkpoint-paircls6\",\n","    max_seq_length=64,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3O0gBggZa18","executionInfo":{"status":"ok","timestamp":1673401171727,"user_tz":-540,"elapsed":5,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"4024aba0-876d-44bd-9ac7-c69b4b0d313d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downstream_model_checkpoint_fpath: /gdrive/MyDrive/nlpbook/checkpoint-paircls6/epoch=1-val_loss=0.82.ckpt\n"]}]},{"cell_type":"code","source":["# 토크나이저 로드\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"id":"4xei-oCwZ-iN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 체크포인트 로드\n","import torch\n","fine_tuned_model_ckpt = torch.load(\n","    args.downstream_model_checkpoint_fpath,\n","    map_location = torch.device('cpu'),\n",")"],"metadata":{"id":"XwkDL2wZZ-cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT 설정 로드\n","from transformers import BertConfig\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n","    num_labels=fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",")"],"metadata":{"id":"2F1ke-yMZ-aR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT 모델 초기화\n","from transformers import BertForSequenceClassification\n","model = BertForSequenceClassification(pretrained_model_config)"],"metadata":{"id":"H78ywPt2Z-YC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 체크포인트 주입하기\n","model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LVlAHTt9Z-Vs","executionInfo":{"status":"ok","timestamp":1673401202750,"user_tz":-540,"elapsed":8,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"40075747-3e65-4ac0-8aed-8224276c6424"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# 평가모드로 전환\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaAFh5okbe2n","executionInfo":{"status":"ok","timestamp":1673401205078,"user_tz":-540,"elapsed":7,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"afaa5a93-0bc5-49c5-b645-0242edbc1ae6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(300, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# 모델 출력값 만들고 후처리하기 - 인퍼런스 함수\n","def inference_fn(premise, hypothesis):\n","  # 전제와 가설을 모델 입력값으로 만들기\n","  inputs = tokenizer(\n","      [(premise, hypothesis)],\n","       max_length=args.max_seq_length,\n","       padding='max_length',\n","       truncation=True,\n","  )\n","  with torch.no_grad():\n","    # inputs 를 파이토치 텐서로 바꾸기\n","    # 모델 계산하기\n","    outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()})\n","    \n","    # 로짓에 소프트맥스 취하기\n","    prob = outputs.logits.softmax(dim=1)\n","\n","    # 확률을 소수점 두 자리에서 반올림\n","    entailment_prob = round(prob[0][0].item(), 2)\n","    contradiction_prob = round(prob[0][1].item(), 2)\n","    neutral_prob = round(prob[0][2].item(), 2)\n","\n","    # 예측 확률의 최대값 위치에 따라 pred 만들기\n","    if torch.argmax(prob) == 0:\n","      pred = \"참 (entailment)\"\n","    elif torch.argmax(prob) == 1:\n","      pred = \"거짓 (contradiction)\"\n","    else:\n","      pred = \"중립 (neutral)\"\n","\n","    return {\n","        'premise' : premise,\n","        'hypothesis' : hypothesis,\n","        'prediction' : pred,\n","        'entailment_data' : f\"참 {entailment_prob}\",\n","        'contradiction_data' : f\"거짓 {contradiction_prob}\",\n","        'neutral_data' : f\"중립 {neutral_prob}\",\n","        'entailment_width' : f\"{entailment_prob * 100}%\",\n","        'contradiction_width' : f\"{contradiction_prob * 100}%\",\n","        'neutral_width' : f\"{neutral_prob * 100}%\",\n","    }"],"metadata":{"id":"kCSmM0bobe0A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["premise = '오늘은 수요일이다'\n","hypothesis = '내일은 목요일이다'\n","inference_fn(premise, hypothesis)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhw48iaxbexW","executionInfo":{"status":"ok","timestamp":1673401219583,"user_tz":-540,"elapsed":1107,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"98687eb4-6608-4185-85bd-c083a77b0bcf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'premise': '오늘은 수요일이다',\n"," 'hypothesis': '내일은 목요일이다',\n"," 'prediction': '중립 (neutral)',\n"," 'entailment_data': '참 0.09',\n"," 'contradiction_data': '거짓 0.34',\n"," 'neutral_data': '중립 0.57',\n"," 'entailment_width': '9.0%',\n"," 'contradiction_width': '34.0%',\n"," 'neutral_width': '56.99999999999999%'}"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### 웹서비스 만들기"],"metadata":{"id":"6z6GpxW6ikBI"}},{"cell_type":"code","source":["!rm -rf /root/.ngrok2/"],"metadata":{"id":"DgJBEuc855lJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"id":"dNe5L80255lP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir /root/.ngrok2 && echo \"authtoken: 2K7ID8ZcT7kK16cuvUNOrYHgoxc_81tzFhnzDzLHatQ7xguDQ\" > /root/.ngrok2/ngrok.yml"],"metadata":{"id":"3-dshj4aijcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"id":"33eOgiLDijcl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 웹서비스\n","from ratsnlp.nlpbook.paircls import get_web_service_app\n","app = get_web_service_app(inference_fn)\n","app.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xtQrnvNbeun","executionInfo":{"status":"ok","timestamp":1673402020147,"user_tz":-540,"elapsed":62842,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"17001732-39c6-4e0e-f774-b210df4c2ab7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"ratsnlp.nlpbook.classification.deploy\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://0a3c-34-126-175-160.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [11/Jan/2023 01:52:51] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Jan/2023 01:52:52] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b4Jne28Cberv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 개체명 인식 모델 학습하기"],"metadata":{"id":"gCCw4c9Hl32-"}},{"cell_type":"code","source":["# 환경 설정하기 - 의존성 패키지 설치\n","#!pip install ratsnlp"],"metadata":{"id":"sodW7Mp_mIuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 구글 드라이브 연동하기\n","#from google.colab import drive\n","#drive.mount('/gdrive', force_remount=True)"],"metadata":{"id":"tCCj3FFLmIuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 하이퍼 파라미터와 저장 위치 등 설정 정보 선언\n","import torch\n","from ratsnlp.nlpbook.ner import NERTrainArguments\n","\n","args = NERTrainArguments(\n","    pretrained_model_name = \"beomi/kcbert-base\",\n","    downstream_corpus_name = \"ner\",\n","    downstream_model_dir = \"/gdrive/MyDrive/nlpbook/checkpoint-ner\",\n","    batch_size=32 if torch.cuda.is_available() else 4,\n","    learning_rate=5e-5,\n","    max_seq_length=64,\n","    epochs=1,\n","    tpu_cores=0 if torch.cuda.is_available() else 8,\n","    seed=7,\n",")"],"metadata":{"id":"I2_rnQ2amiOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 재현을 위해 랜덤 시드 고정\n","from ratsnlp import nlpbook\n","nlpbook.set_seed(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403467710,"user_tz":-540,"elapsed":1250,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"e1bf041e-c8dc-43b8-9e06-380196ad4a13","id":"vKkJG97nmiOJ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["set seed: 7\n"]}]},{"cell_type":"code","source":["# 메세지 출력 등을 위한 logger 설정\n","nlpbook.set_logger(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403474810,"user_tz":-540,"elapsed":1404,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"3f8ecb6f-6ad4-446c-fe1f-d5f952a8720a","id":"xXASqemQmiOK"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Training/evaluation parameters NERTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='named-entity-recognition', downstream_corpus_name='ner', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-ner', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n","INFO:ratsnlp:Training/evaluation parameters NERTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='named-entity-recognition', downstream_corpus_name='ner', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-ner', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n","INFO:ratsnlp:Training/evaluation parameters NERTrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_task_name='named-entity-recognition', downstream_corpus_name='ner', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-ner', max_seq_length=64, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, cpu_workers=4, fp16=False, tpu_cores=0)\n"]}]},{"cell_type":"code","source":["# 실습에 사용할 말뭉치 다운로드\n","nlpbook.download_downstream_dataset(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403489685,"user_tz":-540,"elapsed":2806,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"64572b73-7eaa-45ed-fc3b-944c066e8022","id":"IDNQO8vlmiOL"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: 100%|██████████| 17.9M/17.9M [00:00<00:00, 84.2MB/s]\n","Downloading: 100%|██████████| 1.13M/1.13M [00:00<00:00, 51.4MB/s]\n"]}]},{"cell_type":"code","source":["# 토크나이저 준비\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"id":"Ow3lgO88miOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 데이터 세트 구축\n","from ratsnlp.nlpbook.ner import NERCorpus, NERDataset\n","\n","corpus = NERCorpus(args)\n","train_dataset = NERDataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='train',\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403616430,"user_tz":-540,"elapsed":39134,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"be4ad148-bbcc-46e0-f10c-9249b274c886","id":"KSbfRza8miOM"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from dataset file at /content/Korpora/ner\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/ner\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/ner\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/ner/train.txt\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/ner/train.txt\n","INFO:ratsnlp:loading train data... LOOKING AT /content/Korpora/ner/train.txt\n","INFO:ratsnlp:processing NER tag dictionary...\n","INFO:ratsnlp:processing NER tag dictionary...\n","INFO:ratsnlp:processing NER tag dictionary...\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 이어 옆으로 움직여 김일성의 오른쪽에서 한 차례씩 두 번 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n","INFO:ratsnlp:sentence: 이어 옆으로 움직여 김일성의 오른쪽에서 한 차례씩 두 번 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n","INFO:ratsnlp:sentence: 이어 옆으로 움직여 김일성의 오른쪽에서 한 차례씩 두 번 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n","INFO:ratsnlp:target: 이어 옆으로 움직여 <김일성:PER>의 오른쪽에서 <한 차례:NOH>씩 <두 번:NOH> 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n","\n","INFO:ratsnlp:target: 이어 옆으로 움직여 <김일성:PER>의 오른쪽에서 <한 차례:NOH>씩 <두 번:NOH> 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n","\n","INFO:ratsnlp:target: 이어 옆으로 움직여 <김일성:PER>의 오른쪽에서 <한 차례:NOH>씩 <두 번:NOH> 상체를 굽혀 조문했으며 이윽고 안경을 벗고 손수건으로 눈주위를 닦기도 했다.\n","\n","INFO:ratsnlp:tokens: [CLS] 이어 옆 ##으로 움직 ##여 김일성 ##의 오른 ##쪽에서 한 차례 ##씩 두 번 상 ##체를 굽 ##혀 조문 ##했 ##으며 이 ##윽 ##고 안 ##경을 벗고 손 ##수 ##건으로 눈 ##주 ##위를 닦 ##기도 했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 이어 옆 ##으로 움직 ##여 김일성 ##의 오른 ##쪽에서 한 차례 ##씩 두 번 상 ##체를 굽 ##혀 조문 ##했 ##으며 이 ##윽 ##고 안 ##경을 벗고 손 ##수 ##건으로 눈 ##주 ##위를 닦 ##기도 했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 이어 옆 ##으로 움직 ##여 김일성 ##의 오른 ##쪽에서 한 차례 ##씩 두 번 상 ##체를 굽 ##혀 조문 ##했 ##으며 이 ##윽 ##고 안 ##경을 벗고 손 ##수 ##건으로 눈 ##주 ##위를 닦 ##기도 했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O B-PER O O O B-NOH I-NOH O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O B-PER O O O B-NOH I-NOH O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O B-PER O O O B-NOH I-NOH O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 10704, 2287, 7965, 10598, 4327, 10819, 4042, 11790, 17431, 3354, 16729, 4679, 917, 1530, 1801, 9678, 359, 4443, 23831, 4062, 9511, 2451, 5953, 4034, 2173, 19033, 19778, 1898, 4110, 29483, 721, 4043, 10327, 788, 8517, 9212, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 5, 4, 4, 4, 6, 16, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 10704, 2287, 7965, 10598, 4327, 10819, 4042, 11790, 17431, 3354, 16729, 4679, 917, 1530, 1801, 9678, 359, 4443, 23831, 4062, 9511, 2451, 5953, 4034, 2173, 19033, 19778, 1898, 4110, 29483, 721, 4043, 10327, 788, 8517, 9212, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 5, 4, 4, 4, 6, 16, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 10704, 2287, 7965, 10598, 4327, 10819, 4042, 11790, 17431, 3354, 16729, 4679, 917, 1530, 1801, 9678, 359, 4443, 23831, 4062, 9511, 2451, 5953, 4034, 2173, 19033, 19778, 1898, 4110, 29483, 721, 4043, 10327, 788, 8517, 9212, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 5, 4, 4, 4, 6, 16, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 제철과일리코타치즈샐러드는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n","INFO:ratsnlp:sentence: 제철과일리코타치즈샐러드는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n","INFO:ratsnlp:sentence: 제철과일리코타치즈샐러드는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n","INFO:ratsnlp:target: <제철과일리코타치즈샐러드:POH>는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n","\n","INFO:ratsnlp:target: <제철과일리코타치즈샐러드:POH>는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n","\n","INFO:ratsnlp:target: <제철과일리코타치즈샐러드:POH>는 직접 만든 쫀쫀한 치즈도 맛있지만, 영귤청드레싱이 상큼함을 더한다.\n","\n","INFO:ratsnlp:tokens: [CLS] 제 ##철 ##과 ##일 ##리 ##코 ##타 ##치 ##즈 ##샐 ##러 ##드는 직접 만든 쫀 ##쫀 ##한 치 ##즈 ##도 맛 ##있지만 , 영 ##귤 ##청 ##드 ##레 ##싱 ##이 상 ##큼 ##함을 더한 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 제 ##철 ##과 ##일 ##리 ##코 ##타 ##치 ##즈 ##샐 ##러 ##드는 직접 만든 쫀 ##쫀 ##한 치 ##즈 ##도 맛 ##있지만 , 영 ##귤 ##청 ##드 ##레 ##싱 ##이 상 ##큼 ##함을 더한 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 제 ##철 ##과 ##일 ##리 ##코 ##타 ##치 ##즈 ##샐 ##러 ##드는 직접 만든 쫀 ##쫀 ##한 치 ##즈 ##도 맛 ##있지만 , 영 ##귤 ##청 ##드 ##레 ##싱 ##이 상 ##큼 ##함을 더한 ##다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2545, 4748, 4128, 4046, 4038, 4599, 4361, 4077, 4146, 7035, 4053, 8609, 9099, 8634, 2771, 6003, 4047, 2972, 4146, 4029, 1306, 25974, 15, 2282, 5376, 4190, 4273, 4306, 4097, 4017, 1801, 4582, 11091, 11554, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2545, 4748, 4128, 4046, 4038, 4599, 4361, 4077, 4146, 7035, 4053, 8609, 9099, 8634, 2771, 6003, 4047, 2972, 4146, 4029, 1306, 25974, 15, 2282, 5376, 4190, 4273, 4306, 4097, 4017, 1801, 4582, 11091, 11554, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2545, 4748, 4128, 4046, 4038, 4599, 4361, 4077, 4146, 7035, 4053, 8609, 9099, 8634, 2771, 6003, 4047, 2972, 4146, 4029, 1306, 25974, 15, 2282, 5376, 4190, 4273, 4306, 4097, 4017, 1801, 4582, 11091, 11554, 4020, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 7, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 정씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n","INFO:ratsnlp:sentence: 정씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n","INFO:ratsnlp:sentence: 정씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n","INFO:ratsnlp:target: <정:PER>씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n","\n","INFO:ratsnlp:target: <정:PER>씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n","\n","INFO:ratsnlp:target: <정:PER>씨는 “사고 예측을 위한 빅데이터나 전자 항해 등 그동안 알지 못했던 분야에 대해 배울 수 있는 기회였다”며 “새로운 교육이 재취업에 많은 도움이 됐다”고 말했다.\n","\n","INFO:ratsnlp:tokens: [CLS] 정 ##씨는 [UNK] 사고 예측 ##을 위한 빅 ##데이 ##터 ##나 전자 항 ##해 등 그동안 알지 못했 ##던 분야 ##에 대해 배울 수 있는 기회 ##였다 [UNK] 며 [UNK] 새로운 교육이 재 ##취업 ##에 많은 도움이 됐다 [UNK] 고 말했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 정 ##씨는 [UNK] 사고 예측 ##을 위한 빅 ##데이 ##터 ##나 전자 항 ##해 등 그동안 알지 못했 ##던 분야 ##에 대해 배울 수 있는 기회 ##였다 [UNK] 며 [UNK] 새로운 교육이 재 ##취업 ##에 많은 도움이 됐다 [UNK] 고 말했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 정 ##씨는 [UNK] 사고 예측 ##을 위한 빅 ##데이 ##터 ##나 전자 항 ##해 등 그동안 알지 못했 ##던 분야 ##에 대해 배울 수 있는 기회 ##였다 [UNK] 며 [UNK] 새로운 교육이 재 ##취업 ##에 많은 도움이 됐다 [UNK] 고 말했다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-PER O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2539, 10786, 1, 8472, 16843, 4027, 8717, 1665, 19545, 4025, 4136, 12116, 3370, 4032, 963, 8996, 10630, 22474, 4217, 16029, 4113, 9305, 17534, 1931, 8032, 8993, 9827, 1, 1363, 1, 10794, 21266, 2499, 21150, 4113, 8298, 11439, 14054, 1, 303, 19646, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2539, 10786, 1, 8472, 16843, 4027, 8717, 1665, 19545, 4025, 4136, 12116, 3370, 4032, 963, 8996, 10630, 22474, 4217, 16029, 4113, 9305, 17534, 1931, 8032, 8993, 9827, 1, 1363, 1, 10794, 21266, 2499, 21150, 4113, 8298, 11439, 14054, 1, 303, 19646, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2539, 10786, 1, 8472, 16843, 4027, 8717, 1665, 19545, 4025, 4136, 12116, 3370, 4032, 963, 8996, 10630, 22474, 4217, 16029, 4113, 9305, 17534, 1931, 8032, 8993, 9827, 1, 1363, 1, 10794, 21266, 2499, 21150, 4113, 8298, 11439, 14054, 1, 303, 19646, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: ―효진 역의 김환희(14)가 특히 인상적이었다.\n","INFO:ratsnlp:sentence: ―효진 역의 김환희(14)가 특히 인상적이었다.\n","INFO:ratsnlp:sentence: ―효진 역의 김환희(14)가 특히 인상적이었다.\n","INFO:ratsnlp:target: ―<효진:PER> 역의 <김환희:PER>(<14:NOH>)가 특히 인상적이었다.\n","\n","INFO:ratsnlp:target: ―<효진:PER> 역의 <김환희:PER>(<14:NOH>)가 특히 인상적이었다.\n","\n","INFO:ratsnlp:target: ―<효진:PER> 역의 <김환희:PER>(<14:NOH>)가 특히 인상적이었다.\n","\n","INFO:ratsnlp:tokens: [CLS] [UNK] 효 ##진 역 ##의 김 ##환 ##희 ( 14 ) 가 특히 인상 ##적이 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] [UNK] 효 ##진 역 ##의 김 ##환 ##희 ( 14 ) 가 특히 인상 ##적이 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] [UNK] 효 ##진 역 ##의 김 ##환 ##희 ( 14 ) 가 특히 인상 ##적이 ##었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O B-PER I-PER O O B-PER I-PER I-PER O B-NOH O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O B-PER I-PER O O B-PER I-PER I-PER O B-NOH O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O B-PER I-PER O O B-PER I-PER I-PER O B-NOH O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 1, 3476, 4153, 2270, 4042, 420, 4185, 4346, 11, 11524, 12, 197, 9250, 11662, 8805, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 5, 15, 4, 4, 5, 15, 15, 4, 6, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 1, 3476, 4153, 2270, 4042, 420, 4185, 4346, 11, 11524, 12, 197, 9250, 11662, 8805, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 5, 15, 4, 4, 5, 15, 15, 4, 6, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 1, 3476, 4153, 2270, 4042, 420, 4185, 4346, 11, 11524, 12, 197, 9250, 11662, 8805, 8217, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 5, 15, 4, 4, 5, 15, 15, 4, 6, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 전문가들은 미국 증시의 상승세가 유지되고 ‘트럼프노믹스’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n","INFO:ratsnlp:sentence: 전문가들은 미국 증시의 상승세가 유지되고 ‘트럼프노믹스’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n","INFO:ratsnlp:sentence: 전문가들은 미국 증시의 상승세가 유지되고 ‘트럼프노믹스’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n","INFO:ratsnlp:target: 전문가들은 <미국:ORG> 증시의 상승세가 유지되고 ‘<트럼프노믹스:POH>’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n","\n","INFO:ratsnlp:target: 전문가들은 <미국:ORG> 증시의 상승세가 유지되고 ‘<트럼프노믹스:POH>’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n","\n","INFO:ratsnlp:target: 전문가들은 <미국:ORG> 증시의 상승세가 유지되고 ‘<트럼프노믹스:POH>’에 대한 불확실성이 걷히면 국내 증시도 박스권 탈출을 시도할 수 있다는 분석을 내놓았다.\n","\n","INFO:ratsnlp:tokens: [CLS] 전문가들 ##은 미국 증 ##시 ##의 상승 ##세가 유지 ##되고 [UNK] 트럼프 ##노 ##믹 ##스 [UNK] 에 대한 불 ##확 ##실 ##성이 걷 ##히면 국내 증 ##시도 박 ##스 ##권 탈출 ##을 시도 ##할 수 있다는 분석 ##을 내놓 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 전문가들 ##은 미국 증 ##시 ##의 상승 ##세가 유지 ##되고 [UNK] 트럼프 ##노 ##믹 ##스 [UNK] 에 대한 불 ##확 ##실 ##성이 걷 ##히면 국내 증 ##시도 박 ##스 ##권 탈출 ##을 시도 ##할 수 있다는 분석 ##을 내놓 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 전문가들 ##은 미국 증 ##시 ##의 상승 ##세가 유지 ##되고 [UNK] 트럼프 ##노 ##믹 ##스 [UNK] 에 대한 불 ##확 ##실 ##성이 걷 ##히면 국내 증 ##시도 박 ##스 ##권 탈출 ##을 시도 ##할 수 있다는 분석 ##을 내놓 ##았다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O B-ORG O O O O O O O O B-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O B-ORG O O O O O O O O B-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O B-ORG O O O O O O O O B-POH I-POH I-POH I-POH O O O O O O O O O O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 28067, 4057, 8057, 2680, 4039, 4042, 12360, 11279, 9846, 8593, 1, 8565, 4041, 5618, 4103, 1, 2255, 8014, 1616, 4277, 4353, 8361, 253, 15723, 8791, 2680, 15399, 1481, 4103, 4285, 12882, 4027, 13335, 4082, 1931, 9340, 14481, 4027, 11326, 8588, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 28067, 4057, 8057, 2680, 4039, 4042, 12360, 11279, 9846, 8593, 1, 8565, 4041, 5618, 4103, 1, 2255, 8014, 1616, 4277, 4353, 8361, 253, 15723, 8791, 2680, 15399, 1481, 4103, 4285, 12882, 4027, 13335, 4082, 1931, 9340, 14481, 4027, 11326, 8588, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 28067, 4057, 8057, 2680, 4039, 4042, 12360, 11279, 9846, 8593, 1, 8565, 4041, 5618, 4103, 1, 2255, 8014, 1616, 4277, 4353, 8361, 253, 15723, 8791, 2680, 15399, 1481, 4103, 4285, 12882, 4027, 13335, 4082, 1931, 9340, 14481, 4027, 11326, 8588, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 8, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 17, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/ner/cached_train_BertTokenizer_64_ner_named-entity-recognition [took 5.441 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/ner/cached_train_BertTokenizer_64_ner_named-entity-recognition [took 5.441 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/ner/cached_train_BertTokenizer_64_ner_named-entity-recognition [took 5.441 s]\n"]}]},{"cell_type":"code","source":["# 학습 데이터 로더 구축 (랜덤한 비복원추출)\n","from torch.utils.data import DataLoader, RandomSampler\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size = args.batch_size,\n","    sampler = RandomSampler(train_dataset, replacement=False),\n","    collate_fn = nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"id":"RHP4spCKmiOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가용 데이터 로더 구축\n","from torch.utils.data import SequentialSampler\n","val_dataset = NERDataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='val',\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=args.batch_size,\n","    sampler=SequentialSampler(val_dataset),   # 학습 데이터는 RandamSampler\n","    collate_fn=nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403833970,"user_tz":-540,"elapsed":2692,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"0bd3aa7b-1c15-444c-ac65-5504308eb474","id":"lsy4i3P_miON"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from dataset file at /content/Korpora/ner\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/ner\n","INFO:ratsnlp:Creating features from dataset file at /content/Korpora/ner\n","INFO:ratsnlp:loading val data... LOOKING AT /content/Korpora/ner/val.txt\n","INFO:ratsnlp:loading val data... LOOKING AT /content/Korpora/ner/val.txt\n","INFO:ratsnlp:loading val data... LOOKING AT /content/Korpora/ner/val.txt\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다\n","INFO:ratsnlp:sentence: 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다\n","INFO:ratsnlp:sentence: 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다\n","INFO:ratsnlp:target: 결국 초연은 <4년 반:DUR>이 지난 후에 <드레스덴:LOC>에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다\n","\n","INFO:ratsnlp:target: 결국 초연은 <4년 반:DUR>이 지난 후에 <드레스덴:LOC>에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다\n","\n","INFO:ratsnlp:target: 결국 초연은 <4년 반:DUR>이 지난 후에 <드레스덴:LOC>에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다\n","\n","INFO:ratsnlp:tokens: [CLS] 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O B-DUR I-DUR O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O B-DUR I-DUR O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O B-DUR I-DUR O O O B-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 14, 24, 4, 4, 4, 10, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 14, 24, 4, 4, 4, 10, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 14, 24, 4, 4, 4, 10, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다\n","INFO:ratsnlp:sentence: 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다\n","INFO:ratsnlp:sentence: 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다\n","INFO:ratsnlp:target: 한편 <1840년:DAT>부터 <바그너:PER>와 알고 지내던 <리스트:PER>가 잊혀져 있던 <1악장:NOH>을 부활시켜 <1852년:DAT>에 <바이마르:LOC>에서 연주했다\n","\n","INFO:ratsnlp:target: 한편 <1840년:DAT>부터 <바그너:PER>와 알고 지내던 <리스트:PER>가 잊혀져 있던 <1악장:NOH>을 부활시켜 <1852년:DAT>에 <바이마르:LOC>에서 연주했다\n","\n","INFO:ratsnlp:target: 한편 <1840년:DAT>부터 <바그너:PER>와 알고 지내던 <리스트:PER>가 잊혀져 있던 <1악장:NOH>을 부활시켜 <1852년:DAT>에 <바이마르:LOC>에서 연주했다\n","\n","INFO:ratsnlp:tokens: [CLS] 한편 18 ##40 ##년 ##부터 바 ##그 ##너 ##와 알고 지내 ##던 리스트 ##가 잊혀 ##져 있던 1 ##악 ##장을 부활 ##시켜 18 ##5 ##2년 ##에 바이 ##마 ##르 ##에서 연 ##주 ##했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 한편 18 ##40 ##년 ##부터 바 ##그 ##너 ##와 알고 지내 ##던 리스트 ##가 잊혀 ##져 있던 1 ##악 ##장을 부활 ##시켜 18 ##5 ##2년 ##에 바이 ##마 ##르 ##에서 연 ##주 ##했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 한편 18 ##40 ##년 ##부터 바 ##그 ##너 ##와 알고 지내 ##던 리스트 ##가 잊혀 ##져 있던 1 ##악 ##장을 부활 ##시켜 18 ##5 ##2년 ##에 바이 ##마 ##르 ##에서 연 ##주 ##했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O B-DAT I-DAT I-DAT O B-PER I-PER I-PER O O O O B-PER O O O O B-NOH I-NOH I-NOH O O B-DAT I-DAT I-DAT O B-LOC I-LOC I-LOC O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O B-DAT I-DAT I-DAT O B-PER I-PER I-PER O O O O B-PER O O O O B-NOH I-NOH I-NOH O O B-DAT I-DAT I-DAT O B-LOC I-LOC I-LOC O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O B-DAT I-DAT I-DAT O B-PER I-PER I-PER O O O O B-PER O O O O B-NOH I-NOH I-NOH O O B-DAT I-DAT I-DAT O B-LOC I-LOC I-LOC O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 17332, 8601, 13422, 4482, 8042, 1480, 4313, 4538, 4196, 8297, 13683, 4217, 20899, 4009, 21343, 4413, 11759, 20, 4158, 8915, 13705, 8292, 8601, 4044, 13970, 4113, 25418, 4168, 4138, 7971, 2273, 4043, 8258, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 9, 19, 19, 4, 5, 15, 15, 4, 4, 4, 4, 5, 4, 4, 4, 4, 6, 16, 16, 4, 4, 9, 19, 19, 4, 10, 20, 20, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 17332, 8601, 13422, 4482, 8042, 1480, 4313, 4538, 4196, 8297, 13683, 4217, 20899, 4009, 21343, 4413, 11759, 20, 4158, 8915, 13705, 8292, 8601, 4044, 13970, 4113, 25418, 4168, 4138, 7971, 2273, 4043, 8258, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 9, 19, 19, 4, 5, 15, 15, 4, 4, 4, 4, 5, 4, 4, 4, 4, 6, 16, 16, 4, 4, 9, 19, 19, 4, 10, 20, 20, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 17332, 8601, 13422, 4482, 8042, 1480, 4313, 4538, 4196, 8297, 13683, 4217, 20899, 4009, 21343, 4413, 11759, 20, 4158, 8915, 13705, 8292, 8601, 4044, 13970, 4113, 25418, 4168, 4138, 7971, 2273, 4043, 8258, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 9, 19, 19, 4, 5, 15, 15, 4, 4, 4, 4, 5, 4, 4, 4, 4, 6, 16, 16, 4, 4, 9, 19, 19, 4, 10, 20, 20, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다\n","INFO:ratsnlp:sentence: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다\n","INFO:ratsnlp:sentence: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다\n","INFO:ratsnlp:target: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, <제1바이올린:POH>으로 더욱 명확하게 나타난다\n","\n","INFO:ratsnlp:target: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, <제1바이올린:POH>으로 더욱 명확하게 나타난다\n","\n","INFO:ratsnlp:target: 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, <제1바이올린:POH>으로 더욱 명확하게 나타난다\n","\n","INFO:ratsnlp:tokens: [CLS] 첫 부분 ##의 저 ##음 주제 ##는 주요 주제 ( 고 ##뇌 ##와 갈 ##망 동 ##기 , 청춘 ##의 사랑 동 ##기 ) 를 암 ##시하고 있으며 , 제1 ##바이 ##올린 ##으로 더욱 명확 ##하게 나타 ##난다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 첫 부분 ##의 저 ##음 주제 ##는 주요 주제 ( 고 ##뇌 ##와 갈 ##망 동 ##기 , 청춘 ##의 사랑 동 ##기 ) 를 암 ##시하고 있으며 , 제1 ##바이 ##올린 ##으로 더욱 명확 ##하게 나타 ##난다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 첫 부분 ##의 저 ##음 주제 ##는 주요 주제 ( 고 ##뇌 ##와 갈 ##망 동 ##기 , 청춘 ##의 사랑 동 ##기 ) 를 암 ##시하고 있으며 , 제1 ##바이 ##올린 ##으로 더욱 명확 ##하게 나타 ##난다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-POH I-POH I-POH O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-POH I-POH I-POH O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B-POH I-POH I-POH O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2881, 11515, 4042, 2523, 4126, 14654, 4008, 16190, 14654, 11, 303, 4703, 4196, 204, 4227, 875, 4184, 15, 23061, 4042, 9004, 875, 4184, 12, 1265, 2183, 24730, 22886, 15, 12218, 18056, 22881, 7965, 10365, 14635, 8007, 10498, 8647, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2881, 11515, 4042, 2523, 4126, 14654, 4008, 16190, 14654, 11, 303, 4703, 4196, 204, 4227, 875, 4184, 15, 23061, 4042, 9004, 875, 4184, 12, 1265, 2183, 24730, 22886, 15, 12218, 18056, 22881, 7965, 10365, 14635, 8007, 10498, 8647, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 2881, 11515, 4042, 2523, 4126, 14654, 4008, 16190, 14654, 11, 303, 4703, 4196, 204, 4227, 875, 4184, 15, 23061, 4042, 9004, 875, 4184, 12, 1265, 2183, 24730, 22886, 15, 12218, 18056, 22881, 7965, 10365, 14635, 8007, 10498, 8647, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 17, 17, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 3편 (난키 방면 1편, 간사이 공항 방면 2편)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 4편 가까이의 편수가 운전되고 있다\n","INFO:ratsnlp:sentence: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 3편 (난키 방면 1편, 간사이 공항 방면 2편)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 4편 가까이의 편수가 운전되고 있다\n","INFO:ratsnlp:sentence: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 3편 (난키 방면 1편, 간사이 공항 방면 2편)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 4편 가까이의 편수가 운전되고 있다\n","INFO:ratsnlp:target: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 <3편:NOH> (<난키:LOC> 방면 <1편:NOH>, <간사이 공항:LOC> 방면 <2편:NOH>)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 <4편:NOH> 가까이의 편수가 운전되고 있다\n","\n","INFO:ratsnlp:target: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 <3편:NOH> (<난키:LOC> 방면 <1편:NOH>, <간사이 공항:LOC> 방면 <2편:NOH>)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 <4편:NOH> 가까이의 편수가 운전되고 있다\n","\n","INFO:ratsnlp:target: 대낮 시간대 이후에는 심야 시간대를 제외하고는 시간당 <3편:NOH> (<난키:LOC> 방면 <1편:NOH>, <간사이 공항:LOC> 방면 <2편:NOH>)으로 비교적 운행 편수가 많아 승객이 많은 시기에는 임시 열차도 운전되기 때문에 시간당 <4편:NOH> 가까이의 편수가 운전되고 있다\n","\n","INFO:ratsnlp:tokens: [CLS] 대 ##낮 시간 ##대 이후에 ##는 심 ##야 시간 ##대를 제외하고 ##는 시간 ##당 3 ##편 ( 난 ##키 방 ##면 1 ##편 , 간 ##사이 공항 방 ##면 2 ##편 ) 으로 비교 ##적 운행 편 ##수가 많아 승객 ##이 많은 시기에 ##는 임시 열 ##차도 운전 ##되기 때문에 시간 ##당 4 ##편 가까이 ##의 편 ##수가 운전 ##되고 있다 [SEP] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 대 ##낮 시간 ##대 이후에 ##는 심 ##야 시간 ##대를 제외하고 ##는 시간 ##당 3 ##편 ( 난 ##키 방 ##면 1 ##편 , 간 ##사이 공항 방 ##면 2 ##편 ) 으로 비교 ##적 운행 편 ##수가 많아 승객 ##이 많은 시기에 ##는 임시 열 ##차도 운전 ##되기 때문에 시간 ##당 4 ##편 가까이 ##의 편 ##수가 운전 ##되고 있다 [SEP] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 대 ##낮 시간 ##대 이후에 ##는 심 ##야 시간 ##대를 제외하고 ##는 시간 ##당 3 ##편 ( 난 ##키 방 ##면 1 ##편 , 간 ##사이 공항 방 ##면 2 ##편 ) 으로 비교 ##적 운행 편 ##수가 많아 승객 ##이 많은 시기에 ##는 임시 열 ##차도 운전 ##되기 때문에 시간 ##당 4 ##편 가까이 ##의 편 ##수가 운전 ##되고 있다 [SEP] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O B-NOH I-NOH O B-LOC I-LOC O O B-NOH I-NOH O B-LOC I-LOC I-LOC O O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O B-NOH I-NOH O O O O O O O [SEP] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O B-NOH I-NOH O B-LOC I-LOC O O B-NOH I-NOH O B-LOC I-LOC I-LOC O O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O B-NOH I-NOH O O O O O O O [SEP] [PAD]\n","INFO:ratsnlp:label: [CLS] O O O O O O O O O O O O O O B-NOH I-NOH O B-LOC I-LOC O O B-NOH I-NOH O B-LOC I-LOC I-LOC O O B-NOH I-NOH O O O O O O O O O O O O O O O O O O O O O B-NOH I-NOH O O O O O O O [SEP] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 809, 4734, 8485, 4140, 22395, 4008, 2015, 4144, 8485, 10633, 23993, 4008, 8485, 4081, 22, 4393, 11, 591, 4379, 1497, 4063, 20, 4393, 15, 201, 13160, 13862, 1497, 4063, 21, 4393, 12, 10442, 8898, 4022, 23167, 3282, 8356, 9737, 25443, 4017, 8298, 14686, 4008, 18002, 2275, 21509, 9381, 17221, 8360, 8485, 4081, 23, 4393, 15238, 4042, 3282, 8356, 9381, 8593, 8120, 3, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 10, 20, 4, 4, 6, 16, 4, 10, 20, 20, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 1, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 809, 4734, 8485, 4140, 22395, 4008, 2015, 4144, 8485, 10633, 23993, 4008, 8485, 4081, 22, 4393, 11, 591, 4379, 1497, 4063, 20, 4393, 15, 201, 13160, 13862, 1497, 4063, 21, 4393, 12, 10442, 8898, 4022, 23167, 3282, 8356, 9737, 25443, 4017, 8298, 14686, 4008, 18002, 2275, 21509, 9381, 17221, 8360, 8485, 4081, 23, 4393, 15238, 4042, 3282, 8356, 9381, 8593, 8120, 3, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 10, 20, 4, 4, 6, 16, 4, 10, 20, 20, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 1, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 809, 4734, 8485, 4140, 22395, 4008, 2015, 4144, 8485, 10633, 23993, 4008, 8485, 4081, 22, 4393, 11, 591, 4379, 1497, 4063, 20, 4393, 15, 201, 13160, 13862, 1497, 4063, 21, 4393, 12, 10442, 8898, 4022, 23167, 3282, 8356, 9737, 25443, 4017, 8298, 14686, 4008, 18002, 2275, 21509, 9381, 17221, 8360, 8485, 4081, 23, 4393, 15238, 4042, 3282, 8356, 9381, 8593, 8120, 3, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 10, 20, 4, 4, 6, 16, 4, 10, 20, 20, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 16, 4, 4, 4, 4, 4, 4, 4, 1, 2])\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:sentence: 덴노지 역 ~ 와카야마 역 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 한와 선 페이지에서는 생략되어 있다\n","INFO:ratsnlp:sentence: 덴노지 역 ~ 와카야마 역 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 한와 선 페이지에서는 생략되어 있다\n","INFO:ratsnlp:sentence: 덴노지 역 ~ 와카야마 역 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 한와 선 페이지에서는 생략되어 있다\n","INFO:ratsnlp:target: <덴노지 역:LOC> ~ <와카야마 역:LOC> 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 <한와 선:ORG> 페이지에서는 생략되어 있다\n","\n","INFO:ratsnlp:target: <덴노지 역:LOC> ~ <와카야마 역:LOC> 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 <한와 선:ORG> 페이지에서는 생략되어 있다\n","\n","INFO:ratsnlp:target: <덴노지 역:LOC> ~ <와카야마 역:LOC> 간에서는 정차역이 비교적 적기 때문에 시판되고 있는 시간표의 <한와 선:ORG> 페이지에서는 생략되어 있다\n","\n","INFO:ratsnlp:tokens: [CLS] 덴 ##노 ##지 역 ~ 와 ##카 ##야 ##마 역 간에 ##서는 정 ##차 ##역 ##이 비교 ##적 적 ##기 때문에 시 ##판 ##되고 있는 시간 ##표 ##의 한 ##와 선 페 ##이지 ##에서는 생 ##략 ##되어 있다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 덴 ##노 ##지 역 ~ 와 ##카 ##야 ##마 역 간에 ##서는 정 ##차 ##역 ##이 비교 ##적 적 ##기 때문에 시 ##판 ##되고 있는 시간 ##표 ##의 한 ##와 선 페 ##이지 ##에서는 생 ##략 ##되어 있다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:tokens: [CLS] 덴 ##노 ##지 역 ~ 와 ##카 ##야 ##마 역 간에 ##서는 정 ##차 ##역 ##이 비교 ##적 적 ##기 때문에 시 ##판 ##되고 있는 시간 ##표 ##의 한 ##와 선 페 ##이지 ##에서는 생 ##략 ##되어 있다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:label: [CLS] B-LOC I-LOC I-LOC I-LOC O B-LOC I-LOC I-LOC I-LOC I-LOC O O O O O O O O O O O O O O O O O O B-ORG I-ORG I-ORG O O O O O O O [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 849, 4041, 4102, 2270, 95, 2320, 4024, 4144, 4168, 2270, 22590, 9666, 2539, 4495, 4119, 4017, 8898, 4022, 2524, 4184, 8360, 2002, 4448, 8593, 8032, 8485, 4302, 4042, 3354, 4196, 1846, 3272, 8067, 8652, 1821, 4873, 9079, 8120, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 10, 20, 20, 20, 4, 10, 20, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 18, 18, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 849, 4041, 4102, 2270, 95, 2320, 4024, 4144, 4168, 2270, 22590, 9666, 2539, 4495, 4119, 4017, 8898, 4022, 2524, 4184, 8360, 2002, 4448, 8593, 8032, 8485, 4302, 4042, 3354, 4196, 1846, 3272, 8067, 8652, 1821, 4873, 9079, 8120, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 10, 20, 20, 20, 4, 10, 20, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 18, 18, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:features: NERFeatures(input_ids=[2, 849, 4041, 4102, 2270, 95, 2320, 4024, 4144, 4168, 2270, 22590, 9666, 2539, 4495, 4119, 4017, 8898, 4022, 2524, 4184, 8360, 2002, 4448, 8593, 8032, 8485, 4302, 4042, 3354, 4196, 1846, 3272, 8067, 8652, 1821, 4873, 9079, 8120, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label_ids=[0, 10, 20, 20, 20, 4, 10, 20, 20, 20, 20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 8, 18, 18, 4, 4, 4, 4, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/ner/cached_val_BertTokenizer_64_ner_named-entity-recognition [took 0.330 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/ner/cached_val_BertTokenizer_64_ner_named-entity-recognition [took 0.330 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/ner/cached_val_BertTokenizer_64_ner_named-entity-recognition [took 0.330 s]\n"]}]},{"cell_type":"code","source":["# 모델 초기화\n","from transformers import BertConfig, BertForTokenClassification\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n","    num_labels = corpus.num_labels,\n",")\n","\n","model = BertForTokenClassification.from_pretrained(\n","    args.pretrained_model_name,\n","    config=pretrained_model_config,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403929031,"user_tz":-540,"elapsed":3829,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"3f6570e6-8c4e-4c45-e092-a7b151f67bd5","id":"U58uJwJimiOO"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 모델 학습 - 태스크 정의\n","from ratsnlp.nlpbook.ner import NERTask\n","task = NERTask(model, args)\n","\n","# 모델 학습 - 트레이너 정의\n","trainer = nlpbook.get_trainer(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673403973185,"user_tz":-540,"elapsed":1081,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"f7e28a6b-f74d-43ac-a9e9-b96ceb129c5d","id":"ty1Tpvm_miOP"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["# 모델 학습\n","trainer.fit(\n","    task,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309,"referenced_widgets":["6f38cea50add4bcb848305e2997cca10","19e4e0c5ad304c78ac76e46972dc6555","65d5788b684b45ceb32cfcb541a1c7dc","634a35b2d9dc4a8faf3f1230eb047fcf","c193196c528349a68cd51bd6f65e7589","0dc315f19e544fbda0b31e7a6122ff6b","67c933a606f44eb9b263b143602826ae","89874fe7ba154e86b92a0f7d5136f5f7","9109eedb9aeb4384b1e519bc31609579","35ecf4235f124c849727865f6d7c9f06","51f15974764d44c6adcccf494ee94672","ad2b82ca30294425b6b266f7de5518d8","83ab621e534544e690fa6bab55bb50c8","53a1d9291b1648b5a77f89383686b5fc","00aeabfe7fc84d07b792bf7a8238ab9f","5260869685564b02bdf6f747d17f6727","0358e1bf5b25416f891105aae2d19090","37350a9da355428abbd4deb354982821","194f1bf71fec49f1af89f76ebeff83a2","7829d774c8bd402f803f75fae942e316","0f3e129163b44069a93a0f264a14affe","6714d3ac17344c57b7baaa2d9bbcff1f"]},"executionInfo":{"status":"ok","timestamp":1673404548924,"user_tz":-540,"elapsed":558555,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"d45c5b69-23b5-4658-87c2-abef3027bdb3","id":"gqZp6IeomiOP"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /gdrive/MyDrive/nlpbook/checkpoint-ner/lightning_logs\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /gdrive/MyDrive/nlpbook/checkpoint-ner exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n","  rank_zero_warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                       | Params\n","-----------------------------------------------------\n","0 | model | BertForTokenClassification | 108 M \n","-----------------------------------------------------\n","108 M     Trainable params\n","0         Non-trainable params\n","108 M     Total params\n","433.389   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f38cea50add4bcb848305e2997cca10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad2b82ca30294425b6b266f7de5518d8"}},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"DLy4kIasl6fh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 학습을 마친 모델을 실전 투입하기\n","- 개체명 인식 웹서비스 만들기"],"metadata":{"id":"f2Um8LEAp3Q3"}},{"cell_type":"code","source":["# 인퍼런스 설정\n","from ratsnlp.nlpbook.ner import NERDeployArguments\n","args = NERDeployArguments(\n","    pretrained_model_name = \"beomi/kcbert-base\",\n","    downstream_model_dir = \"/gdrive/MyDrive/nlpbook/checkpoint-ner\",\n","    max_seq_length=64,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5SBKShgp8wC","executionInfo":{"status":"ok","timestamp":1673407426231,"user_tz":-540,"elapsed":508,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"e95255cc-f63f-41be-d1a0-c81904bea6e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downstream_model_checkpoint_fpath: /gdrive/MyDrive/nlpbook/checkpoint-ner/epoch=0-val_loss=0.22.ckpt\n","downstream_model_labelmap_fpath: /gdrive/MyDrive/nlpbook/checkpoint-ner/label_map.txt\n"]}]},{"cell_type":"code","source":["# 토크나이저 로드\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"id":"MwNMbnXTqnkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 체크포인트 로드\n","import torch\n","fine_tuned_model_ckpt = torch.load(\n","    args.downstream_model_checkpoint_fpath,\n","    map_location=torch.device('cpu')\n",")"],"metadata":{"id":"FjjvZUhCqngP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT 설정 로드\n","from transformers import BertConfig\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n","    num_labels = fine_tuned_model_ckpt['state_dict']['model.classifier.bias'].shape.numel(),\n",")"],"metadata":{"id":"mxh0EVyEqncw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT 모델 초기화\n","from transformers import BertForTokenClassification\n","model = BertForTokenClassification(pretrained_model_config)"],"metadata":{"id":"ifI5kHKhqnZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 체크 포인트 주입하기\n","model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcXGXHIOqnV-","executionInfo":{"status":"ok","timestamp":1673407472980,"user_tz":-540,"elapsed":9,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"88b44962-a304-461f-bf3c-646ec565b2c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# 평가모드로 전환\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbBSDxnZqnSW","executionInfo":{"status":"ok","timestamp":1673407527829,"user_tz":-540,"elapsed":1076,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"40494449-7d56-4df5-98ca-1d937bbec7af"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(300, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=25, bias=True)\n",")"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["# 레이블 맵 작성\n","labels = [label.strip() for label in open(args.downstream_model_labelmap_fpath, \"r\").readlines()]\n","id_to_label = {}\n","\n","for idx, label in enumerate(labels):\n","  if \"PER\" in label:\n","    label = \"인명\"\n","  elif \"LOC\" in label:\n","    label = \"지명\"\n","  elif \"ORG\" in label:\n","    label = \"기관명\"\n","  elif \"DAT\" in label:\n","    label = \"날짜\"\n","  elif \"TIM\" in label:\n","    label = \"시간\"\n","  elif \"DUR\" in label:\n","    label = \"기간\"\n","  elif \"MNY\" in label:\n","    label = \"통화\"\n","  elif \"PNT\" in label:\n","    label = \"비율\"\n","  elif \"NOH\" in label:\n","    label = \"기타 수량표현\"\n","  elif \"POH\" in label:\n","    label = \"기타\"\n","  else:\n","    label = label\n","  \n","  id_to_label[idx] = label"],"metadata":{"id":"7oCzuiAyqnNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 출력값 만들고 후처리하기 - 인퍼런스 함수\n","def inference_fn(sentence):\n","  # 문장을 토큰화하고 인덱싱하되, max_seq_length 보다 짧으면 이에 맞게 패딩하고, 길면 자르기\n","  inputs = tokenizer(\n","      [sentence],\n","       max_length=args.max_seq_length,\n","       padding='max_length',\n","       truncation=True,\n","  )\n","  with torch.no_grad():\n","    # 모델 계산하기\n","    # inputs 를 파이토치 텐서로 바꾸기\n","    outputs = model(**{k: torch.tensor(v) for k, v in inputs.items()})\n","    \n","    # 로짓에 소프트맥스를 취해 각 토큰이 어떤 개체명에 속하는지 확률 구하기\n","    probs = outputs.logits[0].softmax(dim=1)\n","\n","    # 각 토큰이 속하는 개체명 확률분포(probs) 가운데\n","    # 가장 높은 확률값(top_probs)과 그에 속하는 개체명 인덱스(preds) 구하기\n","    top_probs, preds = torch.topk(probs, dim=1, k=1)\n","\n","    # 토큰 인덱스 시퀀스(List[int])를 토큰 시퀀스(List[str])로 변환하기\n","    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","\n","    # 개체명 인덱스 시퀀스(List[int])를 개체명 시퀀스(List[str])로 변환하기\n","    predicted_tags = [id_to_label[pred.item()] for pred in preds]\n","\n","    result = []\n","    # [CLS], [SEP], [PAD] 를 제외한 토큰 각각에 대해\n","    # 모델이 예측한 개체명(predicted_tag)과 그 확률값(top_prob)을 반환하기\n","    for token, predicted_tag, top_prob in zip(tokens, predicted_tags, top_probs):\n","      if token not in [tokenizer.pad_token, tokenizer.cls_token,\n","                       tokenizer.sep_token]:\n","          token_result = {\n","              \"token\" : token,\n","              \"predicted_tag\" : predicted_tag,\n","              \"top_prob\" : str(round(top_prob[0].item(), 4)),\n","          }\n","          result.append(token_result)\n","\n","  return {\n","      \"sentence\" : sentence,\n","      \"result\" : result,\n","  }"],"metadata":{"id":"PseuKBH3qnI-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_f9o8eyw2qs4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 웹서비스 만들기"],"metadata":{"id":"dDVcYEe_2q6U"}},{"cell_type":"code","source":["!rm -rf /root/.ngrok2/"],"metadata":{"id":"vJ7BUZH14JwP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_NtkRBL4Q53","executionInfo":{"status":"ok","timestamp":1673408165444,"user_tz":-540,"elapsed":489,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"a83eaf26-b0fb-4442-900d-b57d1a661678"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 64\n","drwx------ 1 root root 4096 Jan 11 03:36 .\n","drwxr-xr-x 1 root root 4096 Jan 11 00:14 ..\n","-r-xr-xr-x 1 root root 1169 Jan  1  2000 .bashrc\n","drwxr-xr-x 1 root root 4096 Jan 11 00:24 .cache\n","drwx------ 1 root root 4096 Jan 11 00:13 .config\n","drwxr-xr-x 5 root root 4096 Jan  9 14:51 .ipython\n","drwx------ 2 root root 4096 Jan  9 14:51 .jupyter\n","drwxr-xr-x 2 root root 4096 Jan  9 14:48 .keras\n","drwxr-xr-x 1 root root 4096 Jan  9 14:51 .local\n","drwxr-xr-x 4 root root 4096 Jan  9 14:51 .npm\n","drwx------ 3 root root 4096 Jan 11 01:08 .nv\n","-rw-r--r-- 1 root root  148 Aug 17  2015 .profile\n","-r-xr-xr-x 1 root root  254 Jan  1  2000 .tmux.conf\n","-rw-r--r-- 1 root root  165 Jan  9 14:51 .wget-hsts\n"]}]},{"cell_type":"code","source":["!mkdir /root/.ngrok2 && echo \"authtoken: 2K7ID8ZcT7kK16cuvUNOrYHgoxc_81tzFhnzDzLHatQ7xguDQ\" > /root/.ngrok2/ngrok.yml"],"metadata":{"id":"vW3bzB7y2q6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673408177196,"user_tz":-540,"elapsed":7,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"19c3551d-16a5-4a69-ec22-96adb99d1535","id":"yWGMRpwx2q6W"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 68\n","drwx------ 1 root root 4096 Jan 11 03:36 .\n","drwxr-xr-x 1 root root 4096 Jan 11 00:14 ..\n","-r-xr-xr-x 1 root root 1169 Jan  1  2000 .bashrc\n","drwxr-xr-x 1 root root 4096 Jan 11 00:24 .cache\n","drwx------ 1 root root 4096 Jan 11 00:13 .config\n","drwxr-xr-x 5 root root 4096 Jan  9 14:51 .ipython\n","drwx------ 2 root root 4096 Jan  9 14:51 .jupyter\n","drwxr-xr-x 2 root root 4096 Jan  9 14:48 .keras\n","drwxr-xr-x 1 root root 4096 Jan  9 14:51 .local\n","drwxr-xr-x 2 root root 4096 Jan 11 03:36 .ngrok2\n","drwxr-xr-x 4 root root 4096 Jan  9 14:51 .npm\n","drwx------ 3 root root 4096 Jan 11 01:08 .nv\n","-rw-r--r-- 1 root root  148 Aug 17  2015 .profile\n","-r-xr-xr-x 1 root root  254 Jan  1  2000 .tmux.conf\n","-rw-r--r-- 1 root root  165 Jan  9 14:51 .wget-hsts\n"]}]},{"cell_type":"code","source":["# 웹서비스\n","from ratsnlp.nlpbook.ner import get_web_service_app\n","app = get_web_service_app(inference_fn)\n","app.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673408234708,"user_tz":-540,"elapsed":50216,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"3490e98a-b244-47e6-bf44-58c0f67d17e2","id":"Q1IUKJHF2q6W"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"ratsnlp.nlpbook.ner.deploy\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://e153-34-126-175-160.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [11/Jan/2023 03:36:39] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [11/Jan/2023 03:36:40] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [11/Jan/2023 03:36:58] \"\u001b[37mPOST /api HTTP/1.1\u001b[0m\" 200 -\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lw2Y9CsT2q6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 230112 Day98"],"metadata":{"id":"F_nFhBBLVPC_"}},{"cell_type":"markdown","source":["## 질의응답 모델 학습하기\n","- 질문에 답하는 모델 만들기"],"metadata":{"id":"lmXKjq1_VRe_"}},{"cell_type":"code","source":["!pip install ratsnlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483031240,"user_tz":-540,"elapsed":11502,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"7e7c66b9-c836-4c7a-bb2c-fe383dc1b305","id":"rz-YYdwWW889"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ratsnlp\n","  Downloading ratsnlp-1.0.52-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from ratsnlp) (1.1.4)\n","Collecting Korpora>=0.2.0\n","  Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flask-ngrok>=0.0.25\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Collecting transformers==4.10.0\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-lightning==1.6.1\n","  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.5/582.5 KB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flask-cors>=3.0.10\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Collecting torchmetrics>=0.4.1\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.13.1+cu116)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.64.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (4.4.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (1.21.6)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2.9.1)\n","Collecting pyDeprecate<0.4.0,>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (6.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (2022.11.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.6.1->ratsnlp) (21.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (3.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.10.0->ratsnlp) (2.25.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from flask>=1.1.4->ratsnlp) (1.0.1)\n","Requirement already satisfied: Six in /usr/local/lib/python3.8/dist-packages (from flask-cors>=3.0.10->ratsnlp) (1.15.0)\n","Collecting dataclasses>=0.6\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from Korpora>=0.2.0->ratsnlp) (1.2.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (3.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.1.4->ratsnlp) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.6.1->ratsnlp) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.10.0->ratsnlp) (2022.12.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.38.4)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (2.15.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (57.4.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.19.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.51.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.10.0->ratsnlp) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (22.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (2.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.1->ratsnlp) (4.0.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (5.2.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (6.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.6.1->ratsnlp) (3.2.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fb6905535dd56793c5016ff2ecd13f6d61088a626ebbdfe0b266c8b2b41b59fb\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, dataclasses, sacremoses, pyDeprecate, torchmetrics, Korpora, huggingface-hub, transformers, flask-ngrok, flask-cors, pytorch-lightning, ratsnlp\n","Successfully installed Korpora-0.2.0 dataclasses-0.6 flask-cors-3.0.10 flask-ngrok-0.0.25 huggingface-hub-0.11.1 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 ratsnlp-1.0.52 sacremoses-0.0.53 tokenizers-0.10.3 torchmetrics-0.11.0 transformers-4.10.0\n"]}]},{"cell_type":"code","source":["# 구글 드라이브 연동하기\n","from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483049787,"user_tz":-540,"elapsed":12757,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"72daae92-aee5-425f-ec28-7069eb74f82e","id":"jWdRBVSCW88-"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"code","source":["# 모델 하이퍼 파라미터와 저장 위치 등 설정 정보 선언\n","import torch\n","from ratsnlp.nlpbook.qa import QATrainArguments\n","\n","args = QATrainArguments(\n","    pretrained_model_name = \"beomi/kcbert-base\",\n","    downstream_corpus_name = \"korquad-v1\",\n","    downstream_model_dir = \"/gdrive/MyDrive/nlpbook/checkpoint-qa\",\n","    max_seq_length=128,\n","    max_query_length=32,\n","    doc_stride=64,\n","    batch_size=32 if torch.cuda.is_available() else 4,\n","    learning_rate=5e-5,\n","    epochs=1,\n","    tpu_cores=0 if torch.cuda.is_available() else 8,\n","    seed=7,\n",")"],"metadata":{"id":"W_-XpP6CW88_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 재현을 위해 랜덤 시드 고정\n","from ratsnlp import nlpbook\n","nlpbook.set_seed(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483181741,"user_tz":-540,"elapsed":284,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"a34f2e71-8133-4b8e-928c-e4c12e959498","id":"azmwaDZjW88_"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["set seed: 7\n"]}]},{"cell_type":"code","source":["# 메세지 출력 등을 위한 logger 설정\n","nlpbook.set_logger(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483187361,"user_tz":-540,"elapsed":350,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"4b4b1f22-ffec-4461-8883-59858065133d","id":"RIkI7GO7W88_"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Training/evaluation parameters QATrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_corpus_name='korquad-v1', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-qa', max_seq_length=128, doc_stride=64, max_query_length=32, threads=4, cpu_workers=4, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, fp16=False, tpu_cores=0, tqdm_enabled=True)\n","INFO:ratsnlp:Training/evaluation parameters QATrainArguments(pretrained_model_name='beomi/kcbert-base', downstream_corpus_name='korquad-v1', downstream_corpus_root_dir='/content/Korpora', downstream_model_dir='/gdrive/MyDrive/nlpbook/checkpoint-qa', max_seq_length=128, doc_stride=64, max_query_length=32, threads=4, cpu_workers=4, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=1, batch_size=32, fp16=False, tpu_cores=0, tqdm_enabled=True)\n"]}]},{"cell_type":"code","source":["# 실습에 사용할 말뭉치 다운로드\n","nlpbook.download_downstream_dataset(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483203198,"user_tz":-540,"elapsed":1577,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"807dae14-6fa2-4400-875e-7c1ead7c8629","id":"rLhtdZhiW89A"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: 38.5MB [00:00, 120MB/s]\n","Downloading: 3.88MB [00:00, 97.3MB/s]                  \n"]}]},{"cell_type":"code","source":["# 토크나이저 준비\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["3afad6acfc81492a8117ab083b9798b9","761b0529b63a404684951695ab5d1603","61a9defe0d8a4f679e59cda2cc13c074","2287a43f0f3a4a61bf0c311459af8ae6","b6a627361a694115b503ef5fee190ae7","3258b24ee02241dd906adc3d1ac0d0df","5244a34bc6f047dea9a1af7665654aff","c6407c4e170740feb9ff836cd649ef33","df6e0834ae644822bfabeac2142b9c87","7d90b283ea2a4121a276987dea4ae671","3279c3f035314bcf9678a8d51ae263e2","b70cdd9733e24f62922a99f0f58aaeb3","f70baee7d7574d8fac04c39cef539e4a","02438465cef3449e8a843437a3c899d1","5427950691bc4d3abaefd8cbe2a29611","658e3412eeb740b6aea9c3d3c77d32c5","ae30eb6940c14c24a700e432dda01b05","a109ae64e80940c09d9fef4f35859201","bec71aadf8a24113a5c21318e45dce1f","03b090d683764c3da505fa2f0b1b1a7c","80e7abce8c2e4862b5ecb7ff66fde78b","1bf240bf4088420da8c0dd4d4fe41adb","8b33b4d5c56c448a98428f22c0125503","96aec9e5ec4248b28a88ab118dec2fe1","161fd874e1c645d4b49dd64cb4f92504","b129935b8d994ef0a0d9f4a37b3aff4e","7ebed85d76644fd29c3aa5ccf96f7068","8d0d7bad399a44bbb27060be1f83b5d0","a377ecbe4e13471cbeafa2706e09b206","d4e047f4ba1f4309882b1fc5312ffbfe","c4534aa22dd5461b90cbc6566c77c073","c486765c27964f779142536ae387eec3","82355e0ebe7a4d9cb3fcbe217f430221"]},"executionInfo":{"status":"ok","timestamp":1673483232136,"user_tz":-540,"elapsed":3269,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"99c041a8-5a4a-4dbf-ba72-d1f7ee6309b3","id":"e3khrY7NW89A"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/250k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afad6acfc81492a8117ab083b9798b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70cdd9733e24f62922a99f0f58aaeb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b33b4d5c56c448a98428f22c0125503"}},"metadata":{}}]},{"cell_type":"code","source":["# 학습 데이터 세트 구축\n","from ratsnlp.nlpbook.qa import KorQuADV1Corpus, QADataset\n","\n","corpus = KorQuADV1Corpus()\n","train_dataset = QADataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='train',\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483494173,"user_tz":-540,"elapsed":191468,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"b70e8148-f512-40d7-e71a-fad067105427","id":"bKyN_NrNW89A"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from train dataset file at /content/Korpora/korquad-v1\n","INFO:ratsnlp:Creating features from train dataset file at /content/Korpora/korquad-v1\n","100%|██████████| 1420/1420 [00:00<00:00, 19374.36it/s]\n","convert squad examples to features: 100%|██████████| 57688/57688 [02:37<00:00, 367.22it/s]\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP]\n","INFO:ratsnlp:answer: 교 ##향 ##곡\n","INFO:ratsnlp:answer: 교 ##향 ##곡\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=45, end_positions=47)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=45, end_positions=47)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트를 읽고 무엇을 쓰고 ##자 했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 19143, 13985, 12449, 9194, 4105, 3385, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 18 ##3 ##9년 바 ##그 ##너 ##는 괴 ##테 ##의 파 ##우스 ##트 ##을 처음 읽고 그 내용 ##에 마음이 끌려 이를 소재 ##로 해서 하나의 교 ##향 ##곡 ##을 쓰 ##려는 뜻을 갖 ##는다 . 이 시기 바 ##그 ##너 ##는 18 ##3 ##8년 ##에 빛 독 ##촉 ##으로 산 ##전 ##수 ##전을 다 걲 ##은 상황이 ##라 좌 ##절 ##과 실망 ##에 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 8601, 4633, 29697, 1480, 4313, 4538, 4008, 336, 4065, 4042, 3231, 23243, 4104, 4027, 8793, 13985, 391, 9132, 4113, 10966, 11728, 12023, 14657, 4091, 8598, 16639, 341, 4573, 4771, 4027, 2139, 8478, 14416, 214, 8202, 17, 2451, 13007, 1480, 4313, 4538, 4008, 8601, 4633, 22903, 4113, 1676, 868, 4913, 7965, 1789, 4203, 4110, 15031, 786, 250, 4057, 10878, 4007, 2593, 4094, 4128, 10289, 4113, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 가득 ##했 ##으며 메 ##피 ##스 ##토 ##펠 ##레스 ##를 만나는 파 ##우스 ##트 ##의 심 ##경 ##에 공감 ##했다고 한다 . 또한 파리 ##에서 아 ##브 ##네 ##크 ##의 지휘 ##로 파리 음악 ##원 관 ##현 ##악 ##단이 연 ##주 ##하는 베 ##토 ##벤 ##의 교 ##향 ##곡 9 ##번 ##을 듣고 깊은 감 ##명을 받았는데 , 이것이 이 ##듬 ##해 1월 ##에 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10958, 4062, 9511, 1355, 4600, 4103, 4775, 5602, 10770, 4180, 26732, 3231, 23243, 4104, 4042, 2015, 4012, 4113, 9198, 8763, 8129, 17, 10384, 23008, 7971, 2170, 4408, 4011, 4147, 4042, 17015, 4091, 23008, 21056, 4165, 323, 4175, 4158, 11413, 2273, 4043, 7966, 1543, 4775, 4170, 4042, 341, 4573, 4771, 28, 4566, 4027, 10599, 18907, 208, 9504, 24835, 15, 11060, 2451, 4780, 4032, 18548, 4113, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 파 ##우스 ##트 ##의 서 ##곡 ##으로 쓰여 ##진 이 작품 ##에 조금이라도 영향을 끼 ##쳤 ##으리 ##라는 것은 의심 ##할 여지가 없다 . 여기 ##의 라 ##단 ##조 조성 ##의 경우 ##에도 그의 전기 ##에 적 ##혀 있는 것처럼 단순한 정신적 피로 ##나 실 ##의가 반영 ##된 것이 아니라 베 ##토 ##벤 ##의 합 ##창 ##교 ##향 ##곡 조성 ##의 영향을 받은 것을 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( [SEP]\n","INFO:ratsnlp:answer: 1 ##악 ##장을\n","INFO:ratsnlp:answer: 1 ##악 ##장을\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=108, end_positions=110)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 3231, 23243, 4104, 4042, 1843, 4771, 7965, 28987, 4153, 2451, 15489, 4113, 13928, 17283, 575, 4261, 26783, 8114, 8852, 9107, 4082, 28498, 8131, 17, 8225, 4042, 1114, 4281, 4194, 17138, 4042, 9961, 8222, 14041, 10892, 4113, 2524, 4443, 8032, 12710, 21602, 18625, 24569, 4136, 2009, 11012, 17068, 4130, 8544, 8141, 1543, 4775, 4170, 4042, 3367, 4183, 4267, 4573, 4771, 17138, 4042, 17283, 9838, 9153, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=108, end_positions=110)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] 볼 수 있다 . 그렇게 교 ##향 ##곡 작 ##곡 ##을 18 ##3 ##9년 ##부터 40년 ##에 걸 ##쳐 파리 ##에서 착 ##수 ##했 ##으나 1 ##악 ##장을 쓴 뒤에 중단 ##했다 . 또한 작품 ##의 완성 ##과 동시에 그는 이 서 ##곡 ( 1 ##악 ##장 ) 을 파리 음악 ##원의 연 ##주 ##회에서 연 ##주 ##할 파 ##트 ##보 ##까지 준비 ##하였 ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 [SEP]\n","INFO:ratsnlp:answer: 1 ##악 ##장을\n","INFO:ratsnlp:answer: 1 ##악 ##장을\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=44, end_positions=46)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 1576, 1931, 8120, 17, 8117, 341, 4573, 4771, 2478, 4771, 4027, 8601, 4633, 29697, 8042, 25305, 4113, 254, 4133, 23008, 7971, 2840, 4110, 4062, 10410, 20, 4158, 8915, 2141, 11726, 13014, 8258, 17, 10384, 15489, 4042, 16520, 4128, 15831, 20048, 2451, 1843, 4771, 11, 20, 4158, 4099, 12, 2424, 23008, 21056, 18639, 2273, 4043, 27703, 2273, 4043, 4082, 3231, 4104, 4010, 7999, 9242, 22504, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=44, end_positions=46)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:question & context: [CLS] 바 ##그 ##너 ##는 교 ##향 ##곡 작 ##곡 ##을 어디까지 쓴 뒤에 중단 ##했 ##는가 ? [SEP] ##으나 , 실제로는 이루어 ##지지 ##는 않았다 . 결국 초 ##연 ##은 4년 반 ##이 지난 후에 드 ##레스 ##덴 ##에서 연 ##주 ##되었고 재 ##연 ##도 이루어 ##졌지 ##만 , 이후에 그대로 방치 ##되고 말았 ##다 . 그 사이에 그는 리 ##엔 ##치 ##와 방 ##황 ##하는 네 ##덜 ##란드 ##인을 완성 ##하고 탄 ##호 ##이 ##저 ##에도 착 ##수 ##하는 등 분 ##주 ##한 시간을 보냈 ##는데 , 그런 바쁜 생활 ##이 이 곡 ##을 잊 ##게 한 것이 아닌가 하는 의견 ##도 있다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 1480, 4313, 4538, 4008, 341, 4573, 4771, 2478, 4771, 4027, 13655, 2141, 11726, 13014, 4062, 9411, 32, 3, 10410, 15, 26271, 11877, 8840, 4008, 12626, 17, 8340, 2906, 4132, 4057, 16243, 1483, 4017, 9403, 14180, 947, 10770, 5076, 7971, 2273, 4043, 24984, 2499, 4132, 4029, 11877, 23679, 4049, 15, 22395, 9341, 14218, 8593, 19696, 4020, 17, 391, 19237, 20048, 1279, 4484, 4077, 4196, 1497, 4432, 7966, 654, 4181, 27028, 9700, 16520, 7968, 3110, 4319, 4017, 4488, 8222, 2840, 4110, 7966, 963, 1614, 4043, 4047, 16381, 13079, 7972, 15, 8053, 15118, 9683, 4017, 2451, 304, 4027, 2471, 4199, 3354, 8544, 8260, 7996, 10040, 4029, 8120, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/korquad-v1/cached_train_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 32.698 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/korquad-v1/cached_train_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 32.698 s]\n"]}]},{"cell_type":"code","source":["# 학습 데이터 로더 구축 (랜덤한 비복원추출)\n","from torch.utils.data import DataLoader, RandomSampler\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size = args.batch_size,\n","    sampler = RandomSampler(train_dataset, replacement=False),\n","    collate_fn = nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"id":"ZaRJszzEW89A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 평가용 데이터 로더 구축\n","from torch.utils.data import SequentialSampler\n","val_dataset = QADataset(\n","    args=args,\n","    corpus=corpus,\n","    tokenizer=tokenizer,\n","    mode='val',\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=args.batch_size,\n","    sampler=SequentialSampler(val_dataset),   # 학습 데이터는 RandamSampler\n","    collate_fn=nlpbook.data_collator,\n","    drop_last=False,\n","    num_workers=args.cpu_workers,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483689822,"user_tz":-540,"elapsed":24513,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"b53bedc8-7eba-41c8-9e03-95d284db3ad3","id":"J9R0FaTkW89A"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:ratsnlp:Creating features from val dataset file at /content/Korpora/korquad-v1\n","INFO:ratsnlp:Creating features from val dataset file at /content/Korpora/korquad-v1\n","100%|██████████| 140/140 [00:00<00:00, 12516.57it/s]\n","convert squad examples to features: 100%|██████████| 5533/5533 [00:20<00:00, 271.72it/s]\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 6월 30 ##일 평양 ##축 ##전에 대표로 파견 된 인물 ##은 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 6월 30 ##일 평양 ##축 ##전에 대표로 파견 된 인물 ##은 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 [SEP]\n","INFO:ratsnlp:answer: 임 ##수 ##경을\n","INFO:ratsnlp:answer: 임 ##수 ##경을\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 23343, 8476, 4046, 14601, 4728, 8409, 24345, 20158, 902, 10393, 4057, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=75, end_positions=77)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 23343, 8476, 4046, 14601, 4728, 8409, 24345, 20158, 902, 10393, 4057, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=75, end_positions=77)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 6월 30 ##일 평양 ##축 ##전에 대표로 파견 된 인물 ##은 ? [SEP] ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특 ##공 ##조 및 대 ##공 ##과 직원 12 ##명 등 22 ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 6월 30 ##일 평양 ##축 ##전에 대표로 파견 된 인물 ##은 ? [SEP] ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특 ##공 ##조 및 대 ##공 ##과 직원 12 ##명 등 22 ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 23343, 8476, 4046, 14601, 4728, 8409, 24345, 20158, 902, 10393, 4057, 32, 3, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 7965, 14050, 4047, 3210, 4239, 4194, 1476, 809, 4239, 4128, 9736, 9685, 4268, 963, 15190, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 23343, 8476, 4046, 14601, 4728, 8409, 24345, 20158, 902, 10393, 4057, 32, 3, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 7965, 14050, 4047, 3210, 4239, 4194, 1476, 809, 4239, 4128, 9736, 9685, 4268, 963, 15190, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 6월 30 ##일 평양 ##축 ##전에 대표로 파견 된 인물 ##은 ? [SEP] ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 계 ##단을 내려오 ##는 임종석 ##을 발견 , 검 ##거 ##해 구속 ##을 집행 ##했다 . 임종석 ##은 청 ##량 ##리 ##경찰 ##서에 ##서 약 1시간 동안 조사를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장 ##안 ##동의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로 인 ##계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 6월 30 ##일 평양 ##축 ##전에 대표로 파견 된 인물 ##은 ? [SEP] ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 계 ##단을 내려오 ##는 임종석 ##을 발견 , 검 ##거 ##해 구속 ##을 집행 ##했다 . 임종석 ##은 청 ##량 ##리 ##경찰 ##서에 ##서 약 1시간 동안 조사를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장 ##안 ##동의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로 인 ##계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 23343, 8476, 4046, 14601, 4728, 8409, 24345, 20158, 902, 10393, 4057, 32, 3, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 296, 13321, 13851, 4008, 14749, 4027, 14384, 15, 255, 4014, 4032, 8430, 4027, 11932, 8258, 17, 14749, 4057, 2883, 4667, 4038, 11304, 14029, 4072, 2208, 20600, 10921, 15576, 9838, 934, 28970, 28, 4039, 8583, 4192, 291, 8270, 2492, 4061, 21643, 8270, 4102, 4015, 11304, 4190, 13445, 4192, 24442, 2454, 4067, 14012, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 23343, 8476, 4046, 14601, 4728, 8409, 24345, 20158, 902, 10393, 4057, 32, 3, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 296, 13321, 13851, 4008, 14749, 4027, 14384, 15, 255, 4014, 4032, 8430, 4027, 11932, 8258, 17, 14749, 4057, 2883, 4667, 4038, 11304, 14029, 4072, 2208, 20600, 10921, 15576, 9838, 934, 28970, 28, 4039, 8583, 4192, 291, 8270, 2492, 4061, 21643, 8270, 4102, 4015, 11304, 4190, 13445, 4192, 24442, 2454, 4067, 14012, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##을 검 ##거 ##한 장소 ##는 경 ##희 ##대 내 어디 ##인가 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##을 검 ##거 ##한 장소 ##는 경 ##희 ##대 내 어디 ##인가 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4027, 255, 4014, 4047, 25754, 4008, 291, 4346, 4140, 609, 8118, 8078, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4027, 255, 4014, 4047, 25754, 4008, 291, 4346, 4140, 609, 8118, 8078, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##을 검 ##거 ##한 장소 ##는 경 ##희 ##대 내 어디 ##인가 ? [SEP] ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특 ##공 ##조 및 대 ##공 ##과 직원 12 ##명 등 22 ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##을 검 ##거 ##한 장소 ##는 경 ##희 ##대 내 어디 ##인가 ? [SEP] ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특 ##공 ##조 및 대 ##공 ##과 직원 12 ##명 등 22 ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4027, 255, 4014, 4047, 25754, 4008, 291, 4346, 4140, 609, 8118, 8078, 32, 3, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 7965, 14050, 4047, 3210, 4239, 4194, 1476, 809, 4239, 4128, 9736, 9685, 4268, 963, 15190, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4027, 255, 4014, 4047, 25754, 4008, 291, 4346, 4140, 609, 8118, 8078, 32, 3, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 7965, 14050, 4047, 3210, 4239, 4194, 1476, 809, 4239, 4128, 9736, 9685, 4268, 963, 15190, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##을 검 ##거 ##한 장소 ##는 경 ##희 ##대 내 어디 ##인가 ? [SEP] ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 계 ##단을 내려오 ##는 임종석 ##을 발견 , 검 ##거 ##해 구속 ##을 집행 ##했다 . 임종석 ##은 청 ##량 ##리 ##경찰 ##서에 ##서 약 1시간 동안 조사를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장 ##안 ##동의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로 인 ##계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##을 검 ##거 ##한 장소 ##는 경 ##희 ##대 내 어디 ##인가 ? [SEP] ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 계 ##단을 내려오 ##는 임종석 ##을 발견 , 검 ##거 ##해 구속 ##을 집행 ##했다 . 임종석 ##은 청 ##량 ##리 ##경찰 ##서에 ##서 약 1시간 동안 조사를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장 ##안 ##동의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로 인 ##계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:answer: 학생 ##회 ##관 건물 계 ##단을\n","INFO:ratsnlp:answer: 학생 ##회 ##관 건물 계 ##단을\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4027, 255, 4014, 4047, 25754, 4008, 291, 4346, 4140, 609, 8118, 8078, 32, 3, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 296, 13321, 13851, 4008, 14749, 4027, 14384, 15, 255, 4014, 4032, 8430, 4027, 11932, 8258, 17, 14749, 4057, 2883, 4667, 4038, 11304, 14029, 4072, 2208, 20600, 10921, 15576, 9838, 934, 28970, 28, 4039, 8583, 4192, 291, 8270, 2492, 4061, 21643, 8270, 4102, 4015, 11304, 4190, 13445, 4192, 24442, 2454, 4067, 14012, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=61, end_positions=66)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4027, 255, 4014, 4047, 25754, 4008, 291, 4346, 4140, 609, 8118, 8078, 32, 3, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 296, 13321, 13851, 4008, 14749, 4027, 14384, 15, 255, 4014, 4032, 8430, 4027, 11932, 8258, 17, 14749, 4057, 2883, 4667, 4038, 11304, 14029, 4072, 2208, 20600, 10921, 15576, 9838, 934, 28970, 28, 4039, 8583, 4192, 291, 8270, 2492, 4061, 21643, 8270, 4102, 4015, 11304, 4190, 13445, 4192, 24442, 2454, 4067, 14012, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=61, end_positions=66)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##이 조사를 받은 뒤 인 ##계 ##된 곳은 어딘 ##가 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##이 조사를 받은 뒤 인 ##계 ##된 곳은 어딘 ##가 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4017, 15576, 9838, 934, 2454, 4067, 4130, 17885, 16375, 4009, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4017, 15576, 9838, 934, 2454, 4067, 4130, 17885, 16375, 4009, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##이 조사를 받은 뒤 인 ##계 ##된 곳은 어딘 ##가 ? [SEP] ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특 ##공 ##조 및 대 ##공 ##과 직원 12 ##명 등 22 ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##이 조사를 받은 뒤 인 ##계 ##된 곳은 어딘 ##가 ? [SEP] ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 ##분 경 가스 ##총 ##과 전자 ##봉 ##으로 무장 ##한 특 ##공 ##조 및 대 ##공 ##과 직원 12 ##명 등 22 ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 [SEP]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:answer: [CLS]\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4017, 15576, 9838, 934, 2454, 4067, 4130, 17885, 16375, 4009, 32, 3, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 7965, 14050, 4047, 3210, 4239, 4194, 1476, 809, 4239, 4128, 9736, 9685, 4268, 963, 15190, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4017, 15576, 9838, 934, 2454, 4067, 4130, 17885, 16375, 4009, 32, 3, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 4192, 291, 18357, 4696, 4128, 12116, 4694, 7965, 14050, 4047, 3210, 4239, 4194, 1476, 809, 4239, 4128, 9736, 9685, 4268, 963, 15190, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=0, end_positions=0)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##이 조사를 받은 뒤 인 ##계 ##된 곳은 어딘 ##가 ? [SEP] ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 계 ##단을 내려오 ##는 임종석 ##을 발견 , 검 ##거 ##해 구속 ##을 집행 ##했다 . 임종석 ##은 청 ##량 ##리 ##경찰 ##서에 ##서 약 1시간 동안 조사를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장 ##안 ##동의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로 인 ##계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:question & context: [CLS] 임종석 ##이 조사를 받은 뒤 인 ##계 ##된 곳은 어딘 ##가 ? [SEP] ##명의 사 ##복 경찰을 승 ##용 ##차 8 ##대에 나누어 경 ##희 ##대학 ##교 ##에 투입 ##했다 . 198 ##9년 12월 18 ##일 오전 8 ##시 15 ##분 경 서울 ##청 ##량 ##리 ##경찰 ##서는 호위 학생 5 ##명 ##과 함께 경 ##희 ##대학 ##교 학생 ##회 ##관 건물 계 ##단을 내려오 ##는 임종석 ##을 발견 , 검 ##거 ##해 구속 ##을 집행 ##했다 . 임종석 ##은 청 ##량 ##리 ##경찰 ##서에 ##서 약 1시간 동안 조사를 받은 뒤 오전 9 ##시 50 ##분 경 서울 장 ##안 ##동의 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로 인 ##계 ##되었다 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","INFO:ratsnlp:answer: 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로\n","INFO:ratsnlp:answer: 서울 ##지 ##방 ##경찰 ##청 공안 ##분 ##실로\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4017, 15576, 9838, 934, 2454, 4067, 4130, 17885, 16375, 4009, 32, 3, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 296, 13321, 13851, 4008, 14749, 4027, 14384, 15, 255, 4014, 4032, 8430, 4027, 11932, 8258, 17, 14749, 4057, 2883, 4667, 4038, 11304, 14029, 4072, 2208, 20600, 10921, 15576, 9838, 934, 28970, 28, 4039, 8583, 4192, 291, 8270, 2492, 4061, 21643, 8270, 4102, 4015, 11304, 4190, 13445, 4192, 24442, 2454, 4067, 14012, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=103, end_positions=110)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 14749, 4017, 15576, 9838, 934, 2454, 4067, 4130, 17885, 16375, 4009, 32, 3, 12418, 1785, 4232, 21041, 1991, 4429, 4495, 27, 10883, 26039, 291, 4346, 12839, 4267, 4113, 13333, 8258, 17, 25704, 29697, 25604, 8601, 4046, 28970, 27, 4039, 9138, 4192, 291, 8270, 4190, 4667, 4038, 11304, 9666, 24226, 8842, 24, 4268, 4128, 9158, 291, 4346, 12839, 4267, 8842, 4375, 4337, 10828, 296, 13321, 13851, 4008, 14749, 4027, 14384, 15, 255, 4014, 4032, 8430, 4027, 11932, 8258, 17, 14749, 4057, 2883, 4667, 4038, 11304, 14029, 4072, 2208, 20600, 10921, 15576, 9838, 934, 28970, 28, 4039, 8583, 4192, 291, 8270, 2492, 4061, 21643, 8270, 4102, 4015, 11304, 4190, 13445, 4192, 24442, 2454, 4067, 14012, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], start_positions=103, end_positions=110)\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:*** Example ***\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ##로 지명 ##수 ##배 ##된 사람의 이름은 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 [SEP]\n","INFO:ratsnlp:question & context: [CLS] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ##로 지명 ##수 ##배 ##된 사람의 이름은 ? [SEP] 198 ##9년 2월 15일 여의도 농민 폭력 시위를 주도 ##한 혐의 ( 폭력 ##행위 ##등 ##처벌 ##에 ##관 ##한 ##법 ##률 ##위반 ) 으로 지명 ##수 ##배 ##되었다 . 198 ##9년 3월 12 ##일 서울 ##지 ##방 ##검찰 ##청 공안 ##부는 임종석 ##의 사전 ##구속 ##영장 ##을 발부 ##받았 ##다 . 같은 해 6월 30 ##일 평양 ##축 ##전에 임 ##수 ##경을 대표로 파견 ##하여 국가보안법 ##위반 혐의가 추가 ##되었다 . 경찰은 12월 18 ##일 ~ 20 ##일 사이 서울 경 ##희 ##대학 ##교 ##에서 임종석 ##이 성 ##명 발표 ##를 추진 ##하고 있다는 첩 ##보를 입 ##수 ##했고 , 12월 18 ##일 오전 7시 40 [SEP]\n","INFO:ratsnlp:answer: 임종석\n","INFO:ratsnlp:answer: 임종석\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 4091, 23985, 4110, 4200, 4130, 12162, 25542, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=62, end_positions=62)\n","INFO:ratsnlp:features: QAFeatures(input_ids=[2, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 4091, 23985, 4110, 4200, 4130, 12162, 25542, 32, 3, 25704, 29697, 18163, 26757, 20604, 29804, 10901, 25099, 13225, 4047, 16704, 11, 10901, 12175, 4086, 11417, 4113, 4337, 4047, 4021, 4486, 14273, 12, 10442, 23985, 4110, 4200, 14012, 17, 25704, 29697, 18306, 9685, 4046, 8270, 4102, 4015, 10299, 4190, 13445, 11090, 14749, 4042, 11202, 13140, 18936, 4027, 28095, 14930, 4020, 17, 8066, 3376, 23343, 8476, 4046, 14601, 4728, 8409, 2465, 4110, 19033, 24345, 20158, 8455, 29442, 14273, 25419, 11368, 14012, 17, 14270, 25604, 8601, 4046, 95, 8255, 4046, 8538, 8270, 291, 4346, 12839, 4267, 7971, 14749, 4017, 1856, 4268, 9765, 4180, 12166, 7968, 9340, 2880, 17535, 2466, 4110, 9851, 15, 25604, 8601, 4046, 28970, 22074, 8914, 3], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], start_positions=62, end_positions=62)\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/korquad-v1/cached_val_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 3.318 s]\n","INFO:ratsnlp:Saving features into cached file /content/Korpora/korquad-v1/cached_val_BertTokenizer_maxlen-128_maxquerylen-32_docstride-64_korquad-v1_question-answering [took 3.318 s]\n"]}]},{"cell_type":"code","source":["# 모델 초기화\n","from transformers import BertConfig, BertForQuestionAnswering\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n",")\n","\n","model = BertForQuestionAnswering.from_pretrained(\n","    args.pretrained_model_name,\n","    config=pretrained_model_config,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["dc527e02eade41528187ccd90c38a4e2","9dd3df3b4340457cb3b3bcfaa0530c6f","917c862e6d074e15af581dac892caa8b","3b546b73a133478f9c7b4f5b298d31c9","fefd45c296444db9ad53038fe0c56187","c467eb7690f44cf99397ed853d534870","0543a3173b814608a9bf9dad92747c7f","95e3fb356633455780f19ed6a2add497","4fb280271a104d40ae2715b57dae25ef","3226bb845ae044e18845f17d450112eb","78de8d35890b4409a06cfcc2b228e0c1"]},"executionInfo":{"status":"ok","timestamp":1673483755494,"user_tz":-540,"elapsed":7688,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"d63d75f2-5f95-4df5-a1ef-f31fcf45c0b2","id":"EFCFRCBrW89B"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc527e02eade41528187ccd90c38a4e2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# 모델 학습 - 태스크 정의\n","from ratsnlp.nlpbook.qa import QATask\n","task = QATask(model, args)\n","\n","# 모델 학습 - 트레이너 정의\n","trainer = nlpbook.get_trainer(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673483793792,"user_tz":-540,"elapsed":325,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"544f9167-0b6e-4592-ac3b-65bfa222a7fd","id":"hVcg1pziW89B"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n","INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n","INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["# 모델 학습\n","trainer.fit(\n","    task,\n","    train_dataloaders=train_dataloader,\n","    val_dataloaders=val_dataloader,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["144165179b1240188f998b78223ea8c3","36869bc1933f4761872efcd0be72ab53","ec5be25aebce4843b24c784934db3af8","c233bd6bf94b41ddba58ee7ba807bdca","3a36740160634029bdea81ff9e96f323","79447a470c08427382802d5b2aee5f83","b7f032d0d1454d49a6cd48e2e9d1a932","785b118b04764969ab6a210578a6b39a","b41aa89942b2491b9ac15d48c3504399","dd1d7987f55f46f1b98f0685a0599ae0","470f315f45a44666a7f9b6b3577aa0b3","c667a1bd2e1a49b4ba6239507e4cf2c9","183ff9d02308440ca5c91320ac12ea86","5f3b858b83234a0e94562916d26e2d6b","333a9cc3e163449fad23a6e485954c1d","6c97a4016d084557b21b761707e80a18","161a909dbc0b438687c721d1bf521ee1","26f362fe835644a9b229bb5a9a8cdbec","5e66f26534aa4e7bb7774fd6e348caae","5849421080c349e082b35972e923e1cc","6419f291ddbb4c92ab90438c1b17759f","f4480310a74740c198c07ddacf1f707d"]},"executionInfo":{"status":"ok","timestamp":1673488854733,"user_tz":-540,"elapsed":5049015,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"a1d85810-c049-473d-f12f-69fa76508761","id":"ToN-r9XXW89B"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /gdrive/MyDrive/nlpbook/checkpoint-qa/lightning_logs\n","INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.8/dist-packages/pytorch_lightning/core/optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n","  rank_zero_warn(\n","INFO:pytorch_lightning.callbacks.model_summary:\n","  | Name  | Type                     | Params\n","---------------------------------------------------\n","0 | model | BertForQuestionAnswering | 108 M \n","---------------------------------------------------\n","108 M     Trainable params\n","0         Non-trainable params\n","108 M     Total params\n","433.318   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144165179b1240188f998b78223ea8c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c667a1bd2e1a49b4ba6239507e4cf2c9"}},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"g6MytSn_VQ8H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 학습을 마친 모델을 실전 투입하기\n","- 질문에 답하는 웹서비스 만들기"],"metadata":{"id":"QEi-lH_jbWBd"}},{"cell_type":"code","source":["# 인퍼런스 설정\n","from ratsnlp.nlpbook.qa import QADeployArguments\n","args = QADeployArguments(\n","    pretrained_model_name = \"beomi/kcbert-base\",\n","    downstream_model_dir = \"/gdrive/MyDrive/nlpbook/checkpoint-qa\",\n","    max_seq_length=128,\n","    max_query_lenth=32,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673407426231,"user_tz":-540,"elapsed":508,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"e95255cc-f63f-41be-d1a0-c81904bea6e5","id":"GuwpxBjTbWBf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["downstream_model_checkpoint_fpath: /gdrive/MyDrive/nlpbook/checkpoint-ner/epoch=0-val_loss=0.22.ckpt\n","downstream_model_labelmap_fpath: /gdrive/MyDrive/nlpbook/checkpoint-ner/label_map.txt\n"]}]},{"cell_type":"code","source":["# 토크나이저 로드\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","    args.pretrained_model_name,\n","    do_lower_case=False,\n",")"],"metadata":{"id":"zUQOxygIbWBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 체크포인트 로드\n","import torch\n","fine_tuned_model_ckpt = torch.load(\n","    args.downstream_model_checkpoint_path,\n","    map_location=torch.device('cpu')\n",")"],"metadata":{"id":"_i5nCM0MbWBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT 설정 로드\n","from transformers import BertConfig\n","pretrained_model_config = BertConfig.from_pretrained(\n","    args.pretrained_model_name,\n",")"],"metadata":{"id":"pW0XA8GYbWBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BERT 모델 초기화\n","from transformers import BertForQuestionAnswering\n","model = BertForQuestionAnswering(pretrained_model_config)"],"metadata":{"id":"Y7L5OvxKbWBj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 체크 포인트 주입하기\n","model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673407472980,"user_tz":-540,"elapsed":9,"user":{"displayName":"유예지","userId":"10803328730799868038"}},"outputId":"88b44962-a304-461f-bf3c-646ec565b2c9","id":"Go_Kb1BDbWBj"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# 평가모드로 전환\n","model.eval()"],"metadata":{"id":"9_K2yzhXbWBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 출력값 만들고 후처리하기 - 인퍼런스 함수\n","def inference_fn(question, context):\n","  if question  and context:\n","    # 질문을 토큰화하고 인덱싱하되, max_seq_length 보다 길면 이에 맞게 자르기\n","    truncated_query = tokenizer.encode(\n","        question,\n","        add_special_tokens=False,\n","        truncation=True,\n","        max_length=args.max_query_length,\n","    )\n","\n","    # 앞서 처리한 질문을 지문(context)과 함께 토큰화하고 인덱싱하되,\n","    # 전체 길이가 max_seq_length 보다 길면 지문 자르기\n","    inputs = tokenizer.encode_plus(\n","        text = truncated_query,\n","        text_pair = context,\n","        truncation = \"only_second\",\n","        padding = \"max_length\",\n","        max_length = args.max_seq_length,\n","        return_token_type_ids=True,\n","    )\n","\n","    with torch.no_grad():\n","      # 모델 계산하기\n","      # inputs 를 파이토치 텐서로 바꾸기\n","      outputs = model(**{k: torch.tensor([v]) for k, v in inputs.items()})\n","      \n","      # 정답의 시작위치와 관련된 로짓에서 가장 큰 값이 가리키는 토큰 위치 알아내기\n","      start_pred = outputs.start_logits.argmax(dim=-1).item()\n","\n","      # 정답의 끝 위치와 관련된 로짓에서 가장 큰 값이 가리키는 토큰 위치 알아내기\n","      end_pred = outputs.end_logits.argmax(dim=-1).item()\n","\n","      # 정답 시작부터 끝까지의 토큰들을 이어붙여 정답 만들기\n","      pred_text = tokenizer.decode(inputs['input_ids'][start_pred:end_pred+1])\n","\n","  else:\n","    pred_text=\"\"\n","\n","  return {\n","      \"question\" : question,\n","      \"context\" : context,\n","      \"answer\" : pred_text,\n","  }"],"metadata":{"id":"1aZkA0o-bWBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","question = \n","context = \n","inference_fn(question, context)\n","'''"],"metadata":{"id":"pOjSzkqohiqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"03pp9LG0bWBm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 웹서비스 만들기"],"metadata":{"id":"OT2HVd0ffLwT"}},{"cell_type":"code","source":["!rm -rf /root/.ngrok2/"],"metadata":{"id":"S3tdXhQcfLwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"id":"IvWKcSSpfLwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir /root/.ngrok2 && echo \"authtoken: 2K7ID8ZcT7kK16cuvUNOrYHgoxc_81tzFhnzDzLHatQ7xguDQ\" > /root/.ngrok2/ngrok.yml"],"metadata":{"id":"_ZU49kxLfLwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls /root -al"],"metadata":{"id":"hFxxkYPafLwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 웹서비스\n","from ratsnlp.nlpbook.qa import get_web_service_app\n","app = get_web_service_app(inference_fn)\n","app.run()"],"metadata":{"id":"BOoIHaNgfLwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gmi-3MgVfLwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RenDgKdEfLW4"},"execution_count":null,"outputs":[]}]}